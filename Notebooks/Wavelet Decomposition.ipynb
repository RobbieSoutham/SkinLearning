{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859219b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import jit\n",
    "import pickle\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import pywt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.cuda import FloatTensor\n",
    "\n",
    "# Req for package\n",
    "sys.path.append(\"../\")\n",
    "from SkinLearning.Utils.NN import train, test, DEVICE\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f18dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd97be2",
   "metadata": {},
   "source": [
    "# Wavelet decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f53e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(signals, wavelet='db4', level=2):\n",
    "    \"\"\"\n",
    "    Extract wavelet packet features for each signal in the input array.\n",
    "\n",
    "    Args:\n",
    "    - signals (numpy.ndarray): a 2D array containing two 1D signals\n",
    "    - wavelet (str): the name of the wavelet to use for decomposition\n",
    "    - level (int): the level of wavelet packet decomposition to use\n",
    "\n",
    "    Returns:\n",
    "    - features (dict): a dictionary containing the wavelet packet features for each signal\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionary to store features for each signal\n",
    "    features = []\n",
    "\n",
    "    # Loop over each signal\n",
    "    for i in range(2):\n",
    "        # Perform wavelet packet decomposition\n",
    "        coeffs = pywt.WaveletPacket(data=signals[i], wavelet=wavelet, mode='symmetric', maxlevel=level).get_level(level, 'freq')\n",
    "\n",
    "        # Vectorize the coefficients\n",
    "        coefficients = np.array([c.data for c in coeffs], dtype=object)\n",
    "        coefficients = coefficients.reshape(-1)\n",
    "\n",
    "        # Store the coefficients as features for this signal\n",
    "        features.append(coefficients.tolist())\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a484679",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m waveletExtraction(\u001b[43mdataset\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "waveletExtraction(dataset[0]['input'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499aa36",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00211a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder name will correspond to index of sample\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, scaler, signalFolder=\"D:/SamplingResults2\", sampleFile=\"../Data/newSamples.pkl\", runs=range(65535), steps=128):\n",
    "        # Load both disp1 and disp2 from each folder\n",
    "        # Folders ordered according to index of sample\n",
    "        # Use the corresponding sample as y -> append probe?\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        with open(f\"{sampleFile}\", \"rb\") as f:\n",
    "             samples = pickle.load(f)\n",
    "        \n",
    "        self.min = np.min(samples[runs])\n",
    "        self.max = np.max(samples[runs])\n",
    "        \n",
    "        \n",
    "        for run in tqdm(runs):\n",
    "            inp = []\n",
    "            fail = False\n",
    "            \n",
    "            files = os.listdir(f\"{signalFolder}/{run}/\")\n",
    "            \n",
    "            if files != ['Disp1.csv', 'Disp2.csv']:\n",
    "                continue\n",
    "            \n",
    "            for file in files:\n",
    "                a = pd.read_csv(f\"{signalFolder}/{run}/{file}\")\n",
    "                a.rename(columns = {'0':'x', '0.1': 'y'}, inplace = True)\n",
    "                \n",
    "                # Skip if unconverged\n",
    "                if a['x'].max() != 7.0:\n",
    "                    fail = True\n",
    "                    break\n",
    "\n",
    "                # Interpolate curve for consistent x values\n",
    "                xNew = np.linspace(0, 7, num=steps, endpoint=False)\n",
    "                interped = interp1d(a['x'], a['y'], kind='cubic', fill_value=\"extrapolate\")(xNew)\n",
    "                    \n",
    "                \n",
    "                inp.append(interped.astype(\"float32\"))\n",
    "            \n",
    "            if not fail:\n",
    "                if len(inp) != 2:\n",
    "                    raise Exception(\"sdf\")\n",
    "\n",
    "                self.input.append(inp)\n",
    "                self.output.append(samples[int(run)])\n",
    "        \n",
    "        scaler.fit(self.output)\n",
    "        self.output = scaler.fit_transform(self.output)\n",
    "        self.output = tensor(self.output).type(FloatTensor)\n",
    "        \n",
    "        self.input = [waveletExtraction(sample) for sample in self.input]\n",
    "        self.input = tensor(self.input).type(FloatTensor)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"input\": self.input[idx], \"output\": self.output[idx]}\n",
    "        return sample\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974eb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates the data set from filtered samples\n",
    "    Returns the dataset and the scaler\n",
    "\"\"\"\n",
    "def getDataset(**kwargs):\n",
    "    # Get filtered data\n",
    "    if not 'runs' in kwargs.keys():\n",
    "        with open(\"../Data/filtered.pkl\", \"rb\") as f:\n",
    "            runs = pickle.load(f)\n",
    "\n",
    "        kwargs['runs'] = runs\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset = SkinDataset(scaler=scaler, **kwargs)\n",
    "\n",
    "    return dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bf2c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates a train/test split from the given data\n",
    "    Returns train and test data loaders\n",
    "\"\"\"\n",
    "def getSplit(dataset, p1=0.8):\n",
    "    train_n = int(p1 * len(dataset))\n",
    "    test_n = len(dataset) - train_n\n",
    "    train_set, test_set = random_split(dataset, [train_n, test_n])\n",
    "\n",
    "    return DataLoader(train_set, batch_size=32, shuffle=True), \\\n",
    "        DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9a8c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:28<00:00, 78.71it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, scaler = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98edea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = getSplit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e43bb346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0]['input'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b45846",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee77f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(2, 128, kernel_size=5, padding=1, bias=False)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.RNN(126, 256, batch_first=True)\n",
    "        self.fc1 = nn.Linear(65536, 1024)\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024 , 512)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.d3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        h0 = torch.zeros(1, batch_size, 256).to(x.device)\n",
    "        x, _ = self.rnn(x, h0)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        x = self.d1(torch.relu(self.fc1(x)))\n",
    "        \n",
    "        x = self.d2(torch.relu(self.fc2(x)))\n",
    "        \n",
    "        x = self.d3(torch.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        x = x.view(batch_size, 6)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0c00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, num_layers=1, output_dim=6):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 6)\n",
    "        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, 2)\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Initialize cell state with zeros\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Move tensors to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "            x = x.cuda()\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "693d75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSingle(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, num_layers=2, output_dim=6):\n",
    "        super(RNNSingle, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(131072, 1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, 2)\n",
    "        \n",
    "        h0 = torch.zeros(5, batch_size, 256).to(x.device)\n",
    "        x, _ = self.rnn(x, h0)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98e0de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN2(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=256, output_size=6):\n",
    "        super(RNN2, self).__init__()\n",
    "        \n",
    "        # Define the RNN layers for each input channel\n",
    "        self.rnn1 = nn.RNN(8, 128, batch_first=True)\n",
    "        self.rnn2 = nn.RNN(8, 128, batch_first=True)\n",
    "        \n",
    "        # Define the final RNN layer that takes in the concatenated outputs of the two channels\n",
    "        self.final_rnn = nn.RNN(2, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Define the output layer that maps from the hidden state to the output\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 6)\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, 2, 22, 8)\n",
    "        # Process the two input channels through their respective RNN layers\n",
    "        _, h1 = self.rnn1(x[:, 0, :, :])\n",
    "        _, h2 = self.rnn2(x[:, 1, :, :])\n",
    "        \n",
    "        \n",
    "        # Concatenate the hidden states of the two channels\n",
    "        h_cat = torch.cat([h1, h2])\n",
    "        h_cat = h_cat.reshape(batch_size, -1, 2)\n",
    "\n",
    "        # Pass the concatenated hidden states through the final RNN layer\n",
    "        _, h_final = self.final_rnn(h_cat)\n",
    "        \n",
    "        # Compute the output\n",
    "        output = self.fc(h_final).reshape(batch_size, 6)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a6d1db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRNN(nn.Module):\n",
    "    def __init__(self, input_size=148, hidden_size=128):\n",
    "        super(SiameseRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 6),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x1 = x[:, 0, :].unsqueeze(1)\n",
    "        x2 = x[:, 0, :].unsqueeze(1)\n",
    "        \n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n",
    "        _, h1 = self.rnn(x1, h0)  # Add a batch dimension\n",
    "        _, h2 = self.rnn(x2, h0)  # Add a batch dimension\n",
    "        \n",
    "        out = torch.cat([h1[-1], h2[-1]], dim=1)\n",
    "        out = out.reshape(batch_size, -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "efd5d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, input_size=148, hidden_size=80, num_layers=2):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6),\n",
    "        )\n",
    "\n",
    "     \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x1 = x[:, 0, :].unsqueeze(1)\n",
    "        x2 = x[:, 0, :].unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers*1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*1, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass through the LSTM layer\n",
    "        out1 = self.lstm(x1, (h0, c0))[1][-1]\n",
    "        out2 = self.lstm(x2, (h0, c0))[1][-1]\n",
    "        \n",
    "        \n",
    "        out = torch.cat([out1, out2], dim=0)\n",
    "        out = out.reshape(batch_size, -1)\n",
    "        # Pass the last hidden state through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37af1f",
   "metadata": {},
   "source": [
    "# Test on all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "52ffdd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sRNN = SiameseRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "5766f10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|█████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 172.04batch/s]\n",
      "Epoch 2/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 173.37batch/s, lastLoss=0.199, valLoss=0.193]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 181.82batch/s, lastLoss=0.179, valLoss=0.17]\n",
      "Epoch 4/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 175.83batch/s, lastLoss=0.169, valLoss=0.166]\n",
      "Epoch 5/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 185.74batch/s, lastLoss=0.167, valLoss=0.16]\n",
      "Epoch 6/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 182.41batch/s, lastLoss=0.165, valLoss=0.166]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 182.71batch/s, lastLoss=0.167, valLoss=0.17]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 181.52batch/s, lastLoss=0.165, valLoss=0.16]\n",
      "Epoch 9/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 181.52batch/s, lastLoss=0.164, valLoss=0.17]\n",
      "Epoch 10/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 178.07batch/s, lastLoss=0.164, valLoss=0.169]\n",
      "Epoch 11/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 170.84batch/s, lastLoss=0.165, valLoss=0.161]\n",
      "Epoch 12/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 181.23batch/s, lastLoss=0.163, valLoss=0.164]\n",
      "Epoch 13/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 177.38batch/s, lastLoss=0.164, valLoss=0.169]\n",
      "Epoch 14/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 181.23batch/s, lastLoss=0.163, valLoss=0.161]\n",
      "Epoch 15/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 184.82batch/s, lastLoss=0.163, valLoss=0.163]\n",
      "Epoch 16/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 178.63batch/s, lastLoss=0.164, valLoss=0.166]\n",
      "Epoch 17/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 182.70batch/s, lastLoss=0.162, valLoss=0.164]\n",
      "Epoch 18/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 182.11batch/s, lastLoss=0.162, valLoss=0.157]\n",
      "Epoch 19/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 179.46batch/s, lastLoss=0.162, valLoss=0.158]\n",
      "Epoch 20/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 164.71batch/s, lastLoss=0.164, valLoss=0.172]\n",
      "Epoch 21/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 172.04batch/s, lastLoss=0.164, valLoss=0.161]\n",
      "Epoch 22/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 176.94batch/s, lastLoss=0.164, valLoss=0.162]\n",
      "Epoch 23/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 180.36batch/s, lastLoss=0.163, valLoss=0.163]\n",
      "Epoch 24/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 181.52batch/s, lastLoss=0.161, valLoss=0.167]\n",
      "Epoch 25/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 166.92batch/s, lastLoss=0.16, valLoss=0.161]\n",
      "Epoch 26/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 173.91batch/s, lastLoss=0.158, valLoss=0.158]\n",
      "Epoch 27/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 183.91batch/s, lastLoss=0.158, valLoss=0.157]\n",
      "Epoch 28/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 176.65batch/s, lastLoss=0.16, valLoss=0.157]\n",
      "Epoch 29/400:  36%|███████████▊                     | 20/56 [00:00<00:00, 170.21batch/s, lastLoss=0.157, valLoss=0.152]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [267]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sRNN_train_loss, sRNN_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     25\u001b[0m         cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     26\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 27\u001b[0m         \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sRNN_train_loss, sRNN_val_loss =  train(train_loader, sRNN, val_loader=test_loader, LR=0.01, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "fc891436",
   "metadata": {},
   "outputs": [],
   "source": [
    "sLSTM = SiameseLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5a24e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|█████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 145.22batch/s]\n",
      "Epoch 2/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 151.14batch/s, lastLoss=0.197, valLoss=0.179]\n",
      "Epoch 3/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 145.08batch/s, lastLoss=0.187, valLoss=0.187]\n",
      "Epoch 4/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 153.95batch/s, lastLoss=0.187, valLoss=0.191]\n",
      "Epoch 5/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 145.26batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 6/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 158.18batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 7/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 158.48batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 160.08batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 9/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 162.12batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 10/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 160.23batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 11/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 160.84batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 12/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 162.55batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 13/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.52batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 14/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.64batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 15/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.08batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 16/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.31batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 17/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 156.86batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 18/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.13batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 19/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.64batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 20/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 152.80batch/s, lastLoss=0.186, valLoss=0.192]\n",
      "Epoch 21/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.64batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 22/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 159.54batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 23/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 156.21batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 24/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 147.95batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 25/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 153.63batch/s, lastLoss=0.186, valLoss=0.193]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 156.21batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 27/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 155.56batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 28/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.98batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 29/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.91batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 30/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 160.92batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 31/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 150.94batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 32/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.19batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 33/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 146.41batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 34/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 160.46batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 35/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 157.75batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 36/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.34batch/s, lastLoss=0.185, valLoss=0.177]\n",
      "Epoch 37/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.64batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 38/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 156.79batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 39/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.48batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 40/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.75batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 41/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 152.38batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 42/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.20batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 43/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.56batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 44/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 156.64batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 45/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.48batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 46/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 156.36batch/s, lastLoss=0.185, valLoss=0.179]\n",
      "Epoch 47/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 159.55batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 48/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.78batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 49/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.30batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 50/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 160.92batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 51/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 161.85batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 52/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.75batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 53/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.34batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 54/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.26batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 55/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 152.59batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 56/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 159.77batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 57/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.06batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 58/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.77batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 59/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.70batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 60/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.56batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 61/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 151.35batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 62/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.09batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 63/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 160.23batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 64/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 160.00batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 65/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.09batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 66/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.71batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 67/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.13batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 68/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 148.34batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 69/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 154.18batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 70/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 150.94batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 71/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.20batch/s, lastLoss=0.185, valLoss=0.179]\n",
      "Epoch 72/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 150.94batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 73/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 149.14batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 74/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 144.70batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 75/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 150.34batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 76/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 151.97batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 77/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.51batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 78/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.41batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 79/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 161.14batch/s, lastLoss=0.186, valLoss=0.178]\n",
      "Epoch 80/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.73batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 81/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 152.59batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 82/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 159.99batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 83/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 153.63batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 84/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 153.31batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 85/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 150.53batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 86/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 160.69batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 87/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.91batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 88/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.75batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 89/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 159.75batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 90/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 157.30batch/s, lastLoss=0.185, valLoss=0.19]\n",
      "Epoch 91/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.27batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 92/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.12batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 93/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 156.87batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 94/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.87batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 95/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 161.61batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 96/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.30batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 97/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.74batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 98/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 154.06batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 99/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 159.32batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 100/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.09batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 101/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.09batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 102/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.42batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 103/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.68batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 104/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.46batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 105/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.44batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 106/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.32batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 107/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.08batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 108/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.67batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 109/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.83batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 110/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.27batch/s, lastLoss=0.185, valLoss=0.179]\n",
      "Epoch 111/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.98batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 112/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.02batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 113/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.54batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 114/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.50batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 115/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.56batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 116/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.18batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 117/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.80batch/s, lastLoss=0.185, valLoss=0.179]\n",
      "Epoch 118/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.06batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 119/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.67batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 120/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.19batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 121/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 122/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 158.65batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 123/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 164.76batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 124/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.71batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 125/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.18batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 126/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.99batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 127/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.46batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 128/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.86batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 129/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.29batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 130/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.33batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 131/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.68batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 132/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.19batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 133/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.76batch/s, lastLoss=0.185, valLoss=0.178]\n",
      "Epoch 134/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.26batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 135/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.38batch/s, lastLoss=0.185, valLoss=0.179]\n",
      "Epoch 136/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.01batch/s, lastLoss=0.185, valLoss=0.185]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.34batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 138/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.35batch/s, lastLoss=0.186, valLoss=0.193]\n",
      "Epoch 139/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.65batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 140/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.17batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 141/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 169.20batch/s, lastLoss=0.185, valLoss=0.19]\n",
      "Epoch 142/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.93batch/s, lastLoss=0.185, valLoss=0.191]\n",
      "Epoch 143/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.50batch/s, lastLoss=0.185, valLoss=0.176]\n",
      "Epoch 144/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.95batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 145/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.22batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 146/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.84batch/s, lastLoss=0.186, valLoss=0.193]\n",
      "Epoch 147/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 148/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.81batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 149/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.92batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 150/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.83batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 151/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.26batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 152/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.26batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 153/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.69batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 154/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.12batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 155/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.26batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 156/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.38batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 157/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 157.05batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 158/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 162.93batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 159/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 159.93batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 160/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.47batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 161/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.84batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 162/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.59batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 163/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.03batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 164/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 164.22batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 165/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.15batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 166/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.42batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 167/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.21batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 168/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.00batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 169/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.98batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 170/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.31batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 171/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.44batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 172/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.94batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 173/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 161.38batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 174/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 155.12batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 175/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.15batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 176/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.96batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 177/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.89batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 178/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.61batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 179/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.97batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 180/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.51batch/s, lastLoss=0.185, valLoss=0.177]\n",
      "Epoch 181/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 162.49batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 182/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.96batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 183/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.75batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 184/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 155.99batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 185/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.29batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 186/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.88batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 187/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.98batch/s, lastLoss=0.185, valLoss=0.191]\n",
      "Epoch 188/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.38batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 189/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.52batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 190/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.18batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 191/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 156.52batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 192/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.08batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 193/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.27batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 194/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.09batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 195/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.33batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 196/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 154.61batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 197/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.47batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 198/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.50batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 199/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 200/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.03batch/s, lastLoss=0.186, valLoss=0.192]\n",
      "Epoch 201/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.46batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 202/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.22batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 203/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.91batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 204/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.44batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 205/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 148.94batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 206/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.43batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 207/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.14batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 208/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.08batch/s, lastLoss=0.186, valLoss=0.193]\n",
      "Epoch 209/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.46batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 210/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.94batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 211/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.71batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 212/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.67batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 213/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 168.68batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 214/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.79batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 215/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 167.03batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 216/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.27batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 217/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.74batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 218/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.75batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 219/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 220/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.74batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 221/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.55batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 222/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 151.14batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 223/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.19batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 224/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.22batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 225/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.74batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 226/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.86batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 227/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.83batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 228/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.80batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 229/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.95batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 230/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.22batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 231/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.17batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 232/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.44batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 233/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.06batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 234/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.15batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 235/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.07batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 236/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.76batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 237/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.20batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 238/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.03batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 239/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.99batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 240/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.74batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 241/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.35batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 242/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.51batch/s, lastLoss=0.185, valLoss=0.177]\n",
      "Epoch 243/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 155.54batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 244/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.22batch/s, lastLoss=0.185, valLoss=0.178]\n",
      "Epoch 245/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.48batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 246/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.40batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 247/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.14batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 248/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.71batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 249/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.02batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 250/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.25batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 251/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.93batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 252/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.16batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 253/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.25batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 254/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.03batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 255/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.62batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 256/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.34batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 257/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.02batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 258/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.74batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 259/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.07batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 260/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.68batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 261/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 262/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.71batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 263/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.19batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 264/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.03batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 265/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 266/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.61batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 267/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.74batch/s, lastLoss=0.185, valLoss=0.191]\n",
      "Epoch 268/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.37batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 269/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.46batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 270/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.15batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 271/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.19batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 272/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.19batch/s, lastLoss=0.185, valLoss=0.188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 273/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.35batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 274/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.18batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 275/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.23batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 276/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.87batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 277/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.46batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 278/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.21batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 279/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.78batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 280/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.23batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 281/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.88batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 282/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.78batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 283/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.45batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 284/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.31batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 285/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.75batch/s, lastLoss=0.186, valLoss=0.177]\n",
      "Epoch 286/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.60batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 287/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.22batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 288/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.14batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 289/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.11batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 290/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.38batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 291/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.70batch/s, lastLoss=0.185, valLoss=0.191]\n",
      "Epoch 292/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.92batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 293/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.64batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 294/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.92batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 295/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 167.91batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 296/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.18batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 297/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.77batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 298/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.88batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 299/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.17batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 300/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.27batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 301/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.44batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 302/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 157.30batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 303/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 304/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.71batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 305/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.00batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 306/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.18batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 307/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.97batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 308/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 164.84batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 309/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.64batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 310/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 157.97batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 311/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.57batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 312/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.05batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 313/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.66batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 314/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.27batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 315/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.42batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 316/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.33batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 317/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.32batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 318/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.38batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 319/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.28batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 320/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 157.98batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 321/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 162.59batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 322/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.57batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 323/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.74batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 324/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.20batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 325/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 326/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.14batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 327/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.79batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 328/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.71batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 329/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.84batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 330/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.44batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 331/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.34batch/s, lastLoss=0.185, valLoss=0.194]\n",
      "Epoch 332/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 153.84batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 333/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 163.26batch/s, lastLoss=0.185, valLoss=0.19]\n",
      "Epoch 334/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 161.99batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 335/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.65batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 336/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 162.94batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 337/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.09batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 338/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.87batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 339/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.85batch/s, lastLoss=0.186, valLoss=0.193]\n",
      "Epoch 340/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.38batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 341/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.06batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 342/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.68batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 343/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.92batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 344/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.52batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 345/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.11batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 346/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.19batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 347/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.82batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 348/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.46batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 349/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.14batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 350/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.20batch/s, lastLoss=0.185, valLoss=0.189]\n",
      "Epoch 351/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 163.60batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 352/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.68batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 353/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 163.35batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 354/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.41batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 355/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.93batch/s, lastLoss=0.185, valLoss=0.186]\n",
      "Epoch 356/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 357/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.22batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 358/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.34batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 359/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.71batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 360/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.91batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 361/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 154.38batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 362/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.58batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 363/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.06batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 364/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.22batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 365/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.97batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 366/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 164.21batch/s, lastLoss=0.185, valLoss=0.19]\n",
      "Epoch 367/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.54batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 368/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 161.69batch/s, lastLoss=0.185, valLoss=0.19]\n",
      "Epoch 369/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.21batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 370/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.64batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 371/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.00batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 372/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.59batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 373/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 164.92batch/s, lastLoss=0.185, valLoss=0.18]\n",
      "Epoch 374/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.79batch/s, lastLoss=0.185, valLoss=0.179]\n",
      "Epoch 375/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.09batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 376/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.18batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 377/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 159.81batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 378/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 157.35batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 379/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.23batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 380/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.26batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 381/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.95batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 382/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.00batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 383/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.84batch/s, lastLoss=0.185, valLoss=0.185]\n",
      "Epoch 384/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.02batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 385/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.80batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 386/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.73batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 387/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.08batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 388/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.62batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 389/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.22batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 390/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 164.17batch/s, lastLoss=0.185, valLoss=0.19]\n",
      "Epoch 391/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 165.20batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 392/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.19batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 393/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.54batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 394/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.95batch/s, lastLoss=0.185, valLoss=0.187]\n",
      "Epoch 395/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.38batch/s, lastLoss=0.186, valLoss=0.178]\n",
      "Epoch 396/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 164.42batch/s, lastLoss=0.185, valLoss=0.188]\n",
      "Epoch 397/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.87batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 398/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 162.07batch/s, lastLoss=0.185, valLoss=0.178]\n",
      "Epoch 399/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 158.34batch/s, lastLoss=0.186, valLoss=0.192]\n",
      "Epoch 400/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 160.45batch/s, lastLoss=0.185, valLoss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.18557937770682786\n",
      "Average validation loss: 0.1850056944147994\n"
     ]
    }
   ],
   "source": [
    "sLSTM_train_loss, sLSTM_val_loss =  train(train_loader, sLSTM, val_loader=test_loader, LR=0.01, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "58414825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/15 [00:00<?, ? batch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [250]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:80\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(test_loader, net, scaler)\u001b[0m\n\u001b[0;32m     77\u001b[0m predicted \u001b[38;5;241m=\u001b[39m net(inp)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Denormalise\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m o \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Get column wise and overall MAPE\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Since each column is normalised should also be able to use MAE*100\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:525\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"Undo the scaling of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124;03m    Transformed data.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    523\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 525\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[0;32m    530\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:794\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    790\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to convert array of bytes/strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minto decimal numbers with dtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 794\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    797\u001b[0m     )\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    800\u001b[0m     _assert_all_finite(array, allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "test(test_loader, sRNN, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cedb111",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2 = RNN2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a50ba36b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [101]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss2, val_loss \u001b[38;5;241m=\u001b[39m  train(train_loader, \u001b[43mrnn2\u001b[49m, val_loader\u001b[38;5;241m=\u001b[39mtest_loader, LR\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn2' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss2, val_loss =  train(train_loader, rnn2, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "531f9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "65a577e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:01<00:00, 36.02batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.42batch/s, lastLoss=0.269, valLoss=0.239]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.33batch/s, lastLoss=0.207, valLoss=0.161]\n",
      "Epoch 4/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.50batch/s, lastLoss=0.191, valLoss=0.163]\n",
      "Epoch 5/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.37batch/s, lastLoss=0.183, valLoss=0.139]\n",
      "Epoch 6/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.47batch/s, lastLoss=0.178, valLoss=0.139]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.30batch/s, lastLoss=0.173, valLoss=0.134]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.42batch/s, lastLoss=0.169, valLoss=0.146]\n",
      "Epoch 9/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.32batch/s, lastLoss=0.165, valLoss=0.142]\n",
      "Epoch 10/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.55batch/s, lastLoss=0.162, valLoss=0.127]\n",
      "Epoch 11/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.51batch/s, lastLoss=0.16, valLoss=0.146]\n",
      "Epoch 12/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.34batch/s, lastLoss=0.159, valLoss=0.149]\n",
      "Epoch 13/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.38batch/s, lastLoss=0.155, valLoss=0.132]\n",
      "Epoch 14/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.41batch/s, lastLoss=0.151, valLoss=0.122]\n",
      "Epoch 15/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.42batch/s, lastLoss=0.151, valLoss=0.113]\n",
      "Epoch 16/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.45batch/s, lastLoss=0.148, valLoss=0.146]\n",
      "Epoch 17/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.50batch/s, lastLoss=0.146, valLoss=0.134]\n",
      "Epoch 18/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.37batch/s, lastLoss=0.146, valLoss=0.138]\n",
      "Epoch 19/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.44batch/s, lastLoss=0.143, valLoss=0.123]\n",
      "Epoch 20/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.33batch/s, lastLoss=0.142, valLoss=0.121]\n",
      "Epoch 21/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.49batch/s, lastLoss=0.14, valLoss=0.118]\n",
      "Epoch 22/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.42batch/s, lastLoss=0.14, valLoss=0.126]\n",
      "Epoch 23/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.32batch/s, lastLoss=0.139, valLoss=0.128]\n",
      "Epoch 24/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.11batch/s, lastLoss=0.139, valLoss=0.134]\n",
      "Epoch 25/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.28batch/s, lastLoss=0.137, valLoss=0.122]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.95batch/s, lastLoss=0.135, valLoss=0.126]\n",
      "Epoch 27/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.98batch/s, lastLoss=0.134, valLoss=0.124]\n",
      "Epoch 28/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 38.12batch/s, lastLoss=0.135, valLoss=0.117]\n",
      "Epoch 29/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 37.58batch/s, lastLoss=0.132, valLoss=0.117]\n",
      "Epoch 30/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 38.15batch/s, lastLoss=0.133, valLoss=0.112]\n",
      "Epoch 31/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 35.77batch/s, lastLoss=0.132, valLoss=0.108]\n",
      "Epoch 32/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.17batch/s, lastLoss=0.133, valLoss=0.11]\n",
      "Epoch 33/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.17batch/s, lastLoss=0.131, valLoss=0.119]\n",
      "Epoch 34/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.27batch/s, lastLoss=0.13, valLoss=0.118]\n",
      "Epoch 35/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.21batch/s, lastLoss=0.13, valLoss=0.119]\n",
      "Epoch 36/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.43batch/s, lastLoss=0.129, valLoss=0.121]\n",
      "Epoch 37/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.07batch/s, lastLoss=0.126, valLoss=0.112]\n",
      "Epoch 38/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.30batch/s, lastLoss=0.127, valLoss=0.113]\n",
      "Epoch 39/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.28batch/s, lastLoss=0.125, valLoss=0.106]\n",
      "Epoch 40/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.36batch/s, lastLoss=0.126, valLoss=0.107]\n",
      "Epoch 41/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.50batch/s, lastLoss=0.125, valLoss=0.113]\n",
      "Epoch 42/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 35.98batch/s, lastLoss=0.125, valLoss=0.108]\n",
      "Epoch 43/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 35.43batch/s, lastLoss=0.125, valLoss=0.102]\n",
      "Epoch 44/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 35.45batch/s, lastLoss=0.125, valLoss=0.103]\n",
      "Epoch 45/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.12batch/s, lastLoss=0.122, valLoss=0.0969]\n",
      "Epoch 46/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.07batch/s, lastLoss=0.122, valLoss=0.111]\n",
      "Epoch 47/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 35.53batch/s, lastLoss=0.122, valLoss=0.111]\n",
      "Epoch 48/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.79batch/s, lastLoss=0.121, valLoss=0.0983]\n",
      "Epoch 49/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.39batch/s, lastLoss=0.119, valLoss=0.106]\n",
      "Epoch 50/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.22batch/s, lastLoss=0.12, valLoss=0.0999]\n",
      "Epoch 51/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.25batch/s, lastLoss=0.12, valLoss=0.105]\n",
      "Epoch 52/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.36batch/s, lastLoss=0.118, valLoss=0.0963]\n",
      "Epoch 53/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.22batch/s, lastLoss=0.116, valLoss=0.0995]\n",
      "Epoch 54/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.17batch/s, lastLoss=0.119, valLoss=0.105]\n",
      "Epoch 55/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.27batch/s, lastLoss=0.118, valLoss=0.104]\n",
      "Epoch 56/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.13batch/s, lastLoss=0.117, valLoss=0.096]\n",
      "Epoch 57/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.89batch/s, lastLoss=0.114, valLoss=0.0976]\n",
      "Epoch 58/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.04batch/s, lastLoss=0.114, valLoss=0.0893]\n",
      "Epoch 59/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.63batch/s, lastLoss=0.113, valLoss=0.0941]\n",
      "Epoch 60/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.16batch/s, lastLoss=0.114, valLoss=0.0969]\n",
      "Epoch 61/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.24batch/s, lastLoss=0.114, valLoss=0.0978]\n",
      "Epoch 62/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.35batch/s, lastLoss=0.112, valLoss=0.0968]\n",
      "Epoch 63/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.28batch/s, lastLoss=0.112, valLoss=0.0978]\n",
      "Epoch 64/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.26batch/s, lastLoss=0.115, valLoss=0.101]\n",
      "Epoch 65/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.33batch/s, lastLoss=0.111, valLoss=0.0928]\n",
      "Epoch 66/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.40batch/s, lastLoss=0.111, valLoss=0.0907]\n",
      "Epoch 67/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.34batch/s, lastLoss=0.11, valLoss=0.0951]\n",
      "Epoch 68/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.82batch/s, lastLoss=0.108, valLoss=0.0917]\n",
      "Epoch 69/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.70batch/s, lastLoss=0.109, valLoss=0.0952]\n",
      "Epoch 70/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.06batch/s, lastLoss=0.109, valLoss=0.0867]\n",
      "Epoch 71/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.25batch/s, lastLoss=0.109, valLoss=0.0901]\n",
      "Epoch 72/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.39batch/s, lastLoss=0.109, valLoss=0.0909]\n",
      "Epoch 73/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.32batch/s, lastLoss=0.106, valLoss=0.0912]\n",
      "Epoch 74/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.33batch/s, lastLoss=0.106, valLoss=0.0855]\n",
      "Epoch 75/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.45batch/s, lastLoss=0.105, valLoss=0.086]\n",
      "Epoch 76/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.22batch/s, lastLoss=0.107, valLoss=0.0823]\n",
      "Epoch 77/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.19batch/s, lastLoss=0.107, valLoss=0.0826]\n",
      "Epoch 78/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.26batch/s, lastLoss=0.105, valLoss=0.0922]\n",
      "Epoch 79/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.20batch/s, lastLoss=0.104, valLoss=0.0874]\n",
      "Epoch 80/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.25batch/s, lastLoss=0.105, valLoss=0.0843]\n",
      "Epoch 81/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.00batch/s, lastLoss=0.103, valLoss=0.0837]\n",
      "Epoch 82/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.94batch/s, lastLoss=0.104, valLoss=0.0815]\n",
      "Epoch 83/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 33.53batch/s, lastLoss=0.105, valLoss=0.0813]\n",
      "Epoch 84/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.23batch/s, lastLoss=0.104, valLoss=0.0832]\n",
      "Epoch 85/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.93batch/s, lastLoss=0.103, valLoss=0.0819]\n",
      "Epoch 86/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.95batch/s, lastLoss=0.102, valLoss=0.0881]\n",
      "Epoch 87/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.11batch/s, lastLoss=0.102, valLoss=0.0788]\n",
      "Epoch 88/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.26batch/s, lastLoss=0.101, valLoss=0.0812]\n",
      "Epoch 89/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.78batch/s, lastLoss=0.101, valLoss=0.0792]\n",
      "Epoch 90/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.04batch/s, lastLoss=0.0996, valLoss=0.085]\n",
      "Epoch 91/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.59batch/s, lastLoss=0.101, valLoss=0.0812]\n",
      "Epoch 92/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.28batch/s, lastLoss=0.1, valLoss=0.0854]\n",
      "Epoch 93/400: 100%|████████████████████████████████| 56/56 [00:01<00:00, 36.41batch/s, lastLoss=0.0999, valLoss=0.0846]\n",
      "Epoch 94/400:  14%|████▊                             | 8/56 [00:00<00:01, 35.32batch/s, lastLoss=0.099, valLoss=0.0725]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [172]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     28\u001b[0m inp, out \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))))\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[1;32m---> 30\u001b[0m h0 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, h0)\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, val_loss =  train(train_loader, rnn, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa42becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31eaa56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:01<00:00, 45.55batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.03batch/s, lastLoss=0.281, valLoss=0.204]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.67batch/s, lastLoss=0.223, valLoss=0.189]\n",
      "Epoch 4/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 46.43batch/s, lastLoss=0.215, valLoss=0.188]\n",
      "Epoch 5/400: 100%|████████████████████████████████████| 56/56 [00:01<00:00, 45.86batch/s, lastLoss=0.21, valLoss=0.191]\n",
      "Epoch 6/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 46.02batch/s, lastLoss=0.207, valLoss=0.184]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.94batch/s, lastLoss=0.205, valLoss=0.186]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 43.73batch/s, lastLoss=0.204, valLoss=0.186]\n",
      "Epoch 9/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.70batch/s, lastLoss=0.203, valLoss=0.181]\n",
      "Epoch 10/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.77batch/s, lastLoss=0.201, valLoss=0.185]\n",
      "Epoch 11/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.45batch/s, lastLoss=0.199, valLoss=0.192]\n",
      "Epoch 12/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 43.53batch/s, lastLoss=0.199, valLoss=0.187]\n",
      "Epoch 13/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.89batch/s, lastLoss=0.199, valLoss=0.185]\n",
      "Epoch 14/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.86batch/s, lastLoss=0.198, valLoss=0.187]\n",
      "Epoch 15/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.61batch/s, lastLoss=0.197, valLoss=0.185]\n",
      "Epoch 16/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.11batch/s, lastLoss=0.197, valLoss=0.182]\n",
      "Epoch 17/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.29batch/s, lastLoss=0.196, valLoss=0.184]\n",
      "Epoch 18/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.19batch/s, lastLoss=0.196, valLoss=0.187]\n",
      "Epoch 19/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.43batch/s, lastLoss=0.197, valLoss=0.194]\n",
      "Epoch 20/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.48batch/s, lastLoss=0.195, valLoss=0.184]\n",
      "Epoch 21/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.36batch/s, lastLoss=0.194, valLoss=0.188]\n",
      "Epoch 22/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.54batch/s, lastLoss=0.194, valLoss=0.186]\n",
      "Epoch 23/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.24batch/s, lastLoss=0.196, valLoss=0.186]\n",
      "Epoch 24/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.76batch/s, lastLoss=0.193, valLoss=0.188]\n",
      "Epoch 25/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.29batch/s, lastLoss=0.194, valLoss=0.187]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.16batch/s, lastLoss=0.193, valLoss=0.181]\n",
      "Epoch 27/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.56batch/s, lastLoss=0.193, valLoss=0.186]\n",
      "Epoch 28/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 47.20batch/s, lastLoss=0.192, valLoss=0.185]\n",
      "Epoch 29/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.36batch/s, lastLoss=0.192, valLoss=0.189]\n",
      "Epoch 30/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.82batch/s, lastLoss=0.191, valLoss=0.183]\n",
      "Epoch 31/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.78batch/s, lastLoss=0.192, valLoss=0.178]\n",
      "Epoch 32/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.46batch/s, lastLoss=0.192, valLoss=0.188]\n",
      "Epoch 33/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.41batch/s, lastLoss=0.192, valLoss=0.183]\n",
      "Epoch 34/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.47batch/s, lastLoss=0.192, valLoss=0.184]\n",
      "Epoch 35/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.82batch/s, lastLoss=0.191, valLoss=0.188]\n",
      "Epoch 36/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.95batch/s, lastLoss=0.192, valLoss=0.187]\n",
      "Epoch 37/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.11batch/s, lastLoss=0.192, valLoss=0.182]\n",
      "Epoch 38/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.21batch/s, lastLoss=0.191, valLoss=0.184]\n",
      "Epoch 39/400: 100%|████████████████████████████████████| 56/56 [00:01<00:00, 44.43batch/s, lastLoss=0.19, valLoss=0.19]\n",
      "Epoch 40/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 40.39batch/s, lastLoss=0.191, valLoss=0.189]\n",
      "Epoch 41/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.82batch/s, lastLoss=0.19, valLoss=0.188]\n",
      "Epoch 42/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 46.07batch/s, lastLoss=0.191, valLoss=0.18]\n",
      "Epoch 43/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 43.45batch/s, lastLoss=0.19, valLoss=0.188]\n",
      "Epoch 44/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 44.71batch/s, lastLoss=0.19, valLoss=0.181]\n",
      "Epoch 45/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.28batch/s, lastLoss=0.19, valLoss=0.187]\n",
      "Epoch 46/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.79batch/s, lastLoss=0.19, valLoss=0.183]\n",
      "Epoch 47/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.16batch/s, lastLoss=0.19, valLoss=0.183]\n",
      "Epoch 48/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.76batch/s, lastLoss=0.189, valLoss=0.18]\n",
      "Epoch 49/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 42.92batch/s, lastLoss=0.191, valLoss=0.185]\n",
      "Epoch 50/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.32batch/s, lastLoss=0.189, valLoss=0.184]\n",
      "Epoch 51/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.11batch/s, lastLoss=0.189, valLoss=0.183]\n",
      "Epoch 52/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 43.97batch/s, lastLoss=0.19, valLoss=0.177]\n",
      "Epoch 53/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.23batch/s, lastLoss=0.189, valLoss=0.184]\n",
      "Epoch 54/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 43.40batch/s, lastLoss=0.189, valLoss=0.182]\n",
      "Epoch 55/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 46.22batch/s, lastLoss=0.19, valLoss=0.182]\n",
      "Epoch 56/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.57batch/s, lastLoss=0.189, valLoss=0.183]\n",
      "Epoch 57/400: 100%|████████████████████████████████████| 56/56 [00:01<00:00, 41.24batch/s, lastLoss=0.19, valLoss=0.18]\n",
      "Epoch 58/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.00batch/s, lastLoss=0.189, valLoss=0.186]\n",
      "Epoch 59/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.46batch/s, lastLoss=0.188, valLoss=0.187]\n",
      "Epoch 60/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 43.72batch/s, lastLoss=0.19, valLoss=0.186]\n",
      "Epoch 61/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.70batch/s, lastLoss=0.189, valLoss=0.188]\n",
      "Epoch 62/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 45.36batch/s, lastLoss=0.19, valLoss=0.182]\n",
      "Epoch 63/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.70batch/s, lastLoss=0.189, valLoss=0.186]\n",
      "Epoch 64/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 39.73batch/s, lastLoss=0.189, valLoss=0.179]\n",
      "Epoch 65/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.96batch/s, lastLoss=0.189, valLoss=0.195]\n",
      "Epoch 66/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 43.01batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 67/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.85batch/s, lastLoss=0.189, valLoss=0.183]\n",
      "Epoch 68/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.59batch/s, lastLoss=0.189, valLoss=0.184]\n",
      "Epoch 69/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.34batch/s, lastLoss=0.189, valLoss=0.182]\n",
      "Epoch 70/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.70batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 71/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.65batch/s, lastLoss=0.189, valLoss=0.186]\n",
      "Epoch 72/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.61batch/s, lastLoss=0.188, valLoss=0.191]\n",
      "Epoch 73/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 43.63batch/s, lastLoss=0.188, valLoss=0.184]\n",
      "Epoch 74/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.53batch/s, lastLoss=0.189, valLoss=0.179]\n",
      "Epoch 75/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.21batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 76/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.10batch/s, lastLoss=0.188, valLoss=0.186]\n",
      "Epoch 77/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.83batch/s, lastLoss=0.188, valLoss=0.187]\n",
      "Epoch 78/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.67batch/s, lastLoss=0.188, valLoss=0.188]\n",
      "Epoch 79/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.32batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 80/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 42.50batch/s, lastLoss=0.189, valLoss=0.178]\n",
      "Epoch 81/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.36batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 82/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 43.48batch/s, lastLoss=0.189, valLoss=0.187]\n",
      "Epoch 83/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 42.39batch/s, lastLoss=0.188, valLoss=0.179]\n",
      "Epoch 84/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.05batch/s, lastLoss=0.189, valLoss=0.185]\n",
      "Epoch 85/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 44.46batch/s, lastLoss=0.188, valLoss=0.18]\n",
      "Epoch 86/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.17batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 87/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.85batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 88/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.71batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 89/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.43batch/s, lastLoss=0.188, valLoss=0.181]\n",
      "Epoch 90/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 43.98batch/s, lastLoss=0.188, valLoss=0.188]\n",
      "Epoch 91/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.64batch/s, lastLoss=0.188, valLoss=0.181]\n",
      "Epoch 92/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.25batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 93/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.18batch/s, lastLoss=0.188, valLoss=0.178]\n",
      "Epoch 94/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.77batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 95/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.36batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 96/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.48batch/s, lastLoss=0.188, valLoss=0.184]\n",
      "Epoch 97/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 46.12batch/s, lastLoss=0.189, valLoss=0.18]\n",
      "Epoch 98/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.10batch/s, lastLoss=0.188, valLoss=0.181]\n",
      "Epoch 99/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.46batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 100/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.56batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 101/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.55batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 102/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.42batch/s, lastLoss=0.188, valLoss=0.181]\n",
      "Epoch 103/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.32batch/s, lastLoss=0.188, valLoss=0.178]\n",
      "Epoch 104/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.33batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 105/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.72batch/s, lastLoss=0.188, valLoss=0.177]\n",
      "Epoch 106/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.50batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 107/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.17batch/s, lastLoss=0.187, valLoss=0.187]\n",
      "Epoch 108/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.84batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 109/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.63batch/s, lastLoss=0.187, valLoss=0.188]\n",
      "Epoch 110/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.07batch/s, lastLoss=0.187, valLoss=0.187]\n",
      "Epoch 111/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.89batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 112/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.58batch/s, lastLoss=0.188, valLoss=0.184]\n",
      "Epoch 113/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.14batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 114/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.23batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 115/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.71batch/s, lastLoss=0.188, valLoss=0.187]\n",
      "Epoch 116/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.49batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 117/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.75batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 118/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.59batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 119/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.77batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 120/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.93batch/s, lastLoss=0.188, valLoss=0.184]\n",
      "Epoch 121/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.52batch/s, lastLoss=0.188, valLoss=0.182]\n",
      "Epoch 122/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.71batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 123/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.94batch/s, lastLoss=0.187, valLoss=0.185]\n",
      "Epoch 124/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.51batch/s, lastLoss=0.188, valLoss=0.189]\n",
      "Epoch 125/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.30batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 126/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.27batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 127/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.47batch/s, lastLoss=0.187, valLoss=0.175]\n",
      "Epoch 128/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.62batch/s, lastLoss=0.188, valLoss=0.184]\n",
      "Epoch 129/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.81batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 130/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.48batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 131/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.21batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 132/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.62batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 133/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.55batch/s, lastLoss=0.187, valLoss=0.176]\n",
      "Epoch 134/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.25batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 135/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.12batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 136/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.18batch/s, lastLoss=0.187, valLoss=0.183]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.29batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 138/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.56batch/s, lastLoss=0.187, valLoss=0.191]\n",
      "Epoch 139/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.47batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 140/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.17batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 141/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.74batch/s, lastLoss=0.187, valLoss=0.191]\n",
      "Epoch 142/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.53batch/s, lastLoss=0.187, valLoss=0.187]\n",
      "Epoch 143/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.43batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 144/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.64batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 145/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.79batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 146/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.44batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 147/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.65batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 148/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.03batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 149/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.80batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 150/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.75batch/s, lastLoss=0.187, valLoss=0.178]\n",
      "Epoch 151/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.41batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 152/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.79batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 153/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.81batch/s, lastLoss=0.187, valLoss=0.18]\n",
      "Epoch 154/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.88batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 155/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.65batch/s, lastLoss=0.187, valLoss=0.187]\n",
      "Epoch 156/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.75batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 157/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.24batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 158/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.70batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 159/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.95batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 160/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.95batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 161/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.35batch/s, lastLoss=0.187, valLoss=0.176]\n",
      "Epoch 162/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.45batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 163/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.09batch/s, lastLoss=0.187, valLoss=0.177]\n",
      "Epoch 164/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.55batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 165/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.48batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 166/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.34batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 167/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.69batch/s, lastLoss=0.187, valLoss=0.179]\n",
      "Epoch 168/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 47.01batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 169/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.88batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 170/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.08batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 171/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.11batch/s, lastLoss=0.187, valLoss=0.185]\n",
      "Epoch 172/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.72batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 173/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.59batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 174/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.55batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 175/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.55batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 176/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.43batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 177/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.50batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 178/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.94batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 179/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.42batch/s, lastLoss=0.187, valLoss=0.18]\n",
      "Epoch 180/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 42.99batch/s, lastLoss=0.187, valLoss=0.19]\n",
      "Epoch 181/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.60batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 182/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.30batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 183/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.31batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 184/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.77batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 185/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.40batch/s, lastLoss=0.187, valLoss=0.185]\n",
      "Epoch 186/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.45batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 187/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.53batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 188/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.91batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 189/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.92batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 190/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.83batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 191/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.15batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 192/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 47.34batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 193/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.62batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 194/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.66batch/s, lastLoss=0.187, valLoss=0.187]\n",
      "Epoch 195/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.91batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 196/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.53batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 197/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.50batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 198/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.44batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 199/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.28batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 200/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.61batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 201/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.79batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 202/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.33batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 203/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.30batch/s, lastLoss=0.187, valLoss=0.187]\n",
      "Epoch 204/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.24batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 205/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 40.22batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 206/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.58batch/s, lastLoss=0.187, valLoss=0.185]\n",
      "Epoch 207/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.87batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 208/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.59batch/s, lastLoss=0.187, valLoss=0.179]\n",
      "Epoch 209/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.83batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 210/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.91batch/s, lastLoss=0.187, valLoss=0.189]\n",
      "Epoch 211/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.18batch/s, lastLoss=0.187, valLoss=0.187]\n",
      "Epoch 212/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.77batch/s, lastLoss=0.187, valLoss=0.186]\n",
      "Epoch 213/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.89batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 214/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.54batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 215/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.96batch/s, lastLoss=0.187, valLoss=0.181]\n",
      "Epoch 216/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.32batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 217/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 40.64batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 218/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.12batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 219/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.64batch/s, lastLoss=0.186, valLoss=0.175]\n",
      "Epoch 220/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.14batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 221/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.15batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 222/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.13batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 223/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.23batch/s, lastLoss=0.186, valLoss=0.176]\n",
      "Epoch 224/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.06batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 225/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.21batch/s, lastLoss=0.187, valLoss=0.177]\n",
      "Epoch 226/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.96batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 227/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.35batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 228/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.19batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 229/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.37batch/s, lastLoss=0.187, valLoss=0.178]\n",
      "Epoch 230/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.09batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 231/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 40.39batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 232/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.87batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 233/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.42batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 234/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.54batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 235/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.33batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 236/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.46batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 237/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.36batch/s, lastLoss=0.187, valLoss=0.182]\n",
      "Epoch 238/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.41batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 239/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.90batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 240/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.51batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 241/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.44batch/s, lastLoss=0.187, valLoss=0.184]\n",
      "Epoch 242/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 45.71batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 243/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.12batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 244/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.15batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 245/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.52batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 246/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.06batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 247/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.93batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 248/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.03batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 249/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.49batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 250/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.93batch/s, lastLoss=0.186, valLoss=0.178]\n",
      "Epoch 251/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.95batch/s, lastLoss=0.186, valLoss=0.178]\n",
      "Epoch 252/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.83batch/s, lastLoss=0.186, valLoss=0.178]\n",
      "Epoch 253/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.02batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 254/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.27batch/s, lastLoss=0.186, valLoss=0.177]\n",
      "Epoch 255/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 47.42batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 256/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.70batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 257/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 46.59batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 258/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.21batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 259/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.06batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 260/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.77batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 261/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.69batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 262/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.20batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 263/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.71batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 264/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 47.27batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 265/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.03batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 266/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.05batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 267/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 47.12batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 268/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.43batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 269/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.58batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 270/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.89batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 271/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.03batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 272/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.71batch/s, lastLoss=0.185, valLoss=0.184]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 273/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.80batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 274/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 35.76batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 275/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 34.64batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 276/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.97batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 277/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.05batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 278/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 34.58batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 279/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.04batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 280/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.63batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 281/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.33batch/s, lastLoss=0.186, valLoss=0.194]\n",
      "Epoch 282/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 34.61batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 283/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.66batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 284/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 38.92batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 285/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 38.63batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 286/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.67batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 287/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.30batch/s, lastLoss=0.187, valLoss=0.188]\n",
      "Epoch 288/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.53batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 289/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.76batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 290/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.20batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 291/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 35.82batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 292/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.31batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 293/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.02batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 294/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.67batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 295/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 39.23batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 296/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 34.48batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 297/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 38.13batch/s, lastLoss=0.186, valLoss=0.178]\n",
      "Epoch 298/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.94batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 299/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.99batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 300/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.28batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 301/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.17batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 302/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.93batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 303/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.76batch/s, lastLoss=0.186, valLoss=0.177]\n",
      "Epoch 304/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.06batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 305/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.03batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 306/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.43batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 307/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.53batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 308/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.78batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 309/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.96batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 310/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.51batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 311/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 40.88batch/s, lastLoss=0.185, valLoss=0.183]\n",
      "Epoch 312/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.84batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 313/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.19batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 314/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.33batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 315/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.61batch/s, lastLoss=0.186, valLoss=0.178]\n",
      "Epoch 316/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.68batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 317/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 40.91batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 318/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 39.90batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 319/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.01batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 320/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 40.56batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 321/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.73batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 322/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.01batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 323/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.12batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 324/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 40.73batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 325/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.04batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 326/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.71batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 327/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.34batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 328/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.90batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 329/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.97batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 330/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.48batch/s, lastLoss=0.186, valLoss=0.177]\n",
      "Epoch 331/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.56batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 332/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 40.92batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 333/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.00batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 334/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.07batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 335/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.36batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 336/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 38.70batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 337/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.39batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 338/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.31batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 339/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 40.04batch/s, lastLoss=0.185, valLoss=0.181]\n",
      "Epoch 340/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.50batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 341/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.67batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 342/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.42batch/s, lastLoss=0.185, valLoss=0.182]\n",
      "Epoch 343/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.99batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 344/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.43batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 345/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.84batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 346/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.16batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 347/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.14batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 348/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.73batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 349/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 43.38batch/s, lastLoss=0.186, valLoss=0.19]\n",
      "Epoch 350/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.03batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 351/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.99batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 352/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.65batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 353/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.11batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 354/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.04batch/s, lastLoss=0.186, valLoss=0.186]\n",
      "Epoch 355/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.17batch/s, lastLoss=0.186, valLoss=0.189]\n",
      "Epoch 356/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.71batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 357/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 43.01batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 358/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.65batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 359/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.16batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 360/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.94batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 361/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.89batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 362/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.06batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 363/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.06batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 364/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.39batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 365/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.73batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 366/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.81batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 367/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.43batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 368/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.27batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 369/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.76batch/s, lastLoss=0.186, valLoss=0.179]\n",
      "Epoch 370/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.85batch/s, lastLoss=0.186, valLoss=0.178]\n",
      "Epoch 371/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.70batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 372/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.92batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 373/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.25batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 374/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 42.06batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 375/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.46batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 376/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 43.77batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 377/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.12batch/s, lastLoss=0.186, valLoss=0.191]\n",
      "Epoch 378/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.15batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 379/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.56batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 380/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.74batch/s, lastLoss=0.185, valLoss=0.184]\n",
      "Epoch 381/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.48batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 382/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.37batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 383/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 39.42batch/s, lastLoss=0.186, valLoss=0.185]\n",
      "Epoch 384/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 40.81batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 385/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.33batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 386/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.18batch/s, lastLoss=0.186, valLoss=0.177]\n",
      "Epoch 387/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.31batch/s, lastLoss=0.186, valLoss=0.177]\n",
      "Epoch 388/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 41.44batch/s, lastLoss=0.186, valLoss=0.184]\n",
      "Epoch 389/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 44.78batch/s, lastLoss=0.186, valLoss=0.18]\n",
      "Epoch 390/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.85batch/s, lastLoss=0.186, valLoss=0.188]\n",
      "Epoch 391/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.39batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 392/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.24batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 393/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.66batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 394/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.31batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 395/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 46.48batch/s, lastLoss=0.186, valLoss=0.182]\n",
      "Epoch 396/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.98batch/s, lastLoss=0.186, valLoss=0.187]\n",
      "Epoch 397/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.51batch/s, lastLoss=0.186, valLoss=0.183]\n",
      "Epoch 398/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 44.54batch/s, lastLoss=0.186, valLoss=0.181]\n",
      "Epoch 399/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.45batch/s, lastLoss=0.186, valLoss=0.177]\n",
      "Epoch 400/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 45.86batch/s, lastLoss=0.186, valLoss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.18793364834173448\n",
      "Average validation loss: 0.18375265919479233\n"
     ]
    }
   ],
   "source": [
    "lstm_train_loss, lstm_val_loss =  train(train_loader, lstm, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f28a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnSingle = RNNSingle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e748ee61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:04<00:00, 12.46batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.29batch/s, lastLoss=0.264, valLoss=0.204]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.31batch/s, lastLoss=0.237, valLoss=0.201]\n",
      "Epoch 4/400: 100%|████████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.228, valLoss=0.21]\n",
      "Epoch 5/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.28batch/s, lastLoss=0.223, valLoss=0.192]\n",
      "Epoch 6/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.29batch/s, lastLoss=0.218, valLoss=0.192]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.28batch/s, lastLoss=0.214, valLoss=0.192]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.212, valLoss=0.197]\n",
      "Epoch 9/400: 100%|████████████████████████████████████| 56/56 [00:04<00:00, 13.29batch/s, lastLoss=0.21, valLoss=0.194]\n",
      "Epoch 10/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.209, valLoss=0.196]\n",
      "Epoch 11/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.28batch/s, lastLoss=0.207, valLoss=0.194]\n",
      "Epoch 12/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.204, valLoss=0.187]\n",
      "Epoch 13/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.203, valLoss=0.198]\n",
      "Epoch 14/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.29batch/s, lastLoss=0.205, valLoss=0.194]\n",
      "Epoch 15/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.29batch/s, lastLoss=0.201, valLoss=0.184]\n",
      "Epoch 16/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.202, valLoss=0.195]\n",
      "Epoch 17/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.201, valLoss=0.19]\n",
      "Epoch 18/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.199, valLoss=0.19]\n",
      "Epoch 19/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.199, valLoss=0.19]\n",
      "Epoch 20/400: 100%|████████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.2, valLoss=0.192]\n",
      "Epoch 21/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.199, valLoss=0.187]\n",
      "Epoch 22/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.196, valLoss=0.192]\n",
      "Epoch 23/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.195, valLoss=0.183]\n",
      "Epoch 24/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.192, valLoss=0.178]\n",
      "Epoch 25/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.19, valLoss=0.186]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.187, valLoss=0.177]\n",
      "Epoch 27/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.184, valLoss=0.159]\n",
      "Epoch 28/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.182, valLoss=0.168]\n",
      "Epoch 29/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.176, valLoss=0.164]\n",
      "Epoch 30/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.173, valLoss=0.147]\n",
      "Epoch 31/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.171, valLoss=0.144]\n",
      "Epoch 32/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.167, valLoss=0.145]\n",
      "Epoch 33/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.165, valLoss=0.145]\n",
      "Epoch 34/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.162, valLoss=0.144]\n",
      "Epoch 35/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.161, valLoss=0.135]\n",
      "Epoch 36/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.159, valLoss=0.145]\n",
      "Epoch 37/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.156, valLoss=0.143]\n",
      "Epoch 38/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.158, valLoss=0.133]\n",
      "Epoch 39/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.155, valLoss=0.137]\n",
      "Epoch 40/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.153, valLoss=0.134]\n",
      "Epoch 41/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.153, valLoss=0.136]\n",
      "Epoch 42/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.152, valLoss=0.129]\n",
      "Epoch 43/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.152, valLoss=0.129]\n",
      "Epoch 44/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.149, valLoss=0.134]\n",
      "Epoch 45/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.148, valLoss=0.118]\n",
      "Epoch 46/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.149, valLoss=0.129]\n",
      "Epoch 47/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.147, valLoss=0.124]\n",
      "Epoch 48/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.148, valLoss=0.123]\n",
      "Epoch 49/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.146, valLoss=0.128]\n",
      "Epoch 50/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.143, valLoss=0.116]\n",
      "Epoch 51/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.145, valLoss=0.126]\n",
      "Epoch 52/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.144, valLoss=0.13]\n",
      "Epoch 53/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.143, valLoss=0.129]\n",
      "Epoch 54/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.141, valLoss=0.126]\n",
      "Epoch 55/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.142, valLoss=0.124]\n",
      "Epoch 56/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.144, valLoss=0.127]\n",
      "Epoch 57/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.141, valLoss=0.122]\n",
      "Epoch 58/400: 100%|████████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.14, valLoss=0.12]\n",
      "Epoch 59/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.14, valLoss=0.119]\n",
      "Epoch 60/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.141, valLoss=0.127]\n",
      "Epoch 61/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.138, valLoss=0.124]\n",
      "Epoch 62/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.138, valLoss=0.121]\n",
      "Epoch 63/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.137, valLoss=0.121]\n",
      "Epoch 64/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.138, valLoss=0.123]\n",
      "Epoch 65/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.138, valLoss=0.123]\n",
      "Epoch 66/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.137, valLoss=0.118]\n",
      "Epoch 67/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.135, valLoss=0.118]\n",
      "Epoch 68/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.136, valLoss=0.118]\n",
      "Epoch 69/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.135, valLoss=0.12]\n",
      "Epoch 70/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.135, valLoss=0.119]\n",
      "Epoch 71/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.135, valLoss=0.114]\n",
      "Epoch 72/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.134, valLoss=0.128]\n",
      "Epoch 73/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.135, valLoss=0.123]\n",
      "Epoch 74/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.134, valLoss=0.117]\n",
      "Epoch 75/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.134, valLoss=0.118]\n",
      "Epoch 76/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.135, valLoss=0.116]\n",
      "Epoch 77/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.134, valLoss=0.119]\n",
      "Epoch 78/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.133, valLoss=0.125]\n",
      "Epoch 79/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.132, valLoss=0.118]\n",
      "Epoch 80/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.13, valLoss=0.114]\n",
      "Epoch 81/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.13, valLoss=0.119]\n",
      "Epoch 82/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.28batch/s, lastLoss=0.131, valLoss=0.115]\n",
      "Epoch 83/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.131, valLoss=0.122]\n",
      "Epoch 84/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.132, valLoss=0.119]\n",
      "Epoch 85/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.131, valLoss=0.121]\n",
      "Epoch 86/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.28batch/s, lastLoss=0.13, valLoss=0.123]\n",
      "Epoch 87/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.13, valLoss=0.117]\n",
      "Epoch 88/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.129, valLoss=0.122]\n",
      "Epoch 89/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.13, valLoss=0.116]\n",
      "Epoch 90/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.129, valLoss=0.116]\n",
      "Epoch 91/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.129, valLoss=0.121]\n",
      "Epoch 92/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.128, valLoss=0.115]\n",
      "Epoch 93/400: 100%|███████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.13, valLoss=0.125]\n",
      "Epoch 94/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.129, valLoss=0.128]\n",
      "Epoch 95/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.128, valLoss=0.116]\n",
      "Epoch 96/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.128, valLoss=0.116]\n",
      "Epoch 97/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.128, valLoss=0.111]\n",
      "Epoch 98/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.128, valLoss=0.111]\n",
      "Epoch 99/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.128, valLoss=0.118]\n",
      "Epoch 100/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.127, valLoss=0.114]\n",
      "Epoch 101/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.127, valLoss=0.116]\n",
      "Epoch 102/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.127, valLoss=0.112]\n",
      "Epoch 103/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.127, valLoss=0.119]\n",
      "Epoch 104/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.126, valLoss=0.114]\n",
      "Epoch 105/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.127, valLoss=0.117]\n",
      "Epoch 106/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.126, valLoss=0.111]\n",
      "Epoch 107/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.126, valLoss=0.122]\n",
      "Epoch 108/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.127, valLoss=0.115]\n",
      "Epoch 109/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.126, valLoss=0.111]\n",
      "Epoch 110/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.126, valLoss=0.108]\n",
      "Epoch 111/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.125, valLoss=0.113]\n",
      "Epoch 112/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.126, valLoss=0.116]\n",
      "Epoch 113/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.124, valLoss=0.116]\n",
      "Epoch 114/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.124, valLoss=0.117]\n",
      "Epoch 115/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.125, valLoss=0.116]\n",
      "Epoch 116/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.124, valLoss=0.108]\n",
      "Epoch 117/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.126, valLoss=0.117]\n",
      "Epoch 118/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.123, valLoss=0.113]\n",
      "Epoch 119/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.124, valLoss=0.108]\n",
      "Epoch 120/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.124, valLoss=0.11]\n",
      "Epoch 121/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.124, valLoss=0.116]\n",
      "Epoch 122/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.123, valLoss=0.12]\n",
      "Epoch 123/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.124, valLoss=0.114]\n",
      "Epoch 124/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.123, valLoss=0.109]\n",
      "Epoch 125/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.122, valLoss=0.11]\n",
      "Epoch 126/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.124, valLoss=0.103]\n",
      "Epoch 127/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.124, valLoss=0.113]\n",
      "Epoch 128/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.123, valLoss=0.112]\n",
      "Epoch 129/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.123, valLoss=0.11]\n",
      "Epoch 130/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.123, valLoss=0.114]\n",
      "Epoch 131/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.123, valLoss=0.109]\n",
      "Epoch 132/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.122, valLoss=0.12]\n",
      "Epoch 133/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.123, valLoss=0.113]\n",
      "Epoch 134/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.122, valLoss=0.11]\n",
      "Epoch 135/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.123, valLoss=0.113]\n",
      "Epoch 136/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.122, valLoss=0.113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.123, valLoss=0.108]\n",
      "Epoch 138/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.122, valLoss=0.113]\n",
      "Epoch 139/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.122, valLoss=0.113]\n",
      "Epoch 140/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.123, valLoss=0.114]\n",
      "Epoch 141/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.122, valLoss=0.114]\n",
      "Epoch 142/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.122, valLoss=0.114]\n",
      "Epoch 143/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.123, valLoss=0.112]\n",
      "Epoch 144/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.121, valLoss=0.115]\n",
      "Epoch 145/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.121, valLoss=0.109]\n",
      "Epoch 146/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.121, valLoss=0.114]\n",
      "Epoch 147/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.122, valLoss=0.112]\n",
      "Epoch 148/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.122, valLoss=0.119]\n",
      "Epoch 149/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.121, valLoss=0.112]\n",
      "Epoch 150/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.122, valLoss=0.108]\n",
      "Epoch 151/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.122, valLoss=0.115]\n",
      "Epoch 152/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.121, valLoss=0.115]\n",
      "Epoch 153/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.121, valLoss=0.111]\n",
      "Epoch 154/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.12, valLoss=0.109]\n",
      "Epoch 155/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.12, valLoss=0.113]\n",
      "Epoch 156/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.121, valLoss=0.106]\n",
      "Epoch 157/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.29batch/s, lastLoss=0.121, valLoss=0.111]\n",
      "Epoch 158/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.121, valLoss=0.111]\n",
      "Epoch 159/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.121, valLoss=0.115]\n",
      "Epoch 160/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.12, valLoss=0.116]\n",
      "Epoch 161/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.12, valLoss=0.112]\n",
      "Epoch 162/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.12, valLoss=0.108]\n",
      "Epoch 163/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.29batch/s, lastLoss=0.12, valLoss=0.112]\n",
      "Epoch 164/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.121, valLoss=0.111]\n",
      "Epoch 165/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.122, valLoss=0.114]\n",
      "Epoch 166/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.12, valLoss=0.116]\n",
      "Epoch 167/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.12, valLoss=0.111]\n",
      "Epoch 168/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.121, valLoss=0.113]\n",
      "Epoch 169/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.121, valLoss=0.109]\n",
      "Epoch 170/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.119, valLoss=0.117]\n",
      "Epoch 171/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.12, valLoss=0.109]\n",
      "Epoch 172/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.12, valLoss=0.111]\n",
      "Epoch 173/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.121, valLoss=0.112]\n",
      "Epoch 174/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.12, valLoss=0.113]\n",
      "Epoch 175/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.12, valLoss=0.107]\n",
      "Epoch 176/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.121, valLoss=0.11]\n",
      "Epoch 177/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.12, valLoss=0.117]\n",
      "Epoch 178/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.119, valLoss=0.108]\n",
      "Epoch 179/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.119, valLoss=0.11]\n",
      "Epoch 180/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.119, valLoss=0.112]\n",
      "Epoch 181/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.12, valLoss=0.108]\n",
      "Epoch 182/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.23batch/s, lastLoss=0.119, valLoss=0.108]\n",
      "Epoch 183/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.119, valLoss=0.108]\n",
      "Epoch 184/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.119, valLoss=0.107]\n",
      "Epoch 185/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.119, valLoss=0.11]\n",
      "Epoch 186/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.12, valLoss=0.114]\n",
      "Epoch 187/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.118, valLoss=0.116]\n",
      "Epoch 188/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.119, valLoss=0.118]\n",
      "Epoch 189/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.12, valLoss=0.109]\n",
      "Epoch 190/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.119, valLoss=0.106]\n",
      "Epoch 191/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.119, valLoss=0.111]\n",
      "Epoch 192/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.119, valLoss=0.112]\n",
      "Epoch 193/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.118, valLoss=0.116]\n",
      "Epoch 194/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.119, valLoss=0.109]\n",
      "Epoch 195/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.24batch/s, lastLoss=0.12, valLoss=0.111]\n",
      "Epoch 196/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.27batch/s, lastLoss=0.118, valLoss=0.109]\n",
      "Epoch 197/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.26batch/s, lastLoss=0.118, valLoss=0.108]\n",
      "Epoch 198/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.118, valLoss=0.109]\n",
      "Epoch 199/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.25batch/s, lastLoss=0.118, valLoss=0.115]\n",
      "Epoch 200/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.118, valLoss=0.112]\n",
      "Epoch 201/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.119, valLoss=0.113]\n",
      "Epoch 202/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.118, valLoss=0.112]\n",
      "Epoch 203/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.119, valLoss=0.112]\n",
      "Epoch 204/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.118, valLoss=0.112]\n",
      "Epoch 205/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.117, valLoss=0.11]\n",
      "Epoch 206/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.12, valLoss=0.108]\n",
      "Epoch 207/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.118, valLoss=0.116]\n",
      "Epoch 208/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.117, valLoss=0.107]\n",
      "Epoch 209/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.118, valLoss=0.112]\n",
      "Epoch 210/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.117, valLoss=0.111]\n",
      "Epoch 211/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.118, valLoss=0.111]\n",
      "Epoch 212/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.118, valLoss=0.112]\n",
      "Epoch 213/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.118, valLoss=0.106]\n",
      "Epoch 214/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.13batch/s, lastLoss=0.116, valLoss=0.109]\n",
      "Epoch 215/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.118, valLoss=0.106]\n",
      "Epoch 216/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.118, valLoss=0.11]\n",
      "Epoch 217/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.118, valLoss=0.117]\n",
      "Epoch 218/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.118, valLoss=0.115]\n",
      "Epoch 219/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.118, valLoss=0.111]\n",
      "Epoch 220/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.117, valLoss=0.11]\n",
      "Epoch 221/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.117, valLoss=0.113]\n",
      "Epoch 222/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.117, valLoss=0.109]\n",
      "Epoch 223/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.118, valLoss=0.108]\n",
      "Epoch 224/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.12batch/s, lastLoss=0.118, valLoss=0.108]\n",
      "Epoch 225/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.118, valLoss=0.108]\n",
      "Epoch 226/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.117, valLoss=0.109]\n",
      "Epoch 227/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.118, valLoss=0.111]\n",
      "Epoch 228/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.117, valLoss=0.109]\n",
      "Epoch 229/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.117, valLoss=0.113]\n",
      "Epoch 230/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.117, valLoss=0.109]\n",
      "Epoch 231/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 232/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.116, valLoss=0.115]\n",
      "Epoch 233/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.117, valLoss=0.107]\n",
      "Epoch 234/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 235/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.117, valLoss=0.11]\n",
      "Epoch 236/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.117, valLoss=0.107]\n",
      "Epoch 237/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.117, valLoss=0.112]\n",
      "Epoch 238/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 239/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.117, valLoss=0.113]\n",
      "Epoch 240/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.117, valLoss=0.11]\n",
      "Epoch 241/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.13batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 242/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.118, valLoss=0.111]\n",
      "Epoch 243/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 244/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.118, valLoss=0.113]\n",
      "Epoch 245/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.116, valLoss=0.107]\n",
      "Epoch 246/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 247/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.116, valLoss=0.116]\n",
      "Epoch 248/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.116, valLoss=0.111]\n",
      "Epoch 249/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 250/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.117, valLoss=0.109]\n",
      "Epoch 251/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.117, valLoss=0.11]\n",
      "Epoch 252/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.13batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 253/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.117, valLoss=0.108]\n",
      "Epoch 254/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.116, valLoss=0.109]\n",
      "Epoch 255/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.13batch/s, lastLoss=0.116, valLoss=0.117]\n",
      "Epoch 256/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.116, valLoss=0.106]\n",
      "Epoch 257/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 258/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.08batch/s, lastLoss=0.116, valLoss=0.107]\n",
      "Epoch 259/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.117, valLoss=0.111]\n",
      "Epoch 260/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.116, valLoss=0.109]\n",
      "Epoch 261/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.117, valLoss=0.11]\n",
      "Epoch 262/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.117, valLoss=0.104]\n",
      "Epoch 263/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 264/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.116, valLoss=0.106]\n",
      "Epoch 265/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.115, valLoss=0.104]\n",
      "Epoch 266/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 267/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.116, valLoss=0.11]\n",
      "Epoch 268/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.13batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 269/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.116, valLoss=0.119]\n",
      "Epoch 270/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 271/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.117, valLoss=0.109]\n",
      "Epoch 272/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.116, valLoss=0.109]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 273/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 274/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.106]\n",
      "Epoch 275/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 276/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 277/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.108]\n",
      "Epoch 278/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.08batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 279/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 280/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.116, valLoss=0.103]\n",
      "Epoch 281/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 282/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.116, valLoss=0.111]\n",
      "Epoch 283/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.116, valLoss=0.106]\n",
      "Epoch 284/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.117, valLoss=0.109]\n",
      "Epoch 285/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.114, valLoss=0.112]\n",
      "Epoch 286/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 287/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 288/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 289/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.115, valLoss=0.114]\n",
      "Epoch 290/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 291/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 292/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 293/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 294/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.116, valLoss=0.109]\n",
      "Epoch 295/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.116, valLoss=0.115]\n",
      "Epoch 296/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 297/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.116, valLoss=0.111]\n",
      "Epoch 298/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 299/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.114, valLoss=0.116]\n",
      "Epoch 300/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 301/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 302/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 303/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 304/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.11batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 305/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.114, valLoss=0.105]\n",
      "Epoch 306/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 307/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 308/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 309/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 310/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.113, valLoss=0.105]\n",
      "Epoch 311/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.114, valLoss=0.105]\n",
      "Epoch 312/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 313/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.114, valLoss=0.109]\n",
      "Epoch 314/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 315/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.115, valLoss=0.114]\n",
      "Epoch 316/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 317/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.114, valLoss=0.111]\n",
      "Epoch 318/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.114, valLoss=0.114]\n",
      "Epoch 319/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 320/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.113, valLoss=0.112]\n",
      "Epoch 321/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.03batch/s, lastLoss=0.114, valLoss=0.111]\n",
      "Epoch 322/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.12batch/s, lastLoss=0.114, valLoss=0.102]\n",
      "Epoch 323/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 324/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.11batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 325/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.115, valLoss=0.103]\n",
      "Epoch 326/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.13batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 327/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.115, valLoss=0.106]\n",
      "Epoch 328/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.113, valLoss=0.109]\n",
      "Epoch 329/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.114, valLoss=0.114]\n",
      "Epoch 330/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.114, valLoss=0.106]\n",
      "Epoch 331/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.11batch/s, lastLoss=0.114, valLoss=0.103]\n",
      "Epoch 332/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.113, valLoss=0.106]\n",
      "Epoch 333/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.114, valLoss=0.105]\n",
      "Epoch 334/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.113, valLoss=0.112]\n",
      "Epoch 335/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.114, valLoss=0.105]\n",
      "Epoch 336/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.14batch/s, lastLoss=0.114, valLoss=0.106]\n",
      "Epoch 337/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 12.82batch/s, lastLoss=0.114, valLoss=0.106]\n",
      "Epoch 338/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 12.75batch/s, lastLoss=0.113, valLoss=0.107]\n",
      "Epoch 339/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.09batch/s, lastLoss=0.114, valLoss=0.107]\n",
      "Epoch 340/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.10batch/s, lastLoss=0.114, valLoss=0.11]\n",
      "Epoch 341/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.10batch/s, lastLoss=0.114, valLoss=0.109]\n",
      "Epoch 342/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.09batch/s, lastLoss=0.113, valLoss=0.106]\n",
      "Epoch 343/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.10batch/s, lastLoss=0.113, valLoss=0.107]\n",
      "Epoch 344/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.114, valLoss=0.107]\n",
      "Epoch 345/400: 100%|██████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.113, valLoss=0.11]\n",
      "Epoch 346/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.113, valLoss=0.105]\n",
      "Epoch 347/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.113, valLoss=0.101]\n",
      "Epoch 348/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.113, valLoss=0.107]\n",
      "Epoch 349/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.114, valLoss=0.108]\n",
      "Epoch 350/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.21batch/s, lastLoss=0.115, valLoss=0.108]\n",
      "Epoch 351/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.113, valLoss=0.114]\n",
      "Epoch 352/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.17batch/s, lastLoss=0.113, valLoss=0.106]\n",
      "Epoch 353/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.112, valLoss=0.106]\n",
      "Epoch 354/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.114, valLoss=0.107]\n",
      "Epoch 355/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.114, valLoss=0.107]\n",
      "Epoch 356/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.16batch/s, lastLoss=0.114, valLoss=0.109]\n",
      "Epoch 357/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.18batch/s, lastLoss=0.113, valLoss=0.107]\n",
      "Epoch 358/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.20batch/s, lastLoss=0.113, valLoss=0.103]\n",
      "Epoch 359/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.113, valLoss=0.108]\n",
      "Epoch 360/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.19batch/s, lastLoss=0.113, valLoss=0.103]\n",
      "Epoch 361/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.15batch/s, lastLoss=0.114, valLoss=0.112]\n",
      "Epoch 362/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.22batch/s, lastLoss=0.113, valLoss=0.108]\n",
      "Epoch 363/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.00batch/s, lastLoss=0.112, valLoss=0.105]\n",
      "Epoch 364/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 13.11batch/s, lastLoss=0.112, valLoss=0.102]\n",
      "Epoch 365/400: 100%|█████████████████████████████████| 56/56 [00:04<00:00, 12.94batch/s, lastLoss=0.114, valLoss=0.111]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rnnSingle_train_loss, rnnSingle_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnnSingle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_loader):\n\u001b[0;32m     42\u001b[0m     inp, out \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 44\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     46\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36mRNNSingle.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m h0 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, h0)\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnnSingle_train_loss, rnnSingle_val_loss =  train(train_loader, rnnSingle, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61774676",
   "metadata": {},
   "source": [
    "Using the raw coefficients doesnt seem to be effective at any level of decomposition:\n",
    "- The CNN + RNN performs slightly worse with the wavelet packet decomposition\n",
    "- Using some levels\n",
    "- Found an RNN alone that was able to perform a little worse than CNN + RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ecee6",
   "metadata": {},
   "source": [
    "# Try not flattenning coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "6d85d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(signals, wavelet='db4', level=3):\n",
    "    \"\"\"\n",
    "    Extract wavelet packet features for each signal in the input array.\n",
    "\n",
    "    Args:\n",
    "    - signals (numpy.ndarray): a 2D array containing two 1D signals\n",
    "    - wavelet (str): the name of the wavelet to use for decomposition\n",
    "    - level (int): the level of wavelet packet decomposition to use\n",
    "\n",
    "    Returns:\n",
    "    - features (dict): a dictionary containing the wavelet packet features for each signal\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionary to store features for each signal\n",
    "    features = []\n",
    "\n",
    "    # Loop over each signal\n",
    "    for i in range(2):\n",
    "        # Perform wavelet packet decomposition\n",
    "        coeffs = pywt.WaveletPacket(data=signals[i], wavelet=wavelet, mode='symmetric', maxlevel=level).get_level(level, 'freq')\n",
    "\n",
    "        # Vectorize the coefficients\n",
    "        coefficients = np.array([c.data for c in coeffs], dtype=object)\n",
    "\n",
    "        # Store the coefficients as features for this signal\n",
    "        features.append(coefficients.tolist())\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a2c66970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:09<00:00, 241.50it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, scaler = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8b60b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = getSplit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c9c7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSingle(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, num_layers=3, output_dim=6):\n",
    "        super(RNNSingle, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(45056, 1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, 2)\n",
    "        \n",
    "        h0 = torch.zeros(3, batch_size, 256).to(x.device)\n",
    "        x, _ = self.rnn(x, h0)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, num_layers=1, output_dim=6):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 6)\n",
    "        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, 2)\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Initialize cell state with zeros\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Move tensors to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "            x = x.cuda()\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "beac523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRNN(nn.Module):\n",
    "    def __init__(self, input_size=22, hidden_size=10):\n",
    "        super(SiameseRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 6),\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x1 = x[:, 0, :, :]\n",
    "        x2 = x[:, 0, :, :]\n",
    "        \n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n",
    "        _, h1 = self.rnn(x1, h0)  # Add a batch dimension\n",
    "        _, h2 = self.rnn(x2, h0)  # Add a batch dimension\n",
    "        \n",
    "        out = torch.cat([h1[-1], h2[-1]], dim=1)\n",
    "        out = out.reshape(batch_size, -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ad209c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 22])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "24adb747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sRNN = SiameseRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fdb61fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|█████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 191.45batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.195, valLoss=0.18]\n",
      "Epoch 3/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 188.34batch/s, lastLoss=0.182, valLoss=0.166]\n",
      "Epoch 4/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 186.05batch/s, lastLoss=0.17, valLoss=0.165]\n",
      "Epoch 5/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 199.29batch/s, lastLoss=0.169, valLoss=0.161]\n",
      "Epoch 6/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 206.63batch/s, lastLoss=0.168, valLoss=0.167]\n",
      "Epoch 7/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 201.44batch/s, lastLoss=0.168, valLoss=0.163]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 199.29batch/s, lastLoss=0.168, valLoss=0.17]\n",
      "Epoch 9/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 199.28batch/s, lastLoss=0.168, valLoss=0.164]\n",
      "Epoch 10/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.81batch/s, lastLoss=0.168, valLoss=0.172]\n",
      "Epoch 11/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 201.07batch/s, lastLoss=0.167, valLoss=0.168]\n",
      "Epoch 12/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.15batch/s, lastLoss=0.166, valLoss=0.166]\n",
      "Epoch 13/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.38batch/s, lastLoss=0.166, valLoss=0.165]\n",
      "Epoch 14/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.12batch/s, lastLoss=0.166, valLoss=0.162]\n",
      "Epoch 15/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.01batch/s, lastLoss=0.167, valLoss=0.163]\n",
      "Epoch 16/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 206.25batch/s, lastLoss=0.166, valLoss=0.178]\n",
      "Epoch 17/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 210.13batch/s, lastLoss=0.166, valLoss=0.16]\n",
      "Epoch 18/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.164, valLoss=0.159]\n",
      "Epoch 19/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 199.43batch/s, lastLoss=0.162, valLoss=0.156]\n",
      "Epoch 20/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 214.56batch/s, lastLoss=0.16, valLoss=0.161]\n",
      "Epoch 21/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 208.95batch/s, lastLoss=0.16, valLoss=0.153]\n",
      "Epoch 22/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 208.56batch/s, lastLoss=0.16, valLoss=0.162]\n",
      "Epoch 23/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 209.73batch/s, lastLoss=0.158, valLoss=0.155]\n",
      "Epoch 24/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.33batch/s, lastLoss=0.157, valLoss=0.151]\n",
      "Epoch 25/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 211.72batch/s, lastLoss=0.157, valLoss=0.148]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.158, valLoss=0.15]\n",
      "Epoch 27/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.28batch/s, lastLoss=0.156, valLoss=0.147]\n",
      "Epoch 28/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.94batch/s, lastLoss=0.156, valLoss=0.153]\n",
      "Epoch 29/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 204.01batch/s, lastLoss=0.155, valLoss=0.15]\n",
      "Epoch 30/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 211.71batch/s, lastLoss=0.157, valLoss=0.164]\n",
      "Epoch 31/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 211.32batch/s, lastLoss=0.156, valLoss=0.147]\n",
      "Epoch 32/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.52batch/s, lastLoss=0.154, valLoss=0.155]\n",
      "Epoch 33/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 206.64batch/s, lastLoss=0.154, valLoss=0.151]\n",
      "Epoch 34/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.56batch/s, lastLoss=0.155, valLoss=0.149]\n",
      "Epoch 35/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 206.26batch/s, lastLoss=0.153, valLoss=0.149]\n",
      "Epoch 36/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 204.75batch/s, lastLoss=0.153, valLoss=0.153]\n",
      "Epoch 37/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 197.88batch/s, lastLoss=0.152, valLoss=0.151]\n",
      "Epoch 38/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.74batch/s, lastLoss=0.153, valLoss=0.145]\n",
      "Epoch 39/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 207.42batch/s, lastLoss=0.152, valLoss=0.157]\n",
      "Epoch 40/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.52batch/s, lastLoss=0.152, valLoss=0.153]\n",
      "Epoch 41/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.54batch/s, lastLoss=0.152, valLoss=0.152]\n",
      "Epoch 42/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 204.75batch/s, lastLoss=0.152, valLoss=0.151]\n",
      "Epoch 43/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 207.79batch/s, lastLoss=0.154, valLoss=0.154]\n",
      "Epoch 44/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 209.74batch/s, lastLoss=0.151, valLoss=0.146]\n",
      "Epoch 45/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 201.08batch/s, lastLoss=0.148, valLoss=0.152]\n",
      "Epoch 46/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 209.33batch/s, lastLoss=0.15, valLoss=0.157]\n",
      "Epoch 47/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 207.02batch/s, lastLoss=0.148, valLoss=0.151]\n",
      "Epoch 48/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.55batch/s, lastLoss=0.149, valLoss=0.149]\n",
      "Epoch 49/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.54batch/s, lastLoss=0.148, valLoss=0.143]\n",
      "Epoch 50/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.98batch/s, lastLoss=0.148, valLoss=0.149]\n",
      "Epoch 51/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 207.02batch/s, lastLoss=0.148, valLoss=0.143]\n",
      "Epoch 52/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 206.25batch/s, lastLoss=0.145, valLoss=0.146]\n",
      "Epoch 53/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 221.34batch/s, lastLoss=0.146, valLoss=0.144]\n",
      "Epoch 54/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.52batch/s, lastLoss=0.145, valLoss=0.144]\n",
      "Epoch 55/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.48batch/s, lastLoss=0.146, valLoss=0.147]\n",
      "Epoch 56/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 208.94batch/s, lastLoss=0.145, valLoss=0.145]\n",
      "Epoch 57/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 207.02batch/s, lastLoss=0.144, valLoss=0.145]\n",
      "Epoch 58/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.144, valLoss=0.153]\n",
      "Epoch 59/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 201.82batch/s, lastLoss=0.143, valLoss=0.151]\n",
      "Epoch 60/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.96batch/s, lastLoss=0.145, valLoss=0.136]\n",
      "Epoch 61/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.143, valLoss=0.146]\n",
      "Epoch 62/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 199.99batch/s, lastLoss=0.142, valLoss=0.141]\n",
      "Epoch 63/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.53batch/s, lastLoss=0.143, valLoss=0.137]\n",
      "Epoch 64/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 208.97batch/s, lastLoss=0.143, valLoss=0.139]\n",
      "Epoch 65/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.145, valLoss=0.147]\n",
      "Epoch 66/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 205.51batch/s, lastLoss=0.143, valLoss=0.145]\n",
      "Epoch 67/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.55batch/s, lastLoss=0.143, valLoss=0.144]\n",
      "Epoch 68/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.52batch/s, lastLoss=0.143, valLoss=0.145]\n",
      "Epoch 69/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s, lastLoss=0.142, valLoss=0.149]\n",
      "Epoch 70/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 218.74batch/s, lastLoss=0.14, valLoss=0.137]\n",
      "Epoch 71/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 216.20batch/s, lastLoss=0.141, valLoss=0.138]\n",
      "Epoch 72/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 212.92batch/s, lastLoss=0.14, valLoss=0.146]\n",
      "Epoch 73/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 219.17batch/s, lastLoss=0.142, valLoss=0.148]\n",
      "Epoch 74/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 217.89batch/s, lastLoss=0.141, valLoss=0.15]\n",
      "Epoch 75/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 220.90batch/s, lastLoss=0.14, valLoss=0.138]\n",
      "Epoch 76/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.15batch/s, lastLoss=0.143, valLoss=0.146]\n",
      "Epoch 77/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 206.26batch/s, lastLoss=0.141, valLoss=0.142]\n",
      "Epoch 78/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.141, valLoss=0.15]\n",
      "Epoch 79/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.53batch/s, lastLoss=0.138, valLoss=0.132]\n",
      "Epoch 80/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.47batch/s, lastLoss=0.137, valLoss=0.131]\n",
      "Epoch 81/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 216.65batch/s, lastLoss=0.14, valLoss=0.131]\n",
      "Epoch 82/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.135, valLoss=0.145]\n",
      "Epoch 83/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.74batch/s, lastLoss=0.134, valLoss=0.138]\n",
      "Epoch 84/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.90batch/s, lastLoss=0.139, valLoss=0.135]\n",
      "Epoch 85/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.79batch/s, lastLoss=0.133, valLoss=0.134]\n",
      "Epoch 86/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.79batch/s, lastLoss=0.131, valLoss=0.126]\n",
      "Epoch 87/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.39batch/s, lastLoss=0.128, valLoss=0.126]\n",
      "Epoch 88/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s, lastLoss=0.129, valLoss=0.122]\n",
      "Epoch 89/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.46batch/s, lastLoss=0.129, valLoss=0.129]\n",
      "Epoch 90/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 199.99batch/s, lastLoss=0.133, valLoss=0.129]\n",
      "Epoch 91/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 202.90batch/s, lastLoss=0.128, valLoss=0.124]\n",
      "Epoch 92/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 200.00batch/s, lastLoss=0.13, valLoss=0.121]\n",
      "Epoch 93/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 201.44batch/s, lastLoss=0.127, valLoss=0.126]\n",
      "Epoch 94/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.93batch/s, lastLoss=0.127, valLoss=0.126]\n",
      "Epoch 95/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 183.91batch/s, lastLoss=0.127, valLoss=0.121]\n",
      "Epoch 96/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 198.93batch/s, lastLoss=0.126, valLoss=0.125]\n",
      "Epoch 97/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 194.44batch/s, lastLoss=0.125, valLoss=0.118]\n",
      "Epoch 98/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 188.23batch/s, lastLoss=0.126, valLoss=0.122]\n",
      "Epoch 99/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 209.75batch/s, lastLoss=0.125, valLoss=0.118]\n",
      "Epoch 100/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 215.40batch/s, lastLoss=0.124, valLoss=0.123]\n",
      "Epoch 101/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 208.56batch/s, lastLoss=0.125, valLoss=0.129]\n",
      "Epoch 102/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.56batch/s, lastLoss=0.124, valLoss=0.117]\n",
      "Epoch 103/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.26batch/s, lastLoss=0.124, valLoss=0.119]\n",
      "Epoch 104/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.46batch/s, lastLoss=0.124, valLoss=0.121]\n",
      "Epoch 105/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 215.37batch/s, lastLoss=0.124, valLoss=0.117]\n",
      "Epoch 106/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.23batch/s, lastLoss=0.125, valLoss=0.118]\n",
      "Epoch 107/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.24batch/s, lastLoss=0.125, valLoss=0.123]\n",
      "Epoch 108/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.66batch/s, lastLoss=0.123, valLoss=0.117]\n",
      "Epoch 109/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.61batch/s, lastLoss=0.127, valLoss=0.115]\n",
      "Epoch 110/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.72batch/s, lastLoss=0.123, valLoss=0.124]\n",
      "Epoch 111/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.64batch/s, lastLoss=0.123, valLoss=0.131]\n",
      "Epoch 112/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.46batch/s, lastLoss=0.125, valLoss=0.125]\n",
      "Epoch 113/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.34batch/s, lastLoss=0.123, valLoss=0.118]\n",
      "Epoch 114/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.00batch/s, lastLoss=0.122, valLoss=0.117]\n",
      "Epoch 115/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.56batch/s, lastLoss=0.125, valLoss=0.122]\n",
      "Epoch 116/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.81batch/s, lastLoss=0.123, valLoss=0.119]\n",
      "Epoch 117/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.18batch/s, lastLoss=0.122, valLoss=0.123]\n",
      "Epoch 118/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.52batch/s, lastLoss=0.122, valLoss=0.122]\n",
      "Epoch 119/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 204.38batch/s, lastLoss=0.121, valLoss=0.12]\n",
      "Epoch 120/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.18batch/s, lastLoss=0.123, valLoss=0.118]\n",
      "Epoch 121/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 199.29batch/s, lastLoss=0.121, valLoss=0.122]\n",
      "Epoch 122/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 211.32batch/s, lastLoss=0.121, valLoss=0.123]\n",
      "Epoch 123/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.122, valLoss=0.117]\n",
      "Epoch 124/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 200.36batch/s, lastLoss=0.122, valLoss=0.126]\n",
      "Epoch 125/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.46batch/s, lastLoss=0.123, valLoss=0.117]\n",
      "Epoch 126/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 216.27batch/s, lastLoss=0.122, valLoss=0.123]\n",
      "Epoch 127/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 207.41batch/s, lastLoss=0.123, valLoss=0.117]\n",
      "Epoch 128/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.90batch/s, lastLoss=0.12, valLoss=0.126]\n",
      "Epoch 129/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 203.63batch/s, lastLoss=0.121, valLoss=0.116]\n",
      "Epoch 130/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 222.98batch/s, lastLoss=0.121, valLoss=0.12]\n",
      "Epoch 131/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.96batch/s, lastLoss=0.12, valLoss=0.119]\n",
      "Epoch 132/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.90batch/s, lastLoss=0.122, valLoss=0.119]\n",
      "Epoch 133/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.12batch/s, lastLoss=0.121, valLoss=0.115]\n",
      "Epoch 134/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 208.95batch/s, lastLoss=0.123, valLoss=0.119]\n",
      "Epoch 135/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.90batch/s, lastLoss=0.121, valLoss=0.124]\n",
      "Epoch 136/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.13batch/s, lastLoss=0.122, valLoss=0.127]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 230.44batch/s, lastLoss=0.12, valLoss=0.121]\n",
      "Epoch 138/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 229.07batch/s, lastLoss=0.12, valLoss=0.116]\n",
      "Epoch 139/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 232.37batch/s, lastLoss=0.12, valLoss=0.117]\n",
      "Epoch 140/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.34batch/s, lastLoss=0.12, valLoss=0.116]\n",
      "Epoch 141/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 233.33batch/s, lastLoss=0.121, valLoss=0.116]\n",
      "Epoch 142/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 227.62batch/s, lastLoss=0.12, valLoss=0.123]\n",
      "Epoch 143/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 228.05batch/s, lastLoss=0.121, valLoss=0.123]\n",
      "Epoch 144/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.59batch/s, lastLoss=0.119, valLoss=0.121]\n",
      "Epoch 145/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.56batch/s, lastLoss=0.121, valLoss=0.123]\n",
      "Epoch 146/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.26batch/s, lastLoss=0.119, valLoss=0.118]\n",
      "Epoch 147/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.33batch/s, lastLoss=0.118, valLoss=0.117]\n",
      "Epoch 148/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.92batch/s, lastLoss=0.12, valLoss=0.119]\n",
      "Epoch 149/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 216.98batch/s, lastLoss=0.12, valLoss=0.12]\n",
      "Epoch 150/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 215.14batch/s, lastLoss=0.118, valLoss=0.114]\n",
      "Epoch 151/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.75batch/s, lastLoss=0.119, valLoss=0.118]\n",
      "Epoch 152/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 225.82batch/s, lastLoss=0.118, valLoss=0.12]\n",
      "Epoch 153/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 215.80batch/s, lastLoss=0.119, valLoss=0.123]\n",
      "Epoch 154/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.11batch/s, lastLoss=0.118, valLoss=0.114]\n",
      "Epoch 155/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.35batch/s, lastLoss=0.119, valLoss=0.121]\n",
      "Epoch 156/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.74batch/s, lastLoss=0.119, valLoss=0.116]\n",
      "Epoch 157/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 204.61batch/s, lastLoss=0.119, valLoss=0.112]\n",
      "Epoch 158/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 209.75batch/s, lastLoss=0.118, valLoss=0.117]\n",
      "Epoch 159/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.91batch/s, lastLoss=0.121, valLoss=0.114]\n",
      "Epoch 160/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 188.55batch/s, lastLoss=0.12, valLoss=0.113]\n",
      "Epoch 161/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 194.44batch/s, lastLoss=0.119, valLoss=0.116]\n",
      "Epoch 162/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.25batch/s, lastLoss=0.118, valLoss=0.119]\n",
      "Epoch 163/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.25batch/s, lastLoss=0.119, valLoss=0.117]\n",
      "Epoch 164/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 218.49batch/s, lastLoss=0.119, valLoss=0.12]\n",
      "Epoch 165/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 222.26batch/s, lastLoss=0.119, valLoss=0.11]\n",
      "Epoch 166/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 213.74batch/s, lastLoss=0.118, valLoss=0.119]\n",
      "Epoch 167/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 205.13batch/s, lastLoss=0.119, valLoss=0.122]\n",
      "Epoch 168/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 209.35batch/s, lastLoss=0.117, valLoss=0.118]\n",
      "Epoch 169/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.47batch/s, lastLoss=0.117, valLoss=0.121]\n",
      "Epoch 170/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 210.53batch/s, lastLoss=0.118, valLoss=0.119]\n",
      "Epoch 171/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 191.13batch/s, lastLoss=0.121, valLoss=0.121]\n",
      "Epoch 172/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.51batch/s, lastLoss=0.118, valLoss=0.12]\n",
      "Epoch 173/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 216.62batch/s, lastLoss=0.118, valLoss=0.114]\n",
      "Epoch 174/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.32batch/s, lastLoss=0.117, valLoss=0.113]\n",
      "Epoch 175/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.06batch/s, lastLoss=0.117, valLoss=0.116]\n",
      "Epoch 176/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 213.45batch/s, lastLoss=0.119, valLoss=0.116]\n",
      "Epoch 177/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.60batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 178/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.76batch/s, lastLoss=0.118, valLoss=0.119]\n",
      "Epoch 179/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.84batch/s, lastLoss=0.118, valLoss=0.126]\n",
      "Epoch 180/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.27batch/s, lastLoss=0.119, valLoss=0.114]\n",
      "Epoch 181/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.36batch/s, lastLoss=0.119, valLoss=0.119]\n",
      "Epoch 182/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 206.66batch/s, lastLoss=0.119, valLoss=0.118]\n",
      "Epoch 183/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.67batch/s, lastLoss=0.119, valLoss=0.117]\n",
      "Epoch 184/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.54batch/s, lastLoss=0.118, valLoss=0.121]\n",
      "Epoch 185/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.92batch/s, lastLoss=0.117, valLoss=0.119]\n",
      "Epoch 186/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.95batch/s, lastLoss=0.117, valLoss=0.121]\n",
      "Epoch 187/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.23batch/s, lastLoss=0.12, valLoss=0.116]\n",
      "Epoch 188/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.93batch/s, lastLoss=0.117, valLoss=0.119]\n",
      "Epoch 189/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.12batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 190/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.29batch/s, lastLoss=0.117, valLoss=0.112]\n",
      "Epoch 191/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.73batch/s, lastLoss=0.117, valLoss=0.12]\n",
      "Epoch 192/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 210.53batch/s, lastLoss=0.117, valLoss=0.117]\n",
      "Epoch 193/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s, lastLoss=0.116, valLoss=0.119]\n",
      "Epoch 194/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.91batch/s, lastLoss=0.117, valLoss=0.116]\n",
      "Epoch 195/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.05batch/s, lastLoss=0.118, valLoss=0.125]\n",
      "Epoch 196/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 205.90batch/s, lastLoss=0.117, valLoss=0.114]\n",
      "Epoch 197/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 201.80batch/s, lastLoss=0.118, valLoss=0.117]\n",
      "Epoch 198/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 202.53batch/s, lastLoss=0.118, valLoss=0.111]\n",
      "Epoch 199/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.54batch/s, lastLoss=0.117, valLoss=0.116]\n",
      "Epoch 200/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.14batch/s, lastLoss=0.119, valLoss=0.115]\n",
      "Epoch 201/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.60batch/s, lastLoss=0.119, valLoss=0.117]\n",
      "Epoch 202/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.39batch/s, lastLoss=0.116, valLoss=0.124]\n",
      "Epoch 203/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.51batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 204/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.79batch/s, lastLoss=0.117, valLoss=0.113]\n",
      "Epoch 205/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.64batch/s, lastLoss=0.117, valLoss=0.112]\n",
      "Epoch 206/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.77batch/s, lastLoss=0.117, valLoss=0.122]\n",
      "Epoch 207/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.91batch/s, lastLoss=0.119, valLoss=0.119]\n",
      "Epoch 208/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.34batch/s, lastLoss=0.118, valLoss=0.115]\n",
      "Epoch 209/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.75batch/s, lastLoss=0.117, valLoss=0.117]\n",
      "Epoch 210/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.77batch/s, lastLoss=0.117, valLoss=0.117]\n",
      "Epoch 211/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.04batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 212/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.56batch/s, lastLoss=0.117, valLoss=0.113]\n",
      "Epoch 213/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.00batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 214/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.68batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 215/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.13batch/s, lastLoss=0.117, valLoss=0.118]\n",
      "Epoch 216/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.70batch/s, lastLoss=0.117, valLoss=0.112]\n",
      "Epoch 217/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.117, valLoss=0.11]\n",
      "Epoch 218/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 228.57batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 219/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.25batch/s, lastLoss=0.116, valLoss=0.115]\n",
      "Epoch 220/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.75batch/s, lastLoss=0.117, valLoss=0.112]\n",
      "Epoch 221/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.54batch/s, lastLoss=0.116, valLoss=0.115]\n",
      "Epoch 222/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 211.72batch/s, lastLoss=0.117, valLoss=0.117]\n",
      "Epoch 223/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.45batch/s, lastLoss=0.118, valLoss=0.115]\n",
      "Epoch 224/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.04batch/s, lastLoss=0.116, valLoss=0.119]\n",
      "Epoch 225/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.70batch/s, lastLoss=0.117, valLoss=0.113]\n",
      "Epoch 226/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 225.93batch/s, lastLoss=0.116, valLoss=0.11]\n",
      "Epoch 227/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 223.63batch/s, lastLoss=0.115, valLoss=0.12]\n",
      "Epoch 228/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.19batch/s, lastLoss=0.116, valLoss=0.118]\n",
      "Epoch 229/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.90batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 230/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 202.48batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 231/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 208.94batch/s, lastLoss=0.116, valLoss=0.119]\n",
      "Epoch 232/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.34batch/s, lastLoss=0.116, valLoss=0.109]\n",
      "Epoch 233/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 215.80batch/s, lastLoss=0.117, valLoss=0.124]\n",
      "Epoch 234/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 197.53batch/s, lastLoss=0.117, valLoss=0.113]\n",
      "Epoch 235/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 222.82batch/s, lastLoss=0.116, valLoss=0.12]\n",
      "Epoch 236/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 216.08batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 237/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.13batch/s, lastLoss=0.118, valLoss=0.113]\n",
      "Epoch 238/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.59batch/s, lastLoss=0.117, valLoss=0.117]\n",
      "Epoch 239/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.18batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 240/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.70batch/s, lastLoss=0.116, valLoss=0.124]\n",
      "Epoch 241/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 216.21batch/s, lastLoss=0.118, valLoss=0.118]\n",
      "Epoch 242/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.06batch/s, lastLoss=0.118, valLoss=0.113]\n",
      "Epoch 243/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.21batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 244/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 218.69batch/s, lastLoss=0.12, valLoss=0.121]\n",
      "Epoch 245/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.09batch/s, lastLoss=0.117, valLoss=0.114]\n",
      "Epoch 246/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 207.40batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 247/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 216.21batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 248/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.15batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 249/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.85batch/s, lastLoss=0.115, valLoss=0.117]\n",
      "Epoch 250/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.25batch/s, lastLoss=0.116, valLoss=0.121]\n",
      "Epoch 251/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 209.73batch/s, lastLoss=0.117, valLoss=0.109]\n",
      "Epoch 252/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 209.74batch/s, lastLoss=0.116, valLoss=0.111]\n",
      "Epoch 253/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.35batch/s, lastLoss=0.116, valLoss=0.117]\n",
      "Epoch 254/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 228.09batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 255/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.34batch/s, lastLoss=0.115, valLoss=0.116]\n",
      "Epoch 256/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 230.44batch/s, lastLoss=0.117, valLoss=0.118]\n",
      "Epoch 257/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 202.17batch/s, lastLoss=0.116, valLoss=0.119]\n",
      "Epoch 258/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 224.61batch/s, lastLoss=0.116, valLoss=0.12]\n",
      "Epoch 259/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.90batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 260/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.88batch/s, lastLoss=0.117, valLoss=0.111]\n",
      "Epoch 261/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.65batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 262/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 211.99batch/s, lastLoss=0.116, valLoss=0.111]\n",
      "Epoch 263/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 232.83batch/s, lastLoss=0.116, valLoss=0.111]\n",
      "Epoch 264/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.45batch/s, lastLoss=0.115, valLoss=0.113]\n",
      "Epoch 265/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.51batch/s, lastLoss=0.115, valLoss=0.122]\n",
      "Epoch 266/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.28batch/s, lastLoss=0.117, valLoss=0.121]\n",
      "Epoch 267/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.09batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 268/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 209.74batch/s, lastLoss=0.115, valLoss=0.118]\n",
      "Epoch 269/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.88batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 270/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.15batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 271/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.65batch/s, lastLoss=0.115, valLoss=0.116]\n",
      "Epoch 272/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.95batch/s, lastLoss=0.116, valLoss=0.116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 273/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 235.29batch/s, lastLoss=0.116, valLoss=0.117]\n",
      "Epoch 274/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 233.41batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 275/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.15batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 276/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 232.02batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 277/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.32batch/s, lastLoss=0.116, valLoss=0.115]\n",
      "Epoch 278/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.40batch/s, lastLoss=0.116, valLoss=0.115]\n",
      "Epoch 279/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 216.22batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 280/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.64batch/s, lastLoss=0.119, valLoss=0.116]\n",
      "Epoch 281/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 231.88batch/s, lastLoss=0.117, valLoss=0.119]\n",
      "Epoch 282/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 232.74batch/s, lastLoss=0.115, valLoss=0.116]\n",
      "Epoch 283/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.92batch/s, lastLoss=0.115, valLoss=0.118]\n",
      "Epoch 284/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 232.14batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 285/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 231.40batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 286/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.32batch/s, lastLoss=0.116, valLoss=0.119]\n",
      "Epoch 287/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.93batch/s, lastLoss=0.116, valLoss=0.113]\n",
      "Epoch 288/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 231.87batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 289/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 230.09batch/s, lastLoss=0.114, valLoss=0.116]\n",
      "Epoch 290/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.21batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 291/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 230.11batch/s, lastLoss=0.116, valLoss=0.117]\n",
      "Epoch 292/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 232.26batch/s, lastLoss=0.116, valLoss=0.13]\n",
      "Epoch 293/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 232.27batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 294/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.16batch/s, lastLoss=0.115, valLoss=0.113]\n",
      "Epoch 295/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.17batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 296/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.11batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 297/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 207.03batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 298/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 207.02batch/s, lastLoss=0.116, valLoss=0.117]\n",
      "Epoch 299/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.66batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 300/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 198.24batch/s, lastLoss=0.117, valLoss=0.116]\n",
      "Epoch 301/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 197.18batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 302/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 199.29batch/s, lastLoss=0.115, valLoss=0.116]\n",
      "Epoch 303/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 205.00batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 304/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.97batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 305/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 213.85batch/s, lastLoss=0.115, valLoss=0.119]\n",
      "Epoch 306/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 216.64batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 307/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.50batch/s, lastLoss=0.115, valLoss=0.114]\n",
      "Epoch 308/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 205.67batch/s, lastLoss=0.116, valLoss=0.123]\n",
      "Epoch 309/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.117, valLoss=0.119]\n",
      "Epoch 310/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 206.26batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 311/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 213.33batch/s, lastLoss=0.117, valLoss=0.115]\n",
      "Epoch 312/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.11batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 313/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.73batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 314/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 169.19batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 315/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.26batch/s, lastLoss=0.116, valLoss=0.112]\n",
      "Epoch 316/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 241.19batch/s, lastLoss=0.115, valLoss=0.113]\n",
      "Epoch 317/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 204.38batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 318/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.51batch/s, lastLoss=0.114, valLoss=0.108]\n",
      "Epoch 319/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 202.53batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 320/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 185.43batch/s, lastLoss=0.114, valLoss=0.112]\n",
      "Epoch 321/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.51batch/s, lastLoss=0.114, valLoss=0.112]\n",
      "Epoch 322/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 238.88batch/s, lastLoss=0.115, valLoss=0.118]\n",
      "Epoch 323/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 230.98batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 324/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 245.56batch/s, lastLoss=0.115, valLoss=0.114]\n",
      "Epoch 325/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 251.59batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 326/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 249.08batch/s, lastLoss=0.115, valLoss=0.117]\n",
      "Epoch 327/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 241.94batch/s, lastLoss=0.116, valLoss=0.113]\n",
      "Epoch 328/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 247.37batch/s, lastLoss=0.117, valLoss=0.11]\n",
      "Epoch 329/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 250.57batch/s, lastLoss=0.115, valLoss=0.117]\n",
      "Epoch 330/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 248.86batch/s, lastLoss=0.115, valLoss=0.121]\n",
      "Epoch 331/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 243.77batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 332/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 245.60batch/s, lastLoss=0.116, valLoss=0.111]\n",
      "Epoch 333/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 243.36batch/s, lastLoss=0.114, valLoss=0.115]\n",
      "Epoch 334/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 251.11batch/s, lastLoss=0.115, valLoss=0.107]\n",
      "Epoch 335/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 237.94batch/s, lastLoss=0.114, valLoss=0.119]\n",
      "Epoch 336/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 245.46batch/s, lastLoss=0.115, valLoss=0.114]\n",
      "Epoch 337/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 250.57batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 338/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 252.37batch/s, lastLoss=0.116, valLoss=0.118]\n",
      "Epoch 339/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 244.43batch/s, lastLoss=0.115, valLoss=0.117]\n",
      "Epoch 340/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 254.01batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 341/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 242.09batch/s, lastLoss=0.116, valLoss=0.111]\n",
      "Epoch 342/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 253.97batch/s, lastLoss=0.115, valLoss=0.118]\n",
      "Epoch 343/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 244.54batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 344/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 249.74batch/s, lastLoss=0.116, valLoss=0.117]\n",
      "Epoch 345/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 253.80batch/s, lastLoss=0.114, valLoss=0.116]\n",
      "Epoch 346/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 252.03batch/s, lastLoss=0.114, valLoss=0.12]\n",
      "Epoch 347/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 244.43batch/s, lastLoss=0.117, valLoss=0.117]\n",
      "Epoch 348/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 244.53batch/s, lastLoss=0.116, valLoss=0.109]\n",
      "Epoch 349/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 242.98batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 350/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 251.02batch/s, lastLoss=0.115, valLoss=0.113]\n",
      "Epoch 351/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 236.29batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 352/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.69batch/s, lastLoss=0.115, valLoss=0.118]\n",
      "Epoch 353/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.14batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 354/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 247.76batch/s, lastLoss=0.114, valLoss=0.111]\n",
      "Epoch 355/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 246.29batch/s, lastLoss=0.115, valLoss=0.109]\n",
      "Epoch 356/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 248.35batch/s, lastLoss=0.116, valLoss=0.113]\n",
      "Epoch 357/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 251.68batch/s, lastLoss=0.115, valLoss=0.116]\n",
      "Epoch 358/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 218.75batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 359/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 237.70batch/s, lastLoss=0.114, valLoss=0.109]\n",
      "Epoch 360/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 241.90batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 361/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 237.73batch/s, lastLoss=0.114, valLoss=0.109]\n",
      "Epoch 362/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 208.38batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 363/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 194.11batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 364/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 206.64batch/s, lastLoss=0.115, valLoss=0.114]\n",
      "Epoch 365/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 208.57batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 366/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 203.64batch/s, lastLoss=0.115, valLoss=0.112]\n",
      "Epoch 367/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.04batch/s, lastLoss=0.114, valLoss=0.111]\n",
      "Epoch 368/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.53batch/s, lastLoss=0.114, valLoss=0.107]\n",
      "Epoch 369/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.47batch/s, lastLoss=0.114, valLoss=0.118]\n",
      "Epoch 370/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.17batch/s, lastLoss=0.114, valLoss=0.114]\n",
      "Epoch 371/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.04batch/s, lastLoss=0.115, valLoss=0.113]\n",
      "Epoch 372/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 248.65batch/s, lastLoss=0.115, valLoss=0.118]\n",
      "Epoch 373/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 247.34batch/s, lastLoss=0.116, valLoss=0.108]\n",
      "Epoch 374/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 240.36batch/s, lastLoss=0.114, valLoss=0.11]\n",
      "Epoch 375/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 245.61batch/s, lastLoss=0.115, valLoss=0.115]\n",
      "Epoch 376/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 238.80batch/s, lastLoss=0.115, valLoss=0.119]\n",
      "Epoch 377/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 246.70batch/s, lastLoss=0.115, valLoss=0.113]\n",
      "Epoch 378/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 228.29batch/s, lastLoss=0.115, valLoss=0.113]\n",
      "Epoch 379/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 251.67batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 380/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 253.40batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 381/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 250.95batch/s, lastLoss=0.116, valLoss=0.114]\n",
      "Epoch 382/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 232.57batch/s, lastLoss=0.114, valLoss=0.111]\n",
      "Epoch 383/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 249.64batch/s, lastLoss=0.114, valLoss=0.112]\n",
      "Epoch 384/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 253.39batch/s, lastLoss=0.114, valLoss=0.109]\n",
      "Epoch 385/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.00batch/s, lastLoss=0.114, valLoss=0.109]\n",
      "Epoch 386/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 216.22batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 387/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.26batch/s, lastLoss=0.114, valLoss=0.114]\n",
      "Epoch 388/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 234.80batch/s, lastLoss=0.113, valLoss=0.113]\n",
      "Epoch 389/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 218.75batch/s, lastLoss=0.114, valLoss=0.115]\n",
      "Epoch 390/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 226.25batch/s, lastLoss=0.115, valLoss=0.11]\n",
      "Epoch 391/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 241.08batch/s, lastLoss=0.114, valLoss=0.112]\n",
      "Epoch 392/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.39batch/s, lastLoss=0.114, valLoss=0.119]\n",
      "Epoch 393/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.90batch/s, lastLoss=0.115, valLoss=0.111]\n",
      "Epoch 394/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 235.29batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 395/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.88batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 396/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 228.57batch/s, lastLoss=0.114, valLoss=0.115]\n",
      "Epoch 397/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 198.58batch/s, lastLoss=0.114, valLoss=0.117]\n",
      "Epoch 398/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 228.57batch/s, lastLoss=0.113, valLoss=0.112]\n",
      "Epoch 399/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 236.98batch/s, lastLoss=0.114, valLoss=0.113]\n",
      "Epoch 400/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 213.74batch/s, lastLoss=0.115, valLoss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.12480014465749263\n",
      "Average validation loss: 0.1234719197625915\n"
     ]
    }
   ],
   "source": [
    "sRNN_train_loss, sRNN_val_loss =  train(train_loader, sRNN, val_loader=test_loader, LR=0.01, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "215c6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnSingle = RNNSingle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "952d4490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:01<00:00, 42.14batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 42.38batch/s, lastLoss=0.254, valLoss=0.199]\n",
      "Epoch 3/400:  27%|█████████▍                         | 15/56 [00:00<00:00, 41.03batch/s, lastLoss=0.231, valLoss=0.187]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rnnSingle_train_loss, rnnSingle_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnnSingle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:34\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     31\u001b[0m predicted \u001b[38;5;241m=\u001b[39m net(inp)\n\u001b[0;32m     33\u001b[0m cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[1;32m---> 34\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m cost\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnnSingle_train_loss, rnnSingle_val_loss =  train(train_loader, rnnSingle, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea3333f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50a489f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:02<00:00, 22.57batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.39batch/s, lastLoss=0.308, valLoss=0.206]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.90batch/s, lastLoss=0.241, valLoss=0.182]\n",
      "Epoch 4/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 23.40batch/s, lastLoss=0.229, valLoss=0.191]\n",
      "Epoch 5/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 23.03batch/s, lastLoss=0.225, valLoss=0.182]\n",
      "Epoch 6/400: 100%|████████████████████████████████████| 56/56 [00:02<00:00, 23.43batch/s, lastLoss=0.22, valLoss=0.188]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.87batch/s, lastLoss=0.218, valLoss=0.193]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.42batch/s, lastLoss=0.214, valLoss=0.196]\n",
      "Epoch 9/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 23.06batch/s, lastLoss=0.211, valLoss=0.182]\n",
      "Epoch 10/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.05batch/s, lastLoss=0.209, valLoss=0.188]\n",
      "Epoch 11/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.35batch/s, lastLoss=0.209, valLoss=0.179]\n",
      "Epoch 12/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.83batch/s, lastLoss=0.205, valLoss=0.189]\n",
      "Epoch 13/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.83batch/s, lastLoss=0.206, valLoss=0.184]\n",
      "Epoch 14/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.86batch/s, lastLoss=0.204, valLoss=0.186]\n",
      "Epoch 15/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.09batch/s, lastLoss=0.204, valLoss=0.186]\n",
      "Epoch 16/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.00batch/s, lastLoss=0.205, valLoss=0.181]\n",
      "Epoch 17/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.55batch/s, lastLoss=0.202, valLoss=0.18]\n",
      "Epoch 18/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.23batch/s, lastLoss=0.202, valLoss=0.188]\n",
      "Epoch 19/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.68batch/s, lastLoss=0.201, valLoss=0.184]\n",
      "Epoch 20/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.16batch/s, lastLoss=0.199, valLoss=0.182]\n",
      "Epoch 21/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.30batch/s, lastLoss=0.199, valLoss=0.184]\n",
      "Epoch 22/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.59batch/s, lastLoss=0.198, valLoss=0.19]\n",
      "Epoch 23/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.59batch/s, lastLoss=0.198, valLoss=0.182]\n",
      "Epoch 24/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.71batch/s, lastLoss=0.197, valLoss=0.187]\n",
      "Epoch 25/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.98batch/s, lastLoss=0.198, valLoss=0.187]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.67batch/s, lastLoss=0.195, valLoss=0.186]\n",
      "Epoch 27/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.96batch/s, lastLoss=0.197, valLoss=0.188]\n",
      "Epoch 28/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.90batch/s, lastLoss=0.195, valLoss=0.183]\n",
      "Epoch 29/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.10batch/s, lastLoss=0.195, valLoss=0.182]\n",
      "Epoch 30/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.69batch/s, lastLoss=0.195, valLoss=0.186]\n",
      "Epoch 31/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.06batch/s, lastLoss=0.195, valLoss=0.184]\n",
      "Epoch 32/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.06batch/s, lastLoss=0.194, valLoss=0.186]\n",
      "Epoch 33/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.82batch/s, lastLoss=0.195, valLoss=0.187]\n",
      "Epoch 34/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.66batch/s, lastLoss=0.194, valLoss=0.182]\n",
      "Epoch 35/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.53batch/s, lastLoss=0.194, valLoss=0.185]\n",
      "Epoch 36/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.13batch/s, lastLoss=0.194, valLoss=0.186]\n",
      "Epoch 37/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.12batch/s, lastLoss=0.193, valLoss=0.183]\n",
      "Epoch 38/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 23.14batch/s, lastLoss=0.194, valLoss=0.187]\n",
      "Epoch 39/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.83batch/s, lastLoss=0.192, valLoss=0.184]\n",
      "Epoch 40/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.71batch/s, lastLoss=0.194, valLoss=0.183]\n",
      "Epoch 41/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.49batch/s, lastLoss=0.193, valLoss=0.181]\n",
      "Epoch 42/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.96batch/s, lastLoss=0.194, valLoss=0.186]\n",
      "Epoch 43/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.52batch/s, lastLoss=0.193, valLoss=0.19]\n",
      "Epoch 44/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.18batch/s, lastLoss=0.193, valLoss=0.18]\n",
      "Epoch 45/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.89batch/s, lastLoss=0.192, valLoss=0.181]\n",
      "Epoch 46/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.68batch/s, lastLoss=0.193, valLoss=0.187]\n",
      "Epoch 47/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.10batch/s, lastLoss=0.192, valLoss=0.182]\n",
      "Epoch 48/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.87batch/s, lastLoss=0.193, valLoss=0.188]\n",
      "Epoch 49/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 22.89batch/s, lastLoss=0.192, valLoss=0.19]\n",
      "Epoch 50/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.53batch/s, lastLoss=0.192, valLoss=0.182]\n",
      "Epoch 51/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 22.52batch/s, lastLoss=0.192, valLoss=0.186]\n",
      "Epoch 52/400:  75%|█████████████████████████▌        | 42/56 [00:02<00:00, 20.98batch/s, lastLoss=0.191, valLoss=0.183]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lstm_train_loss, lstm_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     33\u001b[0m         cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     34\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 35\u001b[0m         \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_train_loss, lstm_val_loss =  train(train_loader, lstm, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7b6bb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6772"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f6de9",
   "metadata": {},
   "source": [
    "# Try capturing per level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8a7583f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(signals, wavelet='db4', levels=3):\n",
    "    \"\"\"\n",
    "    Extract wavelet packet features for each signal in the input array.\n",
    "\n",
    "    Args:\n",
    "    - signals (numpy.ndarray): a 2D array containing two 1D signals\n",
    "    - wavelet (str): the name of the wavelet to use for decomposition\n",
    "    - level (int): the level of wavelet packet decomposition to use\n",
    "\n",
    "    Returns:\n",
    "    - features (dict): a dictionary containing the wavelet packet features for each signal\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionary to store features for each signal\n",
    "    features = []\n",
    "\n",
    "    # Loop over each signal\n",
    "    for i in range(2):\n",
    "        # Perform wavelet packet decomposition\n",
    "        wp = pywt.WaveletPacket(data=signals[i], wavelet=wavelet, mode='symmetric', maxlevel=levels)\n",
    "        \n",
    "        coeffs = []\n",
    "        for level in range(1, levels+1):\n",
    "            level_coeffs = np.array([n.data for n in wp.get_level(level, 'freq')])  # get the coefficients at the current level\n",
    "            \n",
    "            stats = np.array([\n",
    "                [np.mean(level_coeffs[i]), np.std(level_coeffs[i]), np.ptp(level_coeffs[i])]\n",
    "                 for i in range(len(level_coeffs))\n",
    "                ])\n",
    "\n",
    "            for stat in stats:\n",
    "                for s in stat:\n",
    "                    coeffs.append(s)\n",
    "\n",
    "        # Stack the coefficients vertically to create the final array\n",
    "        features.append(coeffs)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "03dd4bb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:09<00:00, 241.53it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, scaler = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d35cda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = getSplit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2e558a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSingle(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=2048, num_layers=2, output_dim=6):\n",
    "        super(RNNSingle, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(86016, 1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, 2)\n",
    "        \n",
    "        h0 = torch.zeros(2, batch_size, self.hidden_dim).to(x.device)\n",
    "        x, _ = self.rnn(x, h0)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1f2aaa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=512, num_layers=5, output_dim=6):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 6)\n",
    "        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, 2)\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Initialize cell state with zeros\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Move tensors to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "            x = x.cuda()\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f4fcfd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRNN(nn.Module):\n",
    "    def __init__(self, input_size=42, hidden_size=10):\n",
    "        super(SiameseRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 6),\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x1 = x[:, 0, :].unsqueeze(1)\n",
    "        x2 = x[:, 0, :].unsqueeze(1)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n",
    "        _, h1 = self.rnn(x1, h0)  # Add a batch dimension\n",
    "        _, h2 = self.rnn(x2, h0)  # Add a batch dimension\n",
    "        \n",
    "        out = torch.cat([h1[-1], h2[-1]], dim=1)\n",
    "        out = out.reshape(batch_size, -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "dfbb3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sRNN = SiameseRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "fb2cb9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|█████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s]\n",
      "Epoch 2/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 216.18batch/s, lastLoss=0.212, valLoss=0.183]\n",
      "Epoch 3/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 188.25batch/s, lastLoss=0.185, valLoss=0.179]\n",
      "Epoch 4/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 210.13batch/s, lastLoss=0.183, valLoss=0.188]\n",
      "Epoch 5/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 208.56batch/s, lastLoss=0.178, valLoss=0.183]\n",
      "Epoch 6/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 206.64batch/s, lastLoss=0.173, valLoss=0.169]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 218.32batch/s, lastLoss=0.17, valLoss=0.173]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 217.89batch/s, lastLoss=0.169, valLoss=0.17]\n",
      "Epoch 9/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 217.05batch/s, lastLoss=0.168, valLoss=0.165]\n",
      "Epoch 10/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 214.98batch/s, lastLoss=0.169, valLoss=0.17]\n",
      "Epoch 11/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.92batch/s, lastLoss=0.167, valLoss=0.173]\n",
      "Epoch 12/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 200.72batch/s, lastLoss=0.167, valLoss=0.166]\n",
      "Epoch 13/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 218.32batch/s, lastLoss=0.168, valLoss=0.166]\n",
      "Epoch 14/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.168, valLoss=0.166]\n",
      "Epoch 15/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 187.92batch/s, lastLoss=0.167, valLoss=0.165]\n",
      "Epoch 16/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 207.03batch/s, lastLoss=0.168, valLoss=0.167]\n",
      "Epoch 17/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 195.46batch/s, lastLoss=0.167, valLoss=0.16]\n",
      "Epoch 18/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.167, valLoss=0.169]\n",
      "Epoch 19/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 203.64batch/s, lastLoss=0.166, valLoss=0.162]\n",
      "Epoch 20/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 220.91batch/s, lastLoss=0.166, valLoss=0.166]\n",
      "Epoch 21/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 201.08batch/s, lastLoss=0.167, valLoss=0.163]\n",
      "Epoch 22/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.80batch/s, lastLoss=0.166, valLoss=0.167]\n",
      "Epoch 23/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s, lastLoss=0.167, valLoss=0.168]\n",
      "Epoch 24/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.06batch/s, lastLoss=0.165, valLoss=0.165]\n",
      "Epoch 25/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 203.27batch/s, lastLoss=0.165, valLoss=0.159]\n",
      "Epoch 26/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.05batch/s, lastLoss=0.163, valLoss=0.168]\n",
      "Epoch 27/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 211.72batch/s, lastLoss=0.165, valLoss=0.166]\n",
      "Epoch 28/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.91batch/s, lastLoss=0.164, valLoss=0.158]\n",
      "Epoch 29/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 216.22batch/s, lastLoss=0.163, valLoss=0.163]\n",
      "Epoch 30/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 209.74batch/s, lastLoss=0.163, valLoss=0.157]\n",
      "Epoch 31/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 216.45batch/s, lastLoss=0.162, valLoss=0.162]\n",
      "Epoch 32/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 200.72batch/s, lastLoss=0.162, valLoss=0.16]\n",
      "Epoch 33/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 207.02batch/s, lastLoss=0.161, valLoss=0.156]\n",
      "Epoch 34/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 211.31batch/s, lastLoss=0.161, valLoss=0.156]\n",
      "Epoch 35/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 191.45batch/s, lastLoss=0.159, valLoss=0.155]\n",
      "Epoch 36/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.16, valLoss=0.157]\n",
      "Epoch 37/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.80batch/s, lastLoss=0.159, valLoss=0.159]\n",
      "Epoch 38/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 201.43batch/s, lastLoss=0.159, valLoss=0.156]\n",
      "Epoch 39/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.11batch/s, lastLoss=0.158, valLoss=0.153]\n",
      "Epoch 40/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.96batch/s, lastLoss=0.158, valLoss=0.162]\n",
      "Epoch 41/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 218.31batch/s, lastLoss=0.16, valLoss=0.151]\n",
      "Epoch 42/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.156, valLoss=0.155]\n",
      "Epoch 43/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s, lastLoss=0.157, valLoss=0.153]\n",
      "Epoch 44/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 203.64batch/s, lastLoss=0.156, valLoss=0.153]\n",
      "Epoch 45/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.47batch/s, lastLoss=0.157, valLoss=0.154]\n",
      "Epoch 46/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.38batch/s, lastLoss=0.158, valLoss=0.153]\n",
      "Epoch 47/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.90batch/s, lastLoss=0.156, valLoss=0.154]\n",
      "Epoch 48/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.89batch/s, lastLoss=0.157, valLoss=0.159]\n",
      "Epoch 49/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.55batch/s, lastLoss=0.155, valLoss=0.154]\n",
      "Epoch 50/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.155, valLoss=0.153]\n",
      "Epoch 51/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 209.35batch/s, lastLoss=0.156, valLoss=0.148]\n",
      "Epoch 52/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 225.35batch/s, lastLoss=0.155, valLoss=0.151]\n",
      "Epoch 53/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 218.75batch/s, lastLoss=0.154, valLoss=0.152]\n",
      "Epoch 54/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.48batch/s, lastLoss=0.154, valLoss=0.159]\n",
      "Epoch 55/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 220.04batch/s, lastLoss=0.156, valLoss=0.151]\n",
      "Epoch 56/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.74batch/s, lastLoss=0.154, valLoss=0.155]\n",
      "Epoch 57/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 212.12batch/s, lastLoss=0.153, valLoss=0.15]\n",
      "Epoch 58/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.156, valLoss=0.156]\n",
      "Epoch 59/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 224.90batch/s, lastLoss=0.153, valLoss=0.15]\n",
      "Epoch 60/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 220.47batch/s, lastLoss=0.156, valLoss=0.151]\n",
      "Epoch 61/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 220.48batch/s, lastLoss=0.153, valLoss=0.156]\n",
      "Epoch 62/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 221.35batch/s, lastLoss=0.152, valLoss=0.152]\n",
      "Epoch 63/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s, lastLoss=0.155, valLoss=0.151]\n",
      "Epoch 64/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 225.35batch/s, lastLoss=0.152, valLoss=0.153]\n",
      "Epoch 65/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.56batch/s, lastLoss=0.152, valLoss=0.148]\n",
      "Epoch 66/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s, lastLoss=0.151, valLoss=0.166]\n",
      "Epoch 67/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.33batch/s, lastLoss=0.152, valLoss=0.146]\n",
      "Epoch 68/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 212.52batch/s, lastLoss=0.149, valLoss=0.164]\n",
      "Epoch 69/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 218.32batch/s, lastLoss=0.15, valLoss=0.145]\n",
      "Epoch 70/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 222.22batch/s, lastLoss=0.15, valLoss=0.148]\n",
      "Epoch 71/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 215.80batch/s, lastLoss=0.149, valLoss=0.149]\n",
      "Epoch 72/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 216.63batch/s, lastLoss=0.149, valLoss=0.145]\n",
      "Epoch 73/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 219.03batch/s, lastLoss=0.15, valLoss=0.14]\n",
      "Epoch 74/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 200.01batch/s, lastLoss=0.147, valLoss=0.148]\n",
      "Epoch 75/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 198.92batch/s, lastLoss=0.145, valLoss=0.147]\n",
      "Epoch 76/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 207.78batch/s, lastLoss=0.149, valLoss=0.147]\n",
      "Epoch 77/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 200.37batch/s, lastLoss=0.146, valLoss=0.145]\n",
      "Epoch 78/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 214.15batch/s, lastLoss=0.145, valLoss=0.15]\n",
      "Epoch 79/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 178.06batch/s, lastLoss=0.149, valLoss=0.151]\n",
      "Epoch 80/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 214.14batch/s, lastLoss=0.15, valLoss=0.165]\n",
      "Epoch 81/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 217.04batch/s, lastLoss=0.153, valLoss=0.14]\n",
      "Epoch 82/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 213.74batch/s, lastLoss=0.146, valLoss=0.147]\n",
      "Epoch 83/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 211.73batch/s, lastLoss=0.145, valLoss=0.149]\n",
      "Epoch 84/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 223.54batch/s, lastLoss=0.146, valLoss=0.146]\n",
      "Epoch 85/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.15batch/s, lastLoss=0.146, valLoss=0.147]\n",
      "Epoch 86/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 177.78batch/s, lastLoss=0.146, valLoss=0.144]\n",
      "Epoch 87/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 217.04batch/s, lastLoss=0.146, valLoss=0.147]\n",
      "Epoch 88/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 220.04batch/s, lastLoss=0.146, valLoss=0.14]\n",
      "Epoch 89/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 218.31batch/s, lastLoss=0.145, valLoss=0.144]\n",
      "Epoch 90/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 214.96batch/s, lastLoss=0.145, valLoss=0.142]\n",
      "Epoch 91/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 221.34batch/s, lastLoss=0.143, valLoss=0.137]\n",
      "Epoch 92/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 221.35batch/s, lastLoss=0.144, valLoss=0.143]\n",
      "Epoch 93/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 220.91batch/s, lastLoss=0.143, valLoss=0.148]\n",
      "Epoch 94/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 220.04batch/s, lastLoss=0.146, valLoss=0.135]\n",
      "Epoch 95/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 220.30batch/s, lastLoss=0.144, valLoss=0.14]\n",
      "Epoch 96/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 205.88batch/s, lastLoss=0.144, valLoss=0.139]\n",
      "Epoch 97/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 227.19batch/s, lastLoss=0.142, valLoss=0.14]\n",
      "Epoch 98/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 219.18batch/s, lastLoss=0.146, valLoss=0.151]\n",
      "Epoch 99/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 214.56batch/s, lastLoss=0.146, valLoss=0.14]\n",
      "Epoch 100/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 213.33batch/s, lastLoss=0.148, valLoss=0.146]\n",
      "Epoch 101/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.143, valLoss=0.138]\n",
      "Epoch 102/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.78batch/s, lastLoss=0.144, valLoss=0.143]\n",
      "Epoch 103/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 213.05batch/s, lastLoss=0.143, valLoss=0.144]\n",
      "Epoch 104/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 223.09batch/s, lastLoss=0.144, valLoss=0.14]\n",
      "Epoch 105/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.43batch/s, lastLoss=0.143, valLoss=0.141]\n",
      "Epoch 106/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 234.31batch/s, lastLoss=0.143, valLoss=0.164]\n",
      "Epoch 107/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.55batch/s, lastLoss=0.144, valLoss=0.148]\n",
      "Epoch 108/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 210.13batch/s, lastLoss=0.145, valLoss=0.146]\n",
      "Epoch 109/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 195.80batch/s, lastLoss=0.142, valLoss=0.142]\n",
      "Epoch 110/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.05batch/s, lastLoss=0.142, valLoss=0.144]\n",
      "Epoch 111/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.05batch/s, lastLoss=0.143, valLoss=0.138]\n",
      "Epoch 112/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.51batch/s, lastLoss=0.145, valLoss=0.137]\n",
      "Epoch 113/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.29batch/s, lastLoss=0.143, valLoss=0.142]\n",
      "Epoch 114/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.61batch/s, lastLoss=0.142, valLoss=0.144]\n",
      "Epoch 115/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.17batch/s, lastLoss=0.143, valLoss=0.139]\n",
      "Epoch 116/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.92batch/s, lastLoss=0.142, valLoss=0.145]\n",
      "Epoch 117/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 219.61batch/s, lastLoss=0.142, valLoss=0.139]\n",
      "Epoch 118/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 221.78batch/s, lastLoss=0.142, valLoss=0.145]\n",
      "Epoch 119/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 226.72batch/s, lastLoss=0.142, valLoss=0.14]\n",
      "Epoch 120/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 226.73batch/s, lastLoss=0.143, valLoss=0.138]\n",
      "Epoch 121/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.55batch/s, lastLoss=0.143, valLoss=0.138]\n",
      "Epoch 122/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 225.80batch/s, lastLoss=0.14, valLoss=0.142]\n",
      "Epoch 123/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 223.65batch/s, lastLoss=0.144, valLoss=0.143]\n",
      "Epoch 124/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 232.37batch/s, lastLoss=0.146, valLoss=0.138]\n",
      "Epoch 125/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.35batch/s, lastLoss=0.142, valLoss=0.142]\n",
      "Epoch 126/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 228.11batch/s, lastLoss=0.144, valLoss=0.141]\n",
      "Epoch 127/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 217.52batch/s, lastLoss=0.143, valLoss=0.141]\n",
      "Epoch 128/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 216.66batch/s, lastLoss=0.142, valLoss=0.149]\n",
      "Epoch 129/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.142, valLoss=0.138]\n",
      "Epoch 130/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 225.81batch/s, lastLoss=0.141, valLoss=0.144]\n",
      "Epoch 131/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 222.64batch/s, lastLoss=0.143, valLoss=0.141]\n",
      "Epoch 132/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 220.47batch/s, lastLoss=0.142, valLoss=0.138]\n",
      "Epoch 133/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 224.45batch/s, lastLoss=0.142, valLoss=0.14]\n",
      "Epoch 134/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 213.54batch/s, lastLoss=0.143, valLoss=0.135]\n",
      "Epoch 135/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 232.83batch/s, lastLoss=0.139, valLoss=0.138]\n",
      "Epoch 136/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 224.45batch/s, lastLoss=0.142, valLoss=0.137]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 230.45batch/s, lastLoss=0.141, valLoss=0.143]\n",
      "Epoch 138/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 227.17batch/s, lastLoss=0.143, valLoss=0.146]\n",
      "Epoch 139/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 208.95batch/s, lastLoss=0.14, valLoss=0.144]\n",
      "Epoch 140/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 229.51batch/s, lastLoss=0.141, valLoss=0.135]\n",
      "Epoch 141/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.143, valLoss=0.143]\n",
      "Epoch 142/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 198.23batch/s, lastLoss=0.141, valLoss=0.138]\n",
      "Epoch 143/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 205.88batch/s, lastLoss=0.141, valLoss=0.148]\n",
      "Epoch 144/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 215.80batch/s, lastLoss=0.141, valLoss=0.147]\n",
      "Epoch 145/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 212.97batch/s, lastLoss=0.141, valLoss=0.138]\n",
      "Epoch 146/400:  57%|██████████████████▎             | 32/56 [00:00<00:00, 207.12batch/s, lastLoss=0.139, valLoss=0.139]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [330]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sRNN_train_loss, sRNN_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     25\u001b[0m         cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     26\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 27\u001b[0m         \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sRNN_train_loss, sRNN_val_loss =  train(train_loader, sRNN, val_loader=test_loader, LR=0.01, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829765f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b127b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnSingle = RNNSingle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "990527da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:02<00:00, 21.05batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.39batch/s, lastLoss=0.298, valLoss=0.223]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.46batch/s, lastLoss=0.261, valLoss=0.204]\n",
      "Epoch 4/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.40batch/s, lastLoss=0.246, valLoss=0.194]\n",
      "Epoch 5/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.47batch/s, lastLoss=0.241, valLoss=0.199]\n",
      "Epoch 6/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.50batch/s, lastLoss=0.234, valLoss=0.191]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.53batch/s, lastLoss=0.231, valLoss=0.204]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.45batch/s, lastLoss=0.227, valLoss=0.188]\n",
      "Epoch 9/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.51batch/s, lastLoss=0.221, valLoss=0.194]\n",
      "Epoch 10/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.38batch/s, lastLoss=0.22, valLoss=0.202]\n",
      "Epoch 11/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.51batch/s, lastLoss=0.219, valLoss=0.187]\n",
      "Epoch 12/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.51batch/s, lastLoss=0.483, valLoss=1.19]\n",
      "Epoch 13/400: 100%|████████████████████████████████████| 56/56 [00:02<00:00, 21.49batch/s, lastLoss=0.56, valLoss=0.25]\n",
      "Epoch 14/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.54batch/s, lastLoss=0.335, valLoss=0.323]\n",
      "Epoch 15/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.49batch/s, lastLoss=0.278, valLoss=0.262]\n",
      "Epoch 16/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.34batch/s, lastLoss=0.249, valLoss=0.253]\n",
      "Epoch 17/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.36batch/s, lastLoss=0.249, valLoss=0.253]\n",
      "Epoch 18/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.33batch/s, lastLoss=0.243, valLoss=0.244]\n",
      "Epoch 19/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.36batch/s, lastLoss=0.239, valLoss=0.236]\n",
      "Epoch 20/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.38batch/s, lastLoss=0.234, valLoss=0.246]\n",
      "Epoch 21/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.31batch/s, lastLoss=0.231, valLoss=0.22]\n",
      "Epoch 22/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.29batch/s, lastLoss=0.228, valLoss=0.245]\n",
      "Epoch 23/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.51batch/s, lastLoss=0.221, valLoss=0.238]\n",
      "Epoch 24/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.48batch/s, lastLoss=0.221, valLoss=0.218]\n",
      "Epoch 25/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.41batch/s, lastLoss=0.221, valLoss=0.219]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.41batch/s, lastLoss=0.216, valLoss=0.215]\n",
      "Epoch 27/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.43batch/s, lastLoss=0.215, valLoss=0.217]\n",
      "Epoch 28/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.26batch/s, lastLoss=0.216, valLoss=0.203]\n",
      "Epoch 29/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.32batch/s, lastLoss=0.212, valLoss=0.212]\n",
      "Epoch 30/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.31batch/s, lastLoss=0.212, valLoss=0.212]\n",
      "Epoch 31/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.28batch/s, lastLoss=0.21, valLoss=0.209]\n",
      "Epoch 32/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.19batch/s, lastLoss=0.208, valLoss=0.209]\n",
      "Epoch 33/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.27batch/s, lastLoss=0.209, valLoss=0.201]\n",
      "Epoch 34/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.25batch/s, lastLoss=0.207, valLoss=0.198]\n",
      "Epoch 35/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.44batch/s, lastLoss=0.205, valLoss=0.198]\n",
      "Epoch 36/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.50batch/s, lastLoss=0.203, valLoss=0.195]\n",
      "Epoch 37/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.47batch/s, lastLoss=0.201, valLoss=0.202]\n",
      "Epoch 38/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.53batch/s, lastLoss=0.205, valLoss=0.208]\n",
      "Epoch 39/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.57batch/s, lastLoss=0.202, valLoss=0.202]\n",
      "Epoch 40/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.58batch/s, lastLoss=0.198, valLoss=0.183]\n",
      "Epoch 41/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.59batch/s, lastLoss=0.197, valLoss=0.185]\n",
      "Epoch 42/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.63batch/s, lastLoss=0.195, valLoss=0.19]\n",
      "Epoch 43/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.58batch/s, lastLoss=0.193, valLoss=0.186]\n",
      "Epoch 44/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.60batch/s, lastLoss=0.193, valLoss=0.187]\n",
      "Epoch 45/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.67batch/s, lastLoss=0.193, valLoss=0.187]\n",
      "Epoch 46/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.46batch/s, lastLoss=0.194, valLoss=0.195]\n",
      "Epoch 47/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.50batch/s, lastLoss=0.192, valLoss=0.18]\n",
      "Epoch 48/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.27batch/s, lastLoss=0.194, valLoss=0.192]\n",
      "Epoch 49/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.32batch/s, lastLoss=0.193, valLoss=0.184]\n",
      "Epoch 50/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.36batch/s, lastLoss=0.193, valLoss=0.185]\n",
      "Epoch 51/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.52batch/s, lastLoss=0.192, valLoss=0.188]\n",
      "Epoch 52/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.35batch/s, lastLoss=0.193, valLoss=0.186]\n",
      "Epoch 53/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.17batch/s, lastLoss=0.191, valLoss=0.19]\n",
      "Epoch 54/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.52batch/s, lastLoss=0.192, valLoss=0.185]\n",
      "Epoch 55/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.29batch/s, lastLoss=0.193, valLoss=0.181]\n",
      "Epoch 56/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.02batch/s, lastLoss=0.191, valLoss=0.182]\n",
      "Epoch 57/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 20.83batch/s, lastLoss=0.192, valLoss=0.183]\n",
      "Epoch 58/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.32batch/s, lastLoss=0.193, valLoss=0.188]\n",
      "Epoch 59/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.48batch/s, lastLoss=0.192, valLoss=0.182]\n",
      "Epoch 60/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.12batch/s, lastLoss=0.192, valLoss=0.189]\n",
      "Epoch 61/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.58batch/s, lastLoss=0.191, valLoss=0.183]\n",
      "Epoch 62/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.64batch/s, lastLoss=0.191, valLoss=0.185]\n",
      "Epoch 63/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.63batch/s, lastLoss=0.192, valLoss=0.185]\n",
      "Epoch 64/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.58batch/s, lastLoss=0.191, valLoss=0.191]\n",
      "Epoch 65/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.38batch/s, lastLoss=0.192, valLoss=0.185]\n",
      "Epoch 66/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.41batch/s, lastLoss=0.192, valLoss=0.187]\n",
      "Epoch 67/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 20.79batch/s, lastLoss=0.191, valLoss=0.182]\n",
      "Epoch 68/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.34batch/s, lastLoss=0.192, valLoss=0.185]\n",
      "Epoch 69/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.46batch/s, lastLoss=0.191, valLoss=0.185]\n",
      "Epoch 70/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.37batch/s, lastLoss=0.191, valLoss=0.185]\n",
      "Epoch 71/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.43batch/s, lastLoss=0.191, valLoss=0.187]\n",
      "Epoch 72/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.50batch/s, lastLoss=0.191, valLoss=0.18]\n",
      "Epoch 73/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 20.98batch/s, lastLoss=0.191, valLoss=0.182]\n",
      "Epoch 74/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.40batch/s, lastLoss=0.19, valLoss=0.183]\n",
      "Epoch 75/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.28batch/s, lastLoss=0.191, valLoss=0.187]\n",
      "Epoch 76/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.11batch/s, lastLoss=0.191, valLoss=0.182]\n",
      "Epoch 77/400: 100%|████████████████████████████████████| 56/56 [00:02<00:00, 21.37batch/s, lastLoss=0.19, valLoss=0.19]\n",
      "Epoch 78/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.51batch/s, lastLoss=0.191, valLoss=0.19]\n",
      "Epoch 79/400: 100%|██████████████████████████████████| 56/56 [00:02<00:00, 21.51batch/s, lastLoss=0.191, valLoss=0.189]\n",
      "Epoch 80/400: 100%|███████████████████████████████████| 56/56 [00:02<00:00, 21.39batch/s, lastLoss=0.19, valLoss=0.188]\n",
      "Epoch 81/400:  68%|███████████████████████▊           | 38/56 [00:01<00:00, 20.95batch/s, lastLoss=0.19, valLoss=0.185]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [120]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rnnSingle_train_loss, rnnSingle_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnnSingle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     33\u001b[0m         cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     34\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 35\u001b[0m         \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnnSingle_train_loss, rnnSingle_val_loss =  train(train_loader, rnnSingle, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9ec56b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b2166f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:01<00:00, 41.16batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 42.28batch/s, lastLoss=0.243, valLoss=0.189]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.88batch/s, lastLoss=0.198, valLoss=0.184]\n",
      "Epoch 4/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 42.15batch/s, lastLoss=0.196, valLoss=0.184]\n",
      "Epoch 5/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.99batch/s, lastLoss=0.195, valLoss=0.184]\n",
      "Epoch 6/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 42.06batch/s, lastLoss=0.194, valLoss=0.184]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 42.03batch/s, lastLoss=0.194, valLoss=0.194]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 42.07batch/s, lastLoss=0.192, valLoss=0.183]\n",
      "Epoch 9/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.54batch/s, lastLoss=0.193, valLoss=0.187]\n",
      "Epoch 10/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.96batch/s, lastLoss=0.193, valLoss=0.189]\n",
      "Epoch 11/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.88batch/s, lastLoss=0.191, valLoss=0.182]\n",
      "Epoch 12/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 42.07batch/s, lastLoss=0.191, valLoss=0.184]\n",
      "Epoch 13/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.53batch/s, lastLoss=0.192, valLoss=0.183]\n",
      "Epoch 14/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.88batch/s, lastLoss=0.191, valLoss=0.18]\n",
      "Epoch 15/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.65batch/s, lastLoss=0.19, valLoss=0.182]\n",
      "Epoch 16/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.45batch/s, lastLoss=0.191, valLoss=0.177]\n",
      "Epoch 17/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.73batch/s, lastLoss=0.192, valLoss=0.187]\n",
      "Epoch 18/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.78batch/s, lastLoss=0.19, valLoss=0.181]\n",
      "Epoch 19/400: 100%|████████████████████████████████████| 56/56 [00:01<00:00, 41.79batch/s, lastLoss=0.19, valLoss=0.18]\n",
      "Epoch 20/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.92batch/s, lastLoss=0.189, valLoss=0.188]\n",
      "Epoch 21/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.64batch/s, lastLoss=0.191, valLoss=0.189]\n",
      "Epoch 22/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.40batch/s, lastLoss=0.19, valLoss=0.182]\n",
      "Epoch 23/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 40.85batch/s, lastLoss=0.19, valLoss=0.183]\n",
      "Epoch 24/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.73batch/s, lastLoss=0.19, valLoss=0.177]\n",
      "Epoch 25/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.19batch/s, lastLoss=0.19, valLoss=0.187]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.85batch/s, lastLoss=0.189, valLoss=0.184]\n",
      "Epoch 27/400: 100%|████████████████████████████████████| 56/56 [00:01<00:00, 41.19batch/s, lastLoss=0.19, valLoss=0.18]\n",
      "Epoch 28/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.82batch/s, lastLoss=0.189, valLoss=0.183]\n",
      "Epoch 29/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.78batch/s, lastLoss=0.189, valLoss=0.182]\n",
      "Epoch 30/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.36batch/s, lastLoss=0.19, valLoss=0.189]\n",
      "Epoch 31/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.16batch/s, lastLoss=0.19, valLoss=0.183]\n",
      "Epoch 32/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.36batch/s, lastLoss=0.189, valLoss=0.18]\n",
      "Epoch 33/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.53batch/s, lastLoss=0.189, valLoss=0.19]\n",
      "Epoch 34/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.50batch/s, lastLoss=0.19, valLoss=0.189]\n",
      "Epoch 35/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.65batch/s, lastLoss=0.189, valLoss=0.19]\n",
      "Epoch 36/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.93batch/s, lastLoss=0.189, valLoss=0.183]\n",
      "Epoch 37/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.79batch/s, lastLoss=0.189, valLoss=0.192]\n",
      "Epoch 38/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.36batch/s, lastLoss=0.188, valLoss=0.188]\n",
      "Epoch 39/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.51batch/s, lastLoss=0.188, valLoss=0.185]\n",
      "Epoch 40/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.88batch/s, lastLoss=0.189, valLoss=0.18]\n",
      "Epoch 41/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.74batch/s, lastLoss=0.189, valLoss=0.186]\n",
      "Epoch 42/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 41.85batch/s, lastLoss=0.189, valLoss=0.18]\n",
      "Epoch 43/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.22batch/s, lastLoss=0.188, valLoss=0.185]\n",
      "Epoch 44/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.65batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 45/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.65batch/s, lastLoss=0.189, valLoss=0.189]\n",
      "Epoch 46/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.60batch/s, lastLoss=0.187, valLoss=0.188]\n",
      "Epoch 47/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.53batch/s, lastLoss=0.189, valLoss=0.184]\n",
      "Epoch 48/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.48batch/s, lastLoss=0.189, valLoss=0.187]\n",
      "Epoch 49/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.62batch/s, lastLoss=0.188, valLoss=0.185]\n",
      "Epoch 50/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.53batch/s, lastLoss=0.188, valLoss=0.187]\n",
      "Epoch 51/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.39batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 52/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.65batch/s, lastLoss=0.188, valLoss=0.181]\n",
      "Epoch 53/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.36batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 54/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.24batch/s, lastLoss=0.187, valLoss=0.183]\n",
      "Epoch 55/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.62batch/s, lastLoss=0.188, valLoss=0.183]\n",
      "Epoch 56/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 41.39batch/s, lastLoss=0.189, valLoss=0.186]\n",
      "Epoch 57/400:  36%|████████████▌                      | 20/56 [00:00<00:00, 39.41batch/s, lastLoss=0.188, valLoss=0.18]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [145]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lstm_train_loss, lstm_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     34\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m         cost\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 36\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n\u001b[0;32m     39\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_train_loss, lstm_val_loss =  train(train_loader, lstm, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2b35",
   "metadata": {},
   "source": [
    "# Try not flattened stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "e4668d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(signals, wavelet='db4', levels=3):\n",
    "    \"\"\"\n",
    "    Extract wavelet packet features for each signal in the input array.\n",
    "\n",
    "    Args:\n",
    "    - signals (numpy.ndarray): a 2D array containing two 1D signals\n",
    "    - wavelet (str): the name of the wavelet to use for decomposition\n",
    "    - level (int): the level of wavelet packet decomposition to use\n",
    "\n",
    "    Returns:\n",
    "    - features (dict): a dictionary containing the wavelet packet features for each signal\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionary to store features for each signal\n",
    "    features = []\n",
    "\n",
    "    # Loop over each signal\n",
    "    for i in range(2):\n",
    "        # Perform wavelet packet decomposition\n",
    "        wp = pywt.WaveletPacket(data=signals[i], wavelet=wavelet, mode='symmetric', maxlevel=levels)\n",
    "        \n",
    "        coeffs = []\n",
    "        for level in range(1, levels+1):\n",
    "            level_coeffs = np.array([n.data for n in wp.get_level(level, 'freq')])  # get the coefficients at the current level\n",
    "            \n",
    "            stats = np.array([\n",
    "                [np.mean(level_coeffs[i]), np.std(level_coeffs[i]), np.ptp(level_coeffs[i])]\n",
    "                 for i in range(len(level_coeffs))\n",
    "                ])\n",
    "            for i in range(len(level_coeffs)):\n",
    "                coeffs.append([np.mean(level_coeffs[i]), np.std(level_coeffs[i]), np.ptp(level_coeffs[i])])\n",
    "\n",
    "        # Stack the coefficients vertically to create the final array\n",
    "        features.append(coeffs)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "9ebd988c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([[0, 1, 2], [0, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "cd293016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(x, wavelet='db1', level=4):\n",
    "# perform wavelet packet decomposition on signal 1\n",
    "    wp = pywt.WaveletPacket(x[0], wavelet, mode='symmetric', maxlevel=level)\n",
    "    coeffs1 = []\n",
    "    for node in wp.get_level(level, 'natural'):\n",
    "        if node.path.endswith('a') or node.path.endswith('d'):\n",
    "            coeffs1.append(node.data)\n",
    "    coeffs1 = np.concatenate(coeffs1)\n",
    "    \n",
    "    # perform wavelet packet decomposition on signal 2\n",
    "    wp = pywt.WaveletPacket(x[1], wavelet, mode='symmetric', maxlevel=level)\n",
    "    coeffs2 = []\n",
    "    for node in wp.get_level(level, 'natural'):\n",
    "        if node.path.endswith('a') or node.path.endswith('d'):\n",
    "            coeffs2.append(node.data)\n",
    "    coeffs2 = np.concatenate(coeffs2)\n",
    "    \n",
    "    # concatenate the two coefficient arrays\n",
    "    feature_vector = np.concatenate((coeffs1, coeffs2))\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "1fb82fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "437196d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:08<00:00, 254.01it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, scaler = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "1400b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = getSplit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "a403f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSingle(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=512, num_layers=3, output_dim=6):\n",
    "        super(RNNSingle, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn1 = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.rnn2 = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(21504, 1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, 2)\n",
    "        \n",
    "        h0 = torch.zeros(3, batch_size, self.hidden_dim).to(x.device)\n",
    "        x, _ = self.rnn(x, h0)\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "14c5703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, num_layers=2, output_dim=6):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 6)\n",
    "        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1, 2)\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Initialize cell state with zeros\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # Move tensors to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "            x = x.cuda()\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "006981e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRNN(nn.Module):\n",
    "    def __init__(self, input_size=256, hidden_size=1024):\n",
    "        super(SiameseRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "                \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 6)\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, 1, -1)\n",
    "        \n",
    "        #x1 = x[:, 0, :, :].reshape(batch_size, 1, -1)\n",
    "        #x2 = x[:, 0, :, :].reshape(batch_size, 1, -1)\n",
    "        \n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n",
    "        _, h1 = self.rnn(x, h0)  # Add a batch dimension\n",
    "       # _, h2 = self.rnn(x2, h0)  # Add a batch dimension\n",
    "        \n",
    "       # out = torch.cat([h1[-1], h2[-1]], dim=1)\n",
    "        out=h1[-1]\n",
    "        out = out.reshape(batch_size, -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "1cfbc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "sRNN = SiameseRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "997857e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|█████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 167.15batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 169.43batch/s, lastLoss=0.263, valLoss=0.19]\n",
      "Epoch 3/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 180.93batch/s, lastLoss=0.182, valLoss=0.185]\n",
      "Epoch 4/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 167.92batch/s, lastLoss=0.18, valLoss=0.185]\n",
      "Epoch 5/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 177.22batch/s, lastLoss=0.169, valLoss=0.169]\n",
      "Epoch 6/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 167.42batch/s, lastLoss=0.161, valLoss=0.158]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 175.82batch/s, lastLoss=0.157, valLoss=0.16]\n",
      "Epoch 8/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 183.90batch/s, lastLoss=0.154, valLoss=0.153]\n",
      "Epoch 9/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 168.17batch/s, lastLoss=0.152, valLoss=0.15]\n",
      "Epoch 10/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 179.19batch/s, lastLoss=0.148, valLoss=0.143]\n",
      "Epoch 11/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 176.66batch/s, lastLoss=0.144, valLoss=0.144]\n",
      "Epoch 12/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 184.51batch/s, lastLoss=0.143, valLoss=0.139]\n",
      "Epoch 13/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 180.64batch/s, lastLoss=0.14, valLoss=0.139]\n",
      "Epoch 14/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 180.92batch/s, lastLoss=0.139, valLoss=0.138]\n",
      "Epoch 15/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 182.70batch/s, lastLoss=0.137, valLoss=0.142]\n",
      "Epoch 16/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 178.06batch/s, lastLoss=0.136, valLoss=0.138]\n",
      "Epoch 17/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 184.81batch/s, lastLoss=0.132, valLoss=0.132]\n",
      "Epoch 18/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 180.94batch/s, lastLoss=0.126, valLoss=0.121]\n",
      "Epoch 19/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 179.19batch/s, lastLoss=0.121, valLoss=0.121]\n",
      "Epoch 20/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 182.40batch/s, lastLoss=0.118, valLoss=0.119]\n",
      "Epoch 21/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 172.57batch/s, lastLoss=0.115, valLoss=0.117]\n",
      "Epoch 22/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 180.94batch/s, lastLoss=0.113, valLoss=0.11]\n",
      "Epoch 23/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 182.40batch/s, lastLoss=0.112, valLoss=0.114]\n",
      "Epoch 24/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 179.77batch/s, lastLoss=0.109, valLoss=0.113]\n",
      "Epoch 25/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 181.52batch/s, lastLoss=0.108, valLoss=0.122]\n",
      "Epoch 26/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 181.82batch/s, lastLoss=0.11, valLoss=0.111]\n",
      "Epoch 27/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 182.71batch/s, lastLoss=0.107, valLoss=0.1]\n",
      "Epoch 28/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 181.82batch/s, lastLoss=0.106, valLoss=0.105]\n",
      "Epoch 29/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 180.64batch/s, lastLoss=0.103, valLoss=0.108]\n",
      "Epoch 30/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 173.91batch/s, lastLoss=0.102, valLoss=0.0983]\n",
      "Epoch 31/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 182.12batch/s, lastLoss=0.104, valLoss=0.103]\n",
      "Epoch 32/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 183.00batch/s, lastLoss=0.1, valLoss=0.105]\n",
      "Epoch 33/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 178.63batch/s, lastLoss=0.101, valLoss=0.0952]\n",
      "Epoch 34/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 172.57batch/s, lastLoss=0.1, valLoss=0.0946]\n",
      "Epoch 35/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 179.49batch/s, lastLoss=0.0977, valLoss=0.107]\n",
      "Epoch 36/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 173.64batch/s, lastLoss=0.098, valLoss=0.0927]\n",
      "Epoch 37/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 180.34batch/s, lastLoss=0.0978, valLoss=0.11]\n",
      "Epoch 38/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 175.82batch/s, lastLoss=0.102, valLoss=0.0928]\n",
      "Epoch 39/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 170.73batch/s, lastLoss=0.0952, valLoss=0.0964]\n",
      "Epoch 40/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 181.51batch/s, lastLoss=0.0944, valLoss=0.093]\n",
      "Epoch 41/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 176.38batch/s, lastLoss=0.0945, valLoss=0.0944]\n",
      "Epoch 42/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 176.93batch/s, lastLoss=0.0947, valLoss=0.0972]\n",
      "Epoch 43/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 178.91batch/s, lastLoss=0.0942, valLoss=0.0959]\n",
      "Epoch 44/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 177.49batch/s, lastLoss=0.0937, valLoss=0.0911]\n",
      "Epoch 45/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 177.21batch/s, lastLoss=0.0943, valLoss=0.0972]\n",
      "Epoch 46/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 174.73batch/s, lastLoss=0.0921, valLoss=0.0944]\n",
      "Epoch 47/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 161.62batch/s, lastLoss=0.092, valLoss=0.0928]\n",
      "Epoch 48/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 177.49batch/s, lastLoss=0.091, valLoss=0.09]\n",
      "Epoch 49/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 170.21batch/s, lastLoss=0.0937, valLoss=0.101]\n",
      "Epoch 50/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 177.49batch/s, lastLoss=0.0921, valLoss=0.0913]\n",
      "Epoch 51/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 178.06batch/s, lastLoss=0.0891, valLoss=0.0874]\n",
      "Epoch 52/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 176.65batch/s, lastLoss=0.0912, valLoss=0.0893]\n",
      "Epoch 53/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 175.82batch/s, lastLoss=0.0897, valLoss=0.087]\n",
      "Epoch 54/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 169.70batch/s, lastLoss=0.0897, valLoss=0.0954]\n",
      "Epoch 55/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 176.38batch/s, lastLoss=0.0897, valLoss=0.0903]\n",
      "Epoch 56/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 165.93batch/s, lastLoss=0.0876, valLoss=0.0971]\n",
      "Epoch 57/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 167.16batch/s, lastLoss=0.0893, valLoss=0.0856]\n",
      "Epoch 58/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 162.56batch/s, lastLoss=0.0899, valLoss=0.0929]\n",
      "Epoch 59/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 168.17batch/s, lastLoss=0.0898, valLoss=0.0949]\n",
      "Epoch 60/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 158.19batch/s, lastLoss=0.0893, valLoss=0.0863]\n",
      "Epoch 61/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 177.78batch/s, lastLoss=0.0883, valLoss=0.107]\n",
      "Epoch 62/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 176.92batch/s, lastLoss=0.0888, valLoss=0.0854]\n",
      "Epoch 63/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 176.94batch/s, lastLoss=0.0872, valLoss=0.0862]\n",
      "Epoch 64/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 167.16batch/s, lastLoss=0.0878, valLoss=0.0839]\n",
      "Epoch 65/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 175.27batch/s, lastLoss=0.087, valLoss=0.0882]\n",
      "Epoch 66/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 165.43batch/s, lastLoss=0.0867, valLoss=0.0843]\n",
      "Epoch 67/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 161.62batch/s, lastLoss=0.09, valLoss=0.0924]\n",
      "Epoch 68/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 131.46batch/s, lastLoss=0.088, valLoss=0.0912]\n",
      "Epoch 69/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 154.91batch/s, lastLoss=0.0909, valLoss=0.0912]\n",
      "Epoch 70/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 155.56batch/s, lastLoss=0.0876, valLoss=0.0834]\n",
      "Epoch 71/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 184.51batch/s, lastLoss=0.0895, valLoss=0.0989]\n",
      "Epoch 72/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 166.41batch/s, lastLoss=0.0874, valLoss=0.0848]\n",
      "Epoch 73/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 188.88batch/s, lastLoss=0.0863, valLoss=0.0857]\n",
      "Epoch 74/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 184.20batch/s, lastLoss=0.0858, valLoss=0.0819]\n",
      "Epoch 75/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 172.84batch/s, lastLoss=0.0865, valLoss=0.0912]\n",
      "Epoch 76/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 184.51batch/s, lastLoss=0.0869, valLoss=0.0911]\n",
      "Epoch 77/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 175.83batch/s, lastLoss=0.0896, valLoss=0.0869]\n",
      "Epoch 78/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 141.95batch/s, lastLoss=0.0866, valLoss=0.0857]\n",
      "Epoch 79/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 155.34batch/s, lastLoss=0.0867, valLoss=0.0876]\n",
      "Epoch 80/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 155.77batch/s, lastLoss=0.0856, valLoss=0.0974]\n",
      "Epoch 81/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 170.99batch/s, lastLoss=0.0869, valLoss=0.0862]\n",
      "Epoch 82/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 156.43batch/s, lastLoss=0.0873, valLoss=0.0886]\n",
      "Epoch 83/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 150.34batch/s, lastLoss=0.086, valLoss=0.0862]\n",
      "Epoch 84/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.0874, valLoss=0.093]\n",
      "Epoch 85/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 165.44batch/s, lastLoss=0.0863, valLoss=0.0824]\n",
      "Epoch 86/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 189.51batch/s, lastLoss=0.0855, valLoss=0.0885]\n",
      "Epoch 87/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 176.66batch/s, lastLoss=0.0863, valLoss=0.0831]\n",
      "Epoch 88/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 181.23batch/s, lastLoss=0.0861, valLoss=0.0921]\n",
      "Epoch 89/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 177.01batch/s, lastLoss=0.0856, valLoss=0.0836]\n",
      "Epoch 90/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 178.63batch/s, lastLoss=0.0852, valLoss=0.0897]\n",
      "Epoch 91/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 193.10batch/s, lastLoss=0.0848, valLoss=0.086]\n",
      "Epoch 92/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 181.41batch/s, lastLoss=0.0847, valLoss=0.0854]\n",
      "Epoch 93/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 188.24batch/s, lastLoss=0.0843, valLoss=0.0882]\n",
      "Epoch 94/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 193.09batch/s, lastLoss=0.086, valLoss=0.0868]\n",
      "Epoch 95/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 176.10batch/s, lastLoss=0.0859, valLoss=0.0892]\n",
      "Epoch 96/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 190.15batch/s, lastLoss=0.0845, valLoss=0.0877]\n",
      "Epoch 97/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 178.91batch/s, lastLoss=0.0861, valLoss=0.0908]\n",
      "Epoch 98/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 192.11batch/s, lastLoss=0.0845, valLoss=0.0884]\n",
      "Epoch 99/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 195.73batch/s, lastLoss=0.0844, valLoss=0.0825]\n",
      "Epoch 100/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 193.77batch/s, lastLoss=0.0862, valLoss=0.0965]\n",
      "Epoch 101/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 184.21batch/s, lastLoss=0.0846, valLoss=0.0924]\n",
      "Epoch 102/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.15batch/s, lastLoss=0.0853, valLoss=0.0873]\n",
      "Epoch 103/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.89batch/s, lastLoss=0.0866, valLoss=0.0911]\n",
      "Epoch 104/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 203.63batch/s, lastLoss=0.0853, valLoss=0.0864]\n",
      "Epoch 105/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.88batch/s, lastLoss=0.0841, valLoss=0.0947]\n",
      "Epoch 106/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.30batch/s, lastLoss=0.0849, valLoss=0.0858]\n",
      "Epoch 107/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.56batch/s, lastLoss=0.0837, valLoss=0.0897]\n",
      "Epoch 108/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.96batch/s, lastLoss=0.0853, valLoss=0.0846]\n",
      "Epoch 109/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.03batch/s, lastLoss=0.0848, valLoss=0.0857]\n",
      "Epoch 110/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.72batch/s, lastLoss=0.0839, valLoss=0.0894]\n",
      "Epoch 111/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 194.36batch/s, lastLoss=0.0834, valLoss=0.0908]\n",
      "Epoch 112/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.94batch/s, lastLoss=0.0845, valLoss=0.0856]\n",
      "Epoch 113/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 197.88batch/s, lastLoss=0.0829, valLoss=0.087]\n",
      "Epoch 114/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 204.76batch/s, lastLoss=0.0846, valLoss=0.083]\n",
      "Epoch 115/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 200.06batch/s, lastLoss=0.086, valLoss=0.0871]\n",
      "Epoch 116/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.01batch/s, lastLoss=0.0834, valLoss=0.0834]\n",
      "Epoch 117/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 199.77batch/s, lastLoss=0.084, valLoss=0.0869]\n",
      "Epoch 118/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 192.97batch/s, lastLoss=0.0845, valLoss=0.0834]\n",
      "Epoch 119/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.47batch/s, lastLoss=0.0841, valLoss=0.0874]\n",
      "Epoch 120/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.21batch/s, lastLoss=0.0851, valLoss=0.0885]\n",
      "Epoch 121/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 194.93batch/s, lastLoss=0.083, valLoss=0.0874]\n",
      "Epoch 122/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.98batch/s, lastLoss=0.0836, valLoss=0.0851]\n",
      "Epoch 123/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.49batch/s, lastLoss=0.0833, valLoss=0.0944]\n",
      "Epoch 124/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 172.60batch/s, lastLoss=0.0835, valLoss=0.0854]\n",
      "Epoch 125/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 172.88batch/s, lastLoss=0.0857, valLoss=0.0846]\n",
      "Epoch 126/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 188.87batch/s, lastLoss=0.084, valLoss=0.0877]\n",
      "Epoch 127/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.08batch/s, lastLoss=0.0835, valLoss=0.0864]\n",
      "Epoch 128/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.70batch/s, lastLoss=0.0841, valLoss=0.0838]\n",
      "Epoch 129/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.32batch/s, lastLoss=0.0846, valLoss=0.0819]\n",
      "Epoch 130/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.08batch/s, lastLoss=0.0846, valLoss=0.0838]\n",
      "Epoch 131/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 186.53batch/s, lastLoss=0.0833, valLoss=0.0836]\n",
      "Epoch 132/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 186.36batch/s, lastLoss=0.0837, valLoss=0.0824]\n",
      "Epoch 133/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 191.61batch/s, lastLoss=0.0856, valLoss=0.0829]\n",
      "Epoch 134/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 206.00batch/s, lastLoss=0.083, valLoss=0.0845]\n",
      "Epoch 135/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 200.00batch/s, lastLoss=0.0839, valLoss=0.0836]\n",
      "Epoch 136/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.12batch/s, lastLoss=0.0834, valLoss=0.0846]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.72batch/s, lastLoss=0.0837, valLoss=0.0984]\n",
      "Epoch 138/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.49batch/s, lastLoss=0.0838, valLoss=0.0833]\n",
      "Epoch 139/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.92batch/s, lastLoss=0.0828, valLoss=0.0862]\n",
      "Epoch 140/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 203.47batch/s, lastLoss=0.0821, valLoss=0.0926]\n",
      "Epoch 141/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.87batch/s, lastLoss=0.0829, valLoss=0.0878]\n",
      "Epoch 142/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.07batch/s, lastLoss=0.0849, valLoss=0.0837]\n",
      "Epoch 143/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.64batch/s, lastLoss=0.0827, valLoss=0.0827]\n",
      "Epoch 144/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.45batch/s, lastLoss=0.0828, valLoss=0.0987]\n",
      "Epoch 145/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.11batch/s, lastLoss=0.0863, valLoss=0.0864]\n",
      "Epoch 146/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 205.96batch/s, lastLoss=0.0828, valLoss=0.084]\n",
      "Epoch 147/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.69batch/s, lastLoss=0.0833, valLoss=0.0903]\n",
      "Epoch 148/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.49batch/s, lastLoss=0.0831, valLoss=0.0846]\n",
      "Epoch 149/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 188.94batch/s, lastLoss=0.083, valLoss=0.099]\n",
      "Epoch 150/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 203.70batch/s, lastLoss=0.0824, valLoss=0.0871]\n",
      "Epoch 151/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 187.29batch/s, lastLoss=0.0835, valLoss=0.0854]\n",
      "Epoch 152/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.53batch/s, lastLoss=0.0833, valLoss=0.0861]\n",
      "Epoch 153/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 193.69batch/s, lastLoss=0.0816, valLoss=0.0899]\n",
      "Epoch 154/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.0824, valLoss=0.087]\n",
      "Epoch 155/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 207.20batch/s, lastLoss=0.0841, valLoss=0.096]\n",
      "Epoch 156/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.40batch/s, lastLoss=0.0837, valLoss=0.0886]\n",
      "Epoch 157/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.23batch/s, lastLoss=0.0856, valLoss=0.0858]\n",
      "Epoch 158/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 191.28batch/s, lastLoss=0.0825, valLoss=0.085]\n",
      "Epoch 159/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.21batch/s, lastLoss=0.0864, valLoss=0.0888]\n",
      "Epoch 160/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.36batch/s, lastLoss=0.0823, valLoss=0.0879]\n",
      "Epoch 161/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 202.17batch/s, lastLoss=0.085, valLoss=0.0846]\n",
      "Epoch 162/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.12batch/s, lastLoss=0.0847, valLoss=0.0831]\n",
      "Epoch 163/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.16batch/s, lastLoss=0.0835, valLoss=0.0881]\n",
      "Epoch 164/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 186.98batch/s, lastLoss=0.0837, valLoss=0.084]\n",
      "Epoch 165/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 188.55batch/s, lastLoss=0.084, valLoss=0.0908]\n",
      "Epoch 166/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 190.47batch/s, lastLoss=0.0836, valLoss=0.0844]\n",
      "Epoch 167/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.0834, valLoss=0.0877]\n",
      "Epoch 168/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 182.72batch/s, lastLoss=0.0836, valLoss=0.0846]\n",
      "Epoch 169/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.15batch/s, lastLoss=0.0817, valLoss=0.0884]\n",
      "Epoch 170/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 200.21batch/s, lastLoss=0.0835, valLoss=0.0889]\n",
      "Epoch 171/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.80batch/s, lastLoss=0.0831, valLoss=0.0881]\n",
      "Epoch 172/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.91batch/s, lastLoss=0.0837, valLoss=0.0949]\n",
      "Epoch 173/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 187.60batch/s, lastLoss=0.0833, valLoss=0.0859]\n",
      "Epoch 174/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 169.96batch/s, lastLoss=0.0818, valLoss=0.0918]\n",
      "Epoch 175/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 175.80batch/s, lastLoss=0.0817, valLoss=0.0818]\n",
      "Epoch 176/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 179.49batch/s, lastLoss=0.0831, valLoss=0.0851]\n",
      "Epoch 177/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 187.27batch/s, lastLoss=0.0842, valLoss=0.0861]\n",
      "Epoch 178/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 177.78batch/s, lastLoss=0.0847, valLoss=0.0916]\n",
      "Epoch 179/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 190.48batch/s, lastLoss=0.084, valLoss=0.0863]\n",
      "Epoch 180/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 204.97batch/s, lastLoss=0.082, valLoss=0.0853]\n",
      "Epoch 181/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.18batch/s, lastLoss=0.0848, valLoss=0.0836]\n",
      "Epoch 182/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.11batch/s, lastLoss=0.0821, valLoss=0.0837]\n",
      "Epoch 183/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 203.93batch/s, lastLoss=0.0831, valLoss=0.0859]\n",
      "Epoch 184/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 182.71batch/s, lastLoss=0.0833, valLoss=0.0808]\n",
      "Epoch 185/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.02batch/s, lastLoss=0.0819, valLoss=0.0938]\n",
      "Epoch 186/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 181.23batch/s, lastLoss=0.0841, valLoss=0.0865]\n",
      "Epoch 187/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 192.77batch/s, lastLoss=0.0826, valLoss=0.0868]\n",
      "Epoch 188/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.40batch/s, lastLoss=0.0834, valLoss=0.0847]\n",
      "Epoch 189/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 204.68batch/s, lastLoss=0.0813, valLoss=0.104]\n",
      "Epoch 190/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.68batch/s, lastLoss=0.0841, valLoss=0.0862]\n",
      "Epoch 191/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 208.86batch/s, lastLoss=0.083, valLoss=0.0862]\n",
      "Epoch 192/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.14batch/s, lastLoss=0.0836, valLoss=0.0939]\n",
      "Epoch 193/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.77batch/s, lastLoss=0.0842, valLoss=0.0835]\n",
      "Epoch 194/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.03batch/s, lastLoss=0.0829, valLoss=0.0905]\n",
      "Epoch 195/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 210.62batch/s, lastLoss=0.0831, valLoss=0.095]\n",
      "Epoch 196/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 169.44batch/s, lastLoss=0.0815, valLoss=0.0924]\n",
      "Epoch 197/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 182.12batch/s, lastLoss=0.0819, valLoss=0.0818]\n",
      "Epoch 198/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 157.52batch/s, lastLoss=0.0842, valLoss=0.0893]\n",
      "Epoch 199/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 162.79batch/s, lastLoss=0.0843, valLoss=0.082]\n",
      "Epoch 200/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 175.82batch/s, lastLoss=0.0817, valLoss=0.0883]\n",
      "Epoch 201/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 173.37batch/s, lastLoss=0.0823, valLoss=0.0955]\n",
      "Epoch 202/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 173.64batch/s, lastLoss=0.0827, valLoss=0.0871]\n",
      "Epoch 203/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 158.87batch/s, lastLoss=0.0818, valLoss=0.0878]\n",
      "Epoch 204/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 157.08batch/s, lastLoss=0.0819, valLoss=0.0907]\n",
      "Epoch 205/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 163.49batch/s, lastLoss=0.0816, valLoss=0.0865]\n",
      "Epoch 206/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 174.18batch/s, lastLoss=0.0811, valLoss=0.0853]\n",
      "Epoch 207/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 194.45batch/s, lastLoss=0.0813, valLoss=0.086]\n",
      "Epoch 208/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 181.22batch/s, lastLoss=0.083, valLoss=0.0906]\n",
      "Epoch 209/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 178.91batch/s, lastLoss=0.0819, valLoss=0.0828]\n",
      "Epoch 210/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 188.24batch/s, lastLoss=0.082, valLoss=0.0845]\n",
      "Epoch 211/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.34batch/s, lastLoss=0.0813, valLoss=0.0907]\n",
      "Epoch 212/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 186.72batch/s, lastLoss=0.0826, valLoss=0.0892]\n",
      "Epoch 213/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.53batch/s, lastLoss=0.0823, valLoss=0.0841]\n",
      "Epoch 214/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 202.16batch/s, lastLoss=0.081, valLoss=0.0865]\n",
      "Epoch 215/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.21batch/s, lastLoss=0.0816, valLoss=0.0838]\n",
      "Epoch 216/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.23batch/s, lastLoss=0.0807, valLoss=0.0841]\n",
      "Epoch 217/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 188.87batch/s, lastLoss=0.0827, valLoss=0.085]\n",
      "Epoch 218/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 205.87batch/s, lastLoss=0.081, valLoss=0.0816]\n",
      "Epoch 219/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 190.53batch/s, lastLoss=0.0815, valLoss=0.0853]\n",
      "Epoch 220/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.32batch/s, lastLoss=0.0836, valLoss=0.0912]\n",
      "Epoch 221/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.91batch/s, lastLoss=0.0833, valLoss=0.0865]\n",
      "Epoch 222/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 201.08batch/s, lastLoss=0.0806, valLoss=0.083]\n",
      "Epoch 223/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.51batch/s, lastLoss=0.0843, valLoss=0.0897]\n",
      "Epoch 224/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 210.19batch/s, lastLoss=0.081, valLoss=0.0925]\n",
      "Epoch 225/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.62batch/s, lastLoss=0.0818, valLoss=0.0886]\n",
      "Epoch 226/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 210.11batch/s, lastLoss=0.086, valLoss=0.0832]\n",
      "Epoch 227/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.31batch/s, lastLoss=0.0813, valLoss=0.0906]\n",
      "Epoch 228/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.87batch/s, lastLoss=0.0807, valLoss=0.0861]\n",
      "Epoch 229/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 182.71batch/s, lastLoss=0.0811, valLoss=0.0842]\n",
      "Epoch 230/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.82batch/s, lastLoss=0.0817, valLoss=0.0849]\n",
      "Epoch 231/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.10batch/s, lastLoss=0.0808, valLoss=0.0843]\n",
      "Epoch 232/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.89batch/s, lastLoss=0.0814, valLoss=0.0871]\n",
      "Epoch 233/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 185.73batch/s, lastLoss=0.0826, valLoss=0.0879]\n",
      "Epoch 234/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.42batch/s, lastLoss=0.0815, valLoss=0.0811]\n",
      "Epoch 235/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.13batch/s, lastLoss=0.0809, valLoss=0.0865]\n",
      "Epoch 236/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 181.61batch/s, lastLoss=0.0825, valLoss=0.0862]\n",
      "Epoch 237/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 187.29batch/s, lastLoss=0.0839, valLoss=0.0828]\n",
      "Epoch 238/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 152.38batch/s, lastLoss=0.0833, valLoss=0.0862]\n",
      "Epoch 239/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 180.94batch/s, lastLoss=0.0808, valLoss=0.0854]\n",
      "Epoch 240/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 185.43batch/s, lastLoss=0.083, valLoss=0.0926]\n",
      "Epoch 241/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 201.44batch/s, lastLoss=0.0818, valLoss=0.089]\n",
      "Epoch 242/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.01batch/s, lastLoss=0.0812, valLoss=0.0872]\n",
      "Epoch 243/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 184.46batch/s, lastLoss=0.0812, valLoss=0.0842]\n",
      "Epoch 244/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.99batch/s, lastLoss=0.0817, valLoss=0.0902]\n",
      "Epoch 245/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 190.15batch/s, lastLoss=0.0812, valLoss=0.0862]\n",
      "Epoch 246/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 196.86batch/s, lastLoss=0.0823, valLoss=0.085]\n",
      "Epoch 247/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 165.69batch/s, lastLoss=0.0831, valLoss=0.0916]\n",
      "Epoch 248/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.53batch/s, lastLoss=0.0831, valLoss=0.0851]\n",
      "Epoch 249/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 158.64batch/s, lastLoss=0.0815, valLoss=0.087]\n",
      "Epoch 250/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 168.68batch/s, lastLoss=0.083, valLoss=0.0863]\n",
      "Epoch 251/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 166.67batch/s, lastLoss=0.08, valLoss=0.0914]\n",
      "Epoch 252/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 180.34batch/s, lastLoss=0.082, valLoss=0.086]\n",
      "Epoch 253/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 167.16batch/s, lastLoss=0.0799, valLoss=0.0869]\n",
      "Epoch 254/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 162.79batch/s, lastLoss=0.081, valLoss=0.0858]\n",
      "Epoch 255/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 158.19batch/s, lastLoss=0.0804, valLoss=0.0829]\n",
      "Epoch 256/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 187.29batch/s, lastLoss=0.0809, valLoss=0.0825]\n",
      "Epoch 257/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.91batch/s, lastLoss=0.0818, valLoss=0.0877]\n",
      "Epoch 258/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.15batch/s, lastLoss=0.0805, valLoss=0.0901]\n",
      "Epoch 259/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 207.79batch/s, lastLoss=0.0814, valLoss=0.103]\n",
      "Epoch 260/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 184.51batch/s, lastLoss=0.085, valLoss=0.0864]\n",
      "Epoch 261/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 188.24batch/s, lastLoss=0.0813, valLoss=0.0834]\n",
      "Epoch 262/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 200.35batch/s, lastLoss=0.0818, valLoss=0.093]\n",
      "Epoch 263/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.33batch/s, lastLoss=0.0813, valLoss=0.0924]\n",
      "Epoch 264/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 192.79batch/s, lastLoss=0.0818, valLoss=0.0913]\n",
      "Epoch 265/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 196.54batch/s, lastLoss=0.08, valLoss=0.0833]\n",
      "Epoch 266/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.80batch/s, lastLoss=0.0805, valLoss=0.0874]\n",
      "Epoch 267/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.33batch/s, lastLoss=0.0813, valLoss=0.0834]\n",
      "Epoch 268/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 200.88batch/s, lastLoss=0.0799, valLoss=0.0881]\n",
      "Epoch 269/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.46batch/s, lastLoss=0.0818, valLoss=0.0869]\n",
      "Epoch 270/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.17batch/s, lastLoss=0.0823, valLoss=0.0869]\n",
      "Epoch 271/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.18batch/s, lastLoss=0.0793, valLoss=0.0894]\n",
      "Epoch 272/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 188.23batch/s, lastLoss=0.0802, valLoss=0.0845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 273/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.66batch/s, lastLoss=0.0807, valLoss=0.0895]\n",
      "Epoch 274/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 203.09batch/s, lastLoss=0.081, valLoss=0.0866]\n",
      "Epoch 275/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 209.73batch/s, lastLoss=0.0826, valLoss=0.089]\n",
      "Epoch 276/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 203.74batch/s, lastLoss=0.0824, valLoss=0.0845]\n",
      "Epoch 277/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.88batch/s, lastLoss=0.0805, valLoss=0.0914]\n",
      "Epoch 278/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 198.23batch/s, lastLoss=0.0812, valLoss=0.086]\n",
      "Epoch 279/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 188.66batch/s, lastLoss=0.081, valLoss=0.0854]\n",
      "Epoch 280/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.45batch/s, lastLoss=0.0801, valLoss=0.0838]\n",
      "Epoch 281/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.0806, valLoss=0.0895]\n",
      "Epoch 282/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 209.35batch/s, lastLoss=0.082, valLoss=0.0906]\n",
      "Epoch 283/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.76batch/s, lastLoss=0.0817, valLoss=0.0916]\n",
      "Epoch 284/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.89batch/s, lastLoss=0.0838, valLoss=0.0932]\n",
      "Epoch 285/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 191.78batch/s, lastLoss=0.0799, valLoss=0.0886]\n",
      "Epoch 286/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 203.55batch/s, lastLoss=0.0818, valLoss=0.085]\n",
      "Epoch 287/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.42batch/s, lastLoss=0.0803, valLoss=0.0842]\n",
      "Epoch 288/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 190.15batch/s, lastLoss=0.08, valLoss=0.0833]\n",
      "Epoch 289/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.50batch/s, lastLoss=0.0806, valLoss=0.0891]\n",
      "Epoch 290/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.73batch/s, lastLoss=0.0817, valLoss=0.0942]\n",
      "Epoch 291/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.46batch/s, lastLoss=0.0828, valLoss=0.0877]\n",
      "Epoch 292/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 204.38batch/s, lastLoss=0.0796, valLoss=0.082]\n",
      "Epoch 293/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.72batch/s, lastLoss=0.0802, valLoss=0.0841]\n",
      "Epoch 294/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 184.52batch/s, lastLoss=0.0805, valLoss=0.088]\n",
      "Epoch 295/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 199.29batch/s, lastLoss=0.0804, valLoss=0.0857]\n",
      "Epoch 296/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 212.33batch/s, lastLoss=0.079, valLoss=0.0876]\n",
      "Epoch 297/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.80batch/s, lastLoss=0.0806, valLoss=0.0919]\n",
      "Epoch 298/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.26batch/s, lastLoss=0.0819, valLoss=0.0889]\n",
      "Epoch 299/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.38batch/s, lastLoss=0.0799, valLoss=0.0855]\n",
      "Epoch 300/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.40batch/s, lastLoss=0.0791, valLoss=0.0885]\n",
      "Epoch 301/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 190.47batch/s, lastLoss=0.0802, valLoss=0.0855]\n",
      "Epoch 302/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 213.07batch/s, lastLoss=0.0795, valLoss=0.0916]\n",
      "Epoch 303/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.17batch/s, lastLoss=0.0807, valLoss=0.0833]\n",
      "Epoch 304/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.50batch/s, lastLoss=0.0799, valLoss=0.0903]\n",
      "Epoch 305/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 184.59batch/s, lastLoss=0.0803, valLoss=0.084]\n",
      "Epoch 306/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 198.23batch/s, lastLoss=0.0795, valLoss=0.0844]\n",
      "Epoch 307/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.94batch/s, lastLoss=0.0803, valLoss=0.0852]\n",
      "Epoch 308/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.78batch/s, lastLoss=0.0794, valLoss=0.0831]\n",
      "Epoch 309/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.71batch/s, lastLoss=0.0798, valLoss=0.0879]\n",
      "Epoch 310/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 213.33batch/s, lastLoss=0.0821, valLoss=0.0925]\n",
      "Epoch 311/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 204.37batch/s, lastLoss=0.081, valLoss=0.0865]\n",
      "Epoch 312/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.38batch/s, lastLoss=0.0795, valLoss=0.0883]\n",
      "Epoch 313/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 196.83batch/s, lastLoss=0.081, valLoss=0.0852]\n",
      "Epoch 314/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.52batch/s, lastLoss=0.0792, valLoss=0.0874]\n",
      "Epoch 315/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 208.98batch/s, lastLoss=0.081, valLoss=0.0994]\n",
      "Epoch 316/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 213.12batch/s, lastLoss=0.0794, valLoss=0.0913]\n",
      "Epoch 317/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 211.43batch/s, lastLoss=0.0814, valLoss=0.087]\n",
      "Epoch 318/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 200.31batch/s, lastLoss=0.0816, valLoss=0.0921]\n",
      "Epoch 319/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.41batch/s, lastLoss=0.0804, valLoss=0.0891]\n",
      "Epoch 320/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 192.72batch/s, lastLoss=0.0805, valLoss=0.086]\n",
      "Epoch 321/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 192.91batch/s, lastLoss=0.0801, valLoss=0.0853]\n",
      "Epoch 322/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 198.91batch/s, lastLoss=0.0816, valLoss=0.0886]\n",
      "Epoch 323/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.72batch/s, lastLoss=0.0808, valLoss=0.0839]\n",
      "Epoch 324/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 188.23batch/s, lastLoss=0.0789, valLoss=0.0884]\n",
      "Epoch 325/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 194.78batch/s, lastLoss=0.0814, valLoss=0.0927]\n",
      "Epoch 326/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.73batch/s, lastLoss=0.0812, valLoss=0.0879]\n",
      "Epoch 327/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.12batch/s, lastLoss=0.0791, valLoss=0.0813]\n",
      "Epoch 328/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.01batch/s, lastLoss=0.0818, valLoss=0.0911]\n",
      "Epoch 329/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 193.77batch/s, lastLoss=0.0791, valLoss=0.0914]\n",
      "Epoch 330/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 205.53batch/s, lastLoss=0.0802, valLoss=0.086]\n",
      "Epoch 331/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 194.85batch/s, lastLoss=0.0789, valLoss=0.093]\n",
      "Epoch 332/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 193.21batch/s, lastLoss=0.0794, valLoss=0.0894]\n",
      "Epoch 333/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.17batch/s, lastLoss=0.0806, valLoss=0.0885]\n",
      "Epoch 334/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.39batch/s, lastLoss=0.0792, valLoss=0.0859]\n",
      "Epoch 335/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 198.49batch/s, lastLoss=0.0794, valLoss=0.0956]\n",
      "Epoch 336/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 187.89batch/s, lastLoss=0.0805, valLoss=0.089]\n",
      "Epoch 337/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 194.11batch/s, lastLoss=0.0805, valLoss=0.0917]\n",
      "Epoch 338/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 199.33batch/s, lastLoss=0.0792, valLoss=0.0857]\n",
      "Epoch 339/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 192.52batch/s, lastLoss=0.079, valLoss=0.0882]\n",
      "Epoch 340/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 194.78batch/s, lastLoss=0.079, valLoss=0.0839]\n",
      "Epoch 341/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 193.87batch/s, lastLoss=0.0804, valLoss=0.0885]\n",
      "Epoch 342/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.92batch/s, lastLoss=0.0821, valLoss=0.0917]\n",
      "Epoch 343/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.66batch/s, lastLoss=0.0812, valLoss=0.0891]\n",
      "Epoch 344/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 194.78batch/s, lastLoss=0.08, valLoss=0.0851]\n",
      "Epoch 345/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.81batch/s, lastLoss=0.0788, valLoss=0.0885]\n",
      "Epoch 346/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 192.81batch/s, lastLoss=0.0793, valLoss=0.0909]\n",
      "Epoch 347/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 203.26batch/s, lastLoss=0.0793, valLoss=0.0862]\n",
      "Epoch 348/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.53batch/s, lastLoss=0.0797, valLoss=0.0885]\n",
      "Epoch 349/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 187.92batch/s, lastLoss=0.0821, valLoss=0.0909]\n",
      "Epoch 350/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 174.46batch/s, lastLoss=0.0798, valLoss=0.0908]\n",
      "Epoch 351/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 182.41batch/s, lastLoss=0.079, valLoss=0.0868]\n",
      "Epoch 352/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.07batch/s, lastLoss=0.0803, valLoss=0.0877]\n",
      "Epoch 353/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.18batch/s, lastLoss=0.0797, valLoss=0.0842]\n",
      "Epoch 354/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.80batch/s, lastLoss=0.0799, valLoss=0.0879]\n",
      "Epoch 355/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 173.91batch/s, lastLoss=0.0792, valLoss=0.0879]\n",
      "Epoch 356/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 187.29batch/s, lastLoss=0.0816, valLoss=0.0934]\n",
      "Epoch 357/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 191.13batch/s, lastLoss=0.0814, valLoss=0.0956]\n",
      "Epoch 358/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.79batch/s, lastLoss=0.0788, valLoss=0.0883]\n",
      "Epoch 359/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 193.33batch/s, lastLoss=0.0791, valLoss=0.082]\n",
      "Epoch 360/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 185.35batch/s, lastLoss=0.0781, valLoss=0.0921]\n",
      "Epoch 361/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.50batch/s, lastLoss=0.0816, valLoss=0.0935]\n",
      "Epoch 362/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.34batch/s, lastLoss=0.0783, valLoss=0.0856]\n",
      "Epoch 363/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 188.19batch/s, lastLoss=0.0784, valLoss=0.0852]\n",
      "Epoch 364/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.14batch/s, lastLoss=0.0776, valLoss=0.0932]\n",
      "Epoch 365/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 186.36batch/s, lastLoss=0.0771, valLoss=0.0853]\n",
      "Epoch 366/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 189.19batch/s, lastLoss=0.0773, valLoss=0.0928]\n",
      "Epoch 367/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 163.03batch/s, lastLoss=0.0781, valLoss=0.0884]\n",
      "Epoch 368/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 166.17batch/s, lastLoss=0.0823, valLoss=0.0977]\n",
      "Epoch 369/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 181.81batch/s, lastLoss=0.0828, valLoss=0.085]\n",
      "Epoch 370/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 179.20batch/s, lastLoss=0.0783, valLoss=0.0828]\n",
      "Epoch 371/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 182.73batch/s, lastLoss=0.0766, valLoss=0.0913]\n",
      "Epoch 372/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 180.94batch/s, lastLoss=0.078, valLoss=0.083]\n",
      "Epoch 373/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 175.82batch/s, lastLoss=0.079, valLoss=0.0893]\n",
      "Epoch 374/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 180.36batch/s, lastLoss=0.0768, valLoss=0.0866]\n",
      "Epoch 375/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 177.21batch/s, lastLoss=0.08, valLoss=0.0884]\n",
      "Epoch 376/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.63batch/s, lastLoss=0.0802, valLoss=0.0928]\n",
      "Epoch 377/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 182.81batch/s, lastLoss=0.0781, valLoss=0.0913]\n",
      "Epoch 378/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 186.48batch/s, lastLoss=0.0792, valLoss=0.0878]\n",
      "Epoch 379/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 181.54batch/s, lastLoss=0.0784, valLoss=0.0849]\n",
      "Epoch 380/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 185.51batch/s, lastLoss=0.0776, valLoss=0.0878]\n",
      "Epoch 381/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 188.55batch/s, lastLoss=0.0813, valLoss=0.0877]\n",
      "Epoch 382/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 179.77batch/s, lastLoss=0.0776, valLoss=0.0849]\n",
      "Epoch 383/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 178.06batch/s, lastLoss=0.0772, valLoss=0.087]\n",
      "Epoch 384/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 186.81batch/s, lastLoss=0.0778, valLoss=0.0934]\n",
      "Epoch 385/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 164.47batch/s, lastLoss=0.0776, valLoss=0.0824]\n",
      "Epoch 386/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 178.06batch/s, lastLoss=0.078, valLoss=0.0905]\n",
      "Epoch 387/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 188.23batch/s, lastLoss=0.0779, valLoss=0.0848]\n",
      "Epoch 388/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.31batch/s, lastLoss=0.0773, valLoss=0.0894]\n",
      "Epoch 389/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 179.20batch/s, lastLoss=0.0772, valLoss=0.0821]\n",
      "Epoch 390/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 169.70batch/s, lastLoss=0.0765, valLoss=0.0874]\n",
      "Epoch 391/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 176.38batch/s, lastLoss=0.0769, valLoss=0.0802]\n",
      "Epoch 392/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 173.91batch/s, lastLoss=0.0768, valLoss=0.0882]\n",
      "Epoch 393/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 179.15batch/s, lastLoss=0.0774, valLoss=0.0823]\n",
      "Epoch 394/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 178.62batch/s, lastLoss=0.0786, valLoss=0.0887]\n",
      "Epoch 395/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 181.23batch/s, lastLoss=0.0771, valLoss=0.0834]\n",
      "Epoch 396/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 182.71batch/s, lastLoss=0.0776, valLoss=0.0827]\n",
      "Epoch 397/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 179.20batch/s, lastLoss=0.077, valLoss=0.0854]\n",
      "Epoch 398/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.58batch/s, lastLoss=0.0771, valLoss=0.0845]\n",
      "Epoch 399/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 187.29batch/s, lastLoss=0.0785, valLoss=0.0825]\n",
      "Epoch 400/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 168.67batch/s, lastLoss=0.0772, valLoss=0.0824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.08700385513848491\n",
      "Average validation loss: 0.09160339520840594\n"
     ]
    }
   ],
   "source": [
    "sRNN_train_loss, sRNN_val_loss =  train(train_loader, sRNN, val_loader=test_loader, LR=0.001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "e235f27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 576.99 batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(86.84305953979492,\n",
       " array([93.70472 , 59.70045 , 99.71429 , 99.451385, 76.14395 , 92.34357 ],\n",
       "       dtype=float32),\n",
       " 0.08491914172967276)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader, sRNN, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8c342e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnSingle = RNNSingle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2bfdb0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 63.85batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 62.53batch/s, lastLoss=0.257, valLoss=0.192]\n",
      "Epoch 3/400: 100%|████████████████████████████████████| 56/56 [00:00<00:00, 64.44batch/s, lastLoss=0.23, valLoss=0.208]\n",
      "Epoch 4/400:  16%|█████▉                               | 9/56 [00:00<00:00, 58.63batch/s, lastLoss=0.222, valLoss=0.19]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [176]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rnnSingle_train_loss, rnnSingle_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnnSingle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     34\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m         cost\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 36\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n\u001b[0;32m     39\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnnSingle_train_loss, rnnSingle_val_loss =  train(train_loader, rnnSingle, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8995ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9f26a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 99.11batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 102.12batch/s, lastLoss=0.41, valLoss=0.396]\n",
      "Epoch 3/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 106.25batch/s, lastLoss=0.378, valLoss=0.326]\n",
      "Epoch 4/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 104.69batch/s, lastLoss=0.348, valLoss=0.295]\n",
      "Epoch 5/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 104.79batch/s, lastLoss=0.32, valLoss=0.261]\n",
      "Epoch 6/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 105.24batch/s, lastLoss=0.297, valLoss=0.239]\n",
      "Epoch 7/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 101.97batch/s, lastLoss=0.281, valLoss=0.214]\n",
      "Epoch 8/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 103.90batch/s, lastLoss=0.272, valLoss=0.223]\n",
      "Epoch 9/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 105.32batch/s, lastLoss=0.261, valLoss=0.208]\n",
      "Epoch 10/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 104.99batch/s, lastLoss=0.254, valLoss=0.207]\n",
      "Epoch 11/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 103.17batch/s, lastLoss=0.25, valLoss=0.209]\n",
      "Epoch 12/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 105.54batch/s, lastLoss=0.243, valLoss=0.194]\n",
      "Epoch 13/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 105.45batch/s, lastLoss=0.243, valLoss=0.203]\n",
      "Epoch 14/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 105.27batch/s, lastLoss=0.236, valLoss=0.197]\n",
      "Epoch 15/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 105.39batch/s, lastLoss=0.234, valLoss=0.193]\n",
      "Epoch 16/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 105.27batch/s, lastLoss=0.229, valLoss=0.191]\n",
      "Epoch 17/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 102.95batch/s, lastLoss=0.228, valLoss=0.202]\n",
      "Epoch 18/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 104.86batch/s, lastLoss=0.226, valLoss=0.187]\n",
      "Epoch 19/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 104.25batch/s, lastLoss=0.223, valLoss=0.195]\n",
      "Epoch 20/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 104.72batch/s, lastLoss=0.221, valLoss=0.189]\n",
      "Epoch 21/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 103.39batch/s, lastLoss=0.223, valLoss=0.187]\n",
      "Epoch 22/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 102.33batch/s, lastLoss=0.218, valLoss=0.187]\n",
      "Epoch 23/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 105.89batch/s, lastLoss=0.218, valLoss=0.188]\n",
      "Epoch 24/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 105.84batch/s, lastLoss=0.217, valLoss=0.195]\n",
      "Epoch 25/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 105.15batch/s, lastLoss=0.216, valLoss=0.192]\n",
      "Epoch 26/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 104.31batch/s, lastLoss=0.213, valLoss=0.185]\n",
      "Epoch 27/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 102.90batch/s, lastLoss=0.214, valLoss=0.199]\n",
      "Epoch 28/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 103.39batch/s, lastLoss=0.212, valLoss=0.191]\n",
      "Epoch 29/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 103.75batch/s, lastLoss=0.212, valLoss=0.196]\n",
      "Epoch 30/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 104.23batch/s, lastLoss=0.211, valLoss=0.185]\n",
      "Epoch 31/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 102.94batch/s, lastLoss=0.209, valLoss=0.191]\n",
      "Epoch 32/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 101.63batch/s, lastLoss=0.21, valLoss=0.185]\n",
      "Epoch 33/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 104.45batch/s, lastLoss=0.208, valLoss=0.187]\n",
      "Epoch 34/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 100.65batch/s, lastLoss=0.209, valLoss=0.191]\n",
      "Epoch 35/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 103.21batch/s, lastLoss=0.209, valLoss=0.187]\n",
      "Epoch 36/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 96.53batch/s, lastLoss=0.208, valLoss=0.183]\n",
      "Epoch 37/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 97.66batch/s, lastLoss=0.206, valLoss=0.184]\n",
      "Epoch 38/400:   0%|                                           | 0/56 [00:00<?, ?batch/s, lastLoss=0.206, valLoss=0.184]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [187]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lstm_train_loss, lstm_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     28\u001b[0m inp, out \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [185]\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Forward propagate LSTM\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Decode the hidden state of the last time step\u001b[39;00m\n\u001b[0;32m     36\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_train_loss, lstm_val_loss =  train(train_loader, lstm, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(signal1, signal2, decomp_level=5, wavelet_func='db4'):\n",
    "    \"\"\"\n",
    "    Perform wavelet packet decomposition on two 1D signals and return a matrix of features\n",
    "    where each row corresponds to a single time step, and the features are ordered\n",
    "    based on the wavelet packet decomposition coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal1 : array_like\n",
    "        1D array of samples for the first signal.\n",
    "        \n",
    "    signal2 : array_like\n",
    "        1D array of samples for the second signal.\n",
    "        \n",
    "    decomp_level : int, optional\n",
    "        Number of decomposition levels to use in the wavelet packet decomposition. Default is 5.\n",
    "        \n",
    "    wavelet_func : str, optional\n",
    "        Name of the wavelet function to use in the wavelet packet decomposition. Default is 'db4'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    features : numpy.ndarray\n",
    "        2D array of features extracted from the wavelet packet decomposition, where each row corresponds\n",
    "        to a single time step and the columns represent the wavelet packet decomposition coefficients.\n",
    "    \"\"\"\n",
    "    \n",
    "    # perform wavelet packet decomposition on each signal\n",
    "    wp1 = pywt.WaveletPacket(signal1, wavelet_func, mode='symmetric', maxlevel=decomp_level)\n",
    "    wp2 = pywt.WaveletPacket(signal2, wavelet_func, mode='symmetric', maxlevel=decomp_level)\n",
    "    \n",
    "    # collect the wavelet packet decomposition coefficients for each signal\n",
    "    coeffs1 = []\n",
    "    coeffs2 = []\n",
    "    for level in range(decomp_level+1):\n",
    "        for node in wp1.get_level(level, 'natural'):\n",
    "            coeffs1.append(node.data)\n",
    "        for node in wp2.get_level(level, 'natural'):\n",
    "            coeffs2.append(node.data)\n",
    "            \n",
    "    # concatenate the coefficients across time\n",
    "    coeffs_concat = np.concatenate([coeffs1, coeffs2], axis=0)\n",
    "\n",
    "    # reshape the coefficients back into a time series with multiple features\n",
    "    num_features = len(coeffs_concat)\n",
    "    num_samples = len(signal1)\n",
    "    features = np.zeros((num_samples, num_features))\n",
    "    for i in range(num_samples):\n",
    "        start_idx = i*num_features\n",
    "        end_idx = (i+1)*num_features\n",
    "        features[i,:] = coeffs_concat[start_idx:end_idx]\n",
    "        \n",
    "    return features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
