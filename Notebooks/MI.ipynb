{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982a0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import pywt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import FloatTensor\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from math import gcd\n",
    "from functools import reduce\n",
    "import pywt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.special import entr\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Req for package\n",
    "sys.path.append(\"../\")\n",
    "from SkinLearning.NN.Helpers import train, test, DEVICE, set_seed\n",
    "from SkinLearning.NN.Models import MultiTemporal\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54280a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cedc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder name will correspond to index of sample\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scaler,\n",
    "        signalFolder=\"D:/SamplingResults2\",\n",
    "        sampleFile=\"../Data/newSamples.pkl\",\n",
    "        extraction_args={},\n",
    "        runs=range(65535),\n",
    "        steps=128,    \n",
    "    ):\n",
    "        # Load both disp1 and disp2 from each folder\n",
    "        # Folders ordered according to index of sample\n",
    "        # Use the corresponding sample as y -> append probe?\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        with open(f\"{sampleFile}\", \"rb\") as f:\n",
    "             samples = pickle.load(f)\n",
    "        \n",
    "        self.min = np.min(samples[runs])\n",
    "        self.max = np.max(samples[runs])\n",
    "        \n",
    "        \n",
    "        for run in tqdm(runs):\n",
    "            inp = []\n",
    "            fail = False\n",
    "            \n",
    "            files = os.listdir(f\"{signalFolder}/{run}/\")\n",
    "            \n",
    "            if files != ['Disp1.csv', 'Disp2.csv']:\n",
    "                continue\n",
    "            \n",
    "            for file in files:\n",
    "                a = pd.read_csv(f\"{signalFolder}/{run}/{file}\")\n",
    "                a.rename(columns = {'0':'x', '0.1': 'y'}, inplace = True)\n",
    "                \n",
    "                # Skip if unconverged\n",
    "                if a['x'].max() != 7.0:\n",
    "                    fail = True\n",
    "                    break\n",
    "\n",
    "                # Interpolate curve for consistent x values\n",
    "                xNew = np.linspace(0, 7, num=steps, endpoint=False)\n",
    "                interped = interp1d(a['x'], a['y'], kind='cubic', fill_value=\"extrapolate\")(xNew)\n",
    "                    \n",
    "                \n",
    "                inp.append(interped.astype(\"float32\"))\n",
    "            \n",
    "            if not fail:\n",
    "                if len(inp) != 2:\n",
    "                    raise Exception(\"sdf\")\n",
    "\n",
    "                self.input.append(inp)\n",
    "                self.output.append(samples[int(run)])\n",
    "        \n",
    "        scaler.fit(self.output)\n",
    "        self.output = scaler.fit_transform(self.output)\n",
    "        self.output = tensor(self.output).type(FloatTensor)\n",
    "        \n",
    "        # Perform WPD on all sets of signals\n",
    "        # Using the given parameters\n",
    "        extracted_features = []\n",
    "        for i, signals in enumerate(self.input):\n",
    "            extraction_args['signals'] = signals\n",
    "            extracted_features.append(waveletExtraction(**extraction_args))       \n",
    "            \n",
    "        self.input = extracted_features\n",
    "        del extracted_features\n",
    "            \n",
    "        self.input = tensor(np.array(self.input)).type(FloatTensor)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"input\": self.input[idx], \"output\": self.output[idx]}\n",
    "        return sample\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cec7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates the data set from filtered samples\n",
    "    Returns the dataset and the scaler\n",
    "\"\"\"\n",
    "def getDataset(**kwargs):\n",
    "    # Get filtered data\n",
    "    if not 'runs' in kwargs.keys():\n",
    "        with open(\"../Data/filtered.pkl\", \"rb\") as f:\n",
    "            runs = pickle.load(f)\n",
    "\n",
    "        kwargs['runs'] = runs\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset = SkinDataset(scaler=scaler, **kwargs)\n",
    "\n",
    "    return dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1516ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates a train/test split from the given data\n",
    "    Returns train and test data loaders\n",
    "\"\"\"\n",
    "def getSplit(dataset, p1=0.8):\n",
    "    train_n = int(p1 * len(dataset))\n",
    "    test_n = len(dataset) - train_n\n",
    "    train_set, test_set = random_split(dataset, [train_n, test_n])\n",
    "\n",
    "    return DataLoader(train_set, batch_size=32, shuffle=True), \\\n",
    "        DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e0da617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(\n",
    "    signals,\n",
    "    method,\n",
    "    combined=False,\n",
    "    wavelet='db4',\n",
    "    level=3,\n",
    "    combine_method='concatenate',\n",
    "    order='freq',\n",
    "    levels=[3],\n",
    "    stats=['mean', 'std', 'skew', 'kurtosis'],\n",
    "    normalization=None,\n",
    "    flat=False\n",
    "):\n",
    "    def get_statistics(coefficients, stats_list):\n",
    "        features = []\n",
    "        for stat in stats_list:\n",
    "            if stat == 'mean':\n",
    "                features.append(np.mean(coefficients))\n",
    "            elif stat == 'std':\n",
    "                features.append(np.std(coefficients))\n",
    "            elif stat == 'skew':\n",
    "                features.append(skew(coefficients))\n",
    "            elif stat == 'kurtosis':\n",
    "                features.append(kurtosis(coefficients))\n",
    "        return features\n",
    "\n",
    "    def extract_features_single_signal(signal, method, wavelet, level, order, levels, stats_list):\n",
    "        wp = pywt.WaveletPacket(data=signal, wavelet=wavelet, mode='symmetric', maxlevel=level)\n",
    "        \n",
    "        if flat:\n",
    "            return np.array([node.data for node in wp.get_level(level, 'natural')]).flatten()\n",
    "        \n",
    "        features = []\n",
    "\n",
    "        for l in levels:\n",
    "            coeffs =  wp.get_level(level, order)\n",
    "            coeffs = np.array([c.data for c in coeffs])\n",
    "\n",
    "            if method == \"energy\" or method == \"min-max\":\n",
    "                 # Normalise\n",
    "                coeffs = (coeffs - np.mean(coeffs)) / np.std(coeffs)\n",
    "                \n",
    "            if method == 'raw':\n",
    "                features.extend(np.concatenate(coeffs))\n",
    "            elif method == 'energy':       \n",
    "                features.extend([np.sum(np.square(c)) for c in coeffs])\n",
    "            elif method == 'entropy':\n",
    "                features.extend([np.sum(entr(np.abs(c))) for c in coeffs])\n",
    "            elif method == 'min-max':\n",
    "                features.extend([np.min(c) for c in coeffs] + [np.max(c) for c in coeffs])\n",
    "            elif method == 'stats':\n",
    "                for c in coeffs:\n",
    "                    features.extend(get_statistics(c, stats_list))\n",
    "        \n",
    "            # Optional normalisation for raw and energy\n",
    "            if normalization == \"indvidual\":\n",
    "                features = (features - min(features)) / (max(features) - min(features))\n",
    "                \n",
    "        return features\n",
    "\n",
    "    if combined:\n",
    "        combined_signal = np.concatenate(signals, axis=0)\n",
    "        combined_features = extract_features_single_signal(combined_signal, method, wavelet, level, order, levels, stats)\n",
    "        features = combined_features\n",
    "    else:\n",
    "        features_list = [extract_features_single_signal(signal, method, wavelet, level, order, levels, stats) for signal in signals]\n",
    "\n",
    "        if combine_method == 'concatenate':\n",
    "            features = np.concatenate(features_list)\n",
    "        elif combine_method == 'interleave':\n",
    "            features = np.ravel(np.column_stack(features_list))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid combine_method. Choose from 'concatenate' or 'interleave'.\")\n",
    "    \n",
    "    if normalization == \"combined\":\n",
    "        features = (features - np.min(features, axis=0)) / (np.max(features, axis=0) - np.min(features, axis=0))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73971e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:10<00:00, 218.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_best(max_level=6):\n",
    "    def parse_dataset(method, combined, order, normalize, stats):\n",
    "        key = (method, normalize, stats)\n",
    "        normalize = key[1]\n",
    "        stats = key[-1]\n",
    "        extraction_args = {\n",
    "            \"signals\": None,\n",
    "            \"method\": method,\n",
    "            \"combined\": combined,\n",
    "            \"wavelet\": \"db4\",\n",
    "            \"level\": max_level,\n",
    "            \"order\": order,\n",
    "            \"levels\": [6],\n",
    "            \"normalization\": normalize,\n",
    "            \"stats\": stats if method == \"stats\" else None,\n",
    "        }\n",
    "        print(stats)\n",
    "        combined_str = 'combined' if combined else 'separated'\n",
    "        dataset_dict[(combined_str, order)][key] = getDataset(\n",
    "            extraction_args=extraction_args,\n",
    "        )\n",
    "\n",
    "    dataset_dict = {\n",
    "        #(\"combined\", \"freq\"): {},\n",
    "        (\"separated\", \"natural\"): {},\n",
    "        #(\"combined\", \"natural\"): {},\n",
    "       # (\"separated\", \"freq\"): {},\n",
    "    }\n",
    "    \n",
    "    parse_dataset(\n",
    "        \"entropy\",\n",
    "        False,\n",
    "        \"natural\",\n",
    "        False,\n",
    "        (\"None\")\n",
    "    )\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "all_datasets = create_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f259f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths_and_loaders(datasets, split_ratio=0.8, batch_size=32):\n",
    "    lengths = {}\n",
    "    loaders = {}\n",
    "\n",
    "    for comb in datasets.keys():\n",
    "        loaders[comb] = {}\n",
    "        lengths[comb] = {}\n",
    "        # Get train and test split\n",
    "        \n",
    "        for method in datasets[comb].keys():\n",
    "            train_data, test_data = getSplit(datasets[comb][method][0], split_ratio)\n",
    "\n",
    "            # Create DataLoaders\n",
    "            train_loader, test_loader = getSplit(datasets[comb][method][0])\n",
    "\n",
    "            # Update the loaders dictionary\n",
    "            loaders[comb][method] = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "            # Update the lengths dictionary\n",
    "            lengths[comb][method] = len(datasets[comb][method][0][0]['input'])\n",
    "\n",
    "    return lengths, loaders\n",
    "lengths, loaders = create_lengths_and_loaders(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62d4bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_type=None, epochs=700):\n",
    "    comb_losses = {}\n",
    "    non_comb_losses = {}\n",
    "    \n",
    "    for model_type in models.keys() if not model_type else [model_type]:\n",
    "        non_comb_losses[model_type] = {}\n",
    "        comb_losses[model_type] = {}\n",
    "\n",
    "        \n",
    "        for comb in models[model_type].keys():\n",
    "            non_comb_losses[model_type][comb] = {}\n",
    "            comb_losses[model_type][comb] = {}\n",
    "\n",
    "            for ext_method in models[model_type][comb].keys():\n",
    "                non_comb_losses[model_type][comb][ext_method] = {}\n",
    "                comb_losses[model_type][comb][ext_method] = {}\n",
    "\n",
    "                for fc_type in models[model_type][comb][ext_method].keys():\n",
    "                    non_comb_losses[model_type][comb][ext_method][fc_type] = {}\n",
    "                    comb_losses[model_type][comb][ext_method][fc_type] = {}\n",
    "                    \n",
    "                    for model_method in models[model_type][comb][ext_method][fc_type].keys():\n",
    "                        non_comb_losses[model_type][comb][ext_method][fc_type][model_method] = {}\n",
    "                        comb_losses[model_type][comb][ext_method][fc_type][model_method] = {}\n",
    "                        \n",
    "                        for output in models[model_type][comb][ext_method][fc_type][model_method].keys():\n",
    "\n",
    "                            non_comb_losses[model_type][comb][ext_method][fc_type][model_method][output] = {}\n",
    "                            comb_losses[model_type][comb][ext_method][fc_type][model_method][output] = {}\n",
    "                            \n",
    "                            print(f\"Running {model_type} based on {model_method} and {fc_type}, with dataset using {comb} {ext_method}, with {output}\")\n",
    "                            train_loss, val_loss = train(\n",
    "                                loaders[comb][ext_method]['train'],\n",
    "                                models[model_type][comb][ext_method][fc_type][model_method][output],\n",
    "                                val_loader=loaders[comb][ext_method]['test'],\n",
    "                                LR=0.001,\n",
    "                                epochs=epochs,\n",
    "                                early_stopping=True,\n",
    "                                patience=50\n",
    "                            )\n",
    "\n",
    "                            # Stop at 50 epochs\n",
    "                            # Only add if last lost had potential\n",
    "                            if comb == \"Combined\":\n",
    "                                comb_losses[model_type][comb][ext_method][fc_type][model_method][output] = {\n",
    "                                    'Train Loss': train_loss,\n",
    "                                    'Validation Loss': val_loss\n",
    "                                }\n",
    "                            else:\n",
    "                                non_comb_losses[model_type][comb][ext_method][fc_type][model_method][output] = {\n",
    "                                    'Train Loss': train_loss,\n",
    "                                    'Validation Loss': val_loss\n",
    "                                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68e8d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(model_type=None):\n",
    "    temp_dict = {}\n",
    "    for model_type in models.keys() if not model_type else [model_type]:\n",
    "        for comb in models[model_type].keys():\n",
    "\n",
    "            for ext_method in models[model_type][comb].keys():\n",
    "                for fc_type in models[model_type][comb][ext_method].keys():\n",
    "\n",
    "                    for model_method in models[model_type][comb][ext_method][fc_type].keys():\n",
    "                        for output in models[model_type][comb][ext_method][fc_type][model_method].keys():\n",
    "                            #print(f\"Running {model_type} based on {model_method}, with dataset using {comb} {ext_method} using {output}\")\n",
    "\n",
    "                            validation = test(\n",
    "                                    loaders[comb][ext_method]['test'],\n",
    "                                    models[model_type][comb][ext_method][fc_type][model_method][output],\n",
    "                                    all_datasets[comb][ext_method][1]\n",
    "                            )\n",
    "                            \n",
    "                            \"\"\"                            losses = comb_losses if comb == \"combined\" else non_comb_losses\n",
    "\n",
    "                            print(\n",
    "                                \"Final training loss: \",\n",
    "                                losses[model_type][comb][ext_method][fc_type][model_method]['Train Loss'][-1]\n",
    "                            )\"\"\"\n",
    "                            \"\"\"print(\n",
    "                                f\"{fc_type}, MAPE: {validation[0]}, MAE: {validation[2]}\\n\"\n",
    "                            )\"\"\"\n",
    "                            \n",
    "                            \n",
    "                            try:\n",
    "                                temp_dict[(model_type, comb, model_method, ext_method, output)][fc_type] = f\"{fc_type}, MAPE: {validation[0]}, MAE: {validation[2]}\"\n",
    "                            except:\n",
    "                                temp_dict[(model_type, comb, model_method, ext_method, output)] = {}\n",
    "                                temp_dict[(model_type, comb, model_method, ext_method, output)][fc_type] = f\"{fc_type}, MAPE: {validation[0]}, MAE: {validation[2]}\"\n",
    "    for key in temp_dict.keys():\n",
    "        print(f\"{key[0]} based on {key[2]}, with dataset using {key[1]} {key[3]} using {key[4]}\")\n",
    "        for strs in temp_dict[key].values():\n",
    "              print(strs)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "667beefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTemporal(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv=True,\n",
    "        input_size=7,\n",
    "        hidden_size=256,\n",
    "        single_fc=True,\n",
    "        out=\"f_hidden\",\n",
    "        layers=1,\n",
    "        temporal_type=\"RNN\",\n",
    "        fusion_method=\"concatenate\"\n",
    "    ):\n",
    "        super(MultiTemporal, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out = out\n",
    "        self.temporal_type = temporal_type\n",
    "        self.fusion_method = fusion_method\n",
    "        self.input_size = input_size\n",
    "        self.conv = conv\n",
    "        \n",
    "        if conv:\n",
    "            self.cnn = deepcopy(best_CNN)\n",
    "        \n",
    "\n",
    "        if temporal_type == \"RNN\":\n",
    "            net = nn.RNN\n",
    "        elif temporal_type == \"LSTM\":\n",
    "            net = nn.LSTM\n",
    "        elif temporal_type == \"GRU\":\n",
    "            net = nn.GRU\n",
    "        else:\n",
    "            raise Exception(\"Not a valid NN type.\")\n",
    "\n",
    "        if fusion_method == 'concatenate':\n",
    "            self.net = net(input_size, hidden_size, layers, batch_first=True)\n",
    "        elif fusion_method == 'multi_channel':\n",
    "            self.net = net(2, hidden_size, layers, batch_first=True)\n",
    "        elif fusion_method == 'independent':\n",
    "            self.net = net(input_size, hidden_size, layers, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose from 'concatenate', 'multi_channel', or 'independent'.\")\n",
    "        \n",
    "        # Check size of output to determine FC input\n",
    "        input_tensor = torch.zeros(\n",
    "            32,\n",
    "            512,\n",
    "            input_size if fusion_method != \"independent\" else input_size\n",
    "        )\n",
    "        \n",
    "        if self.temporal_type == \"LSTM\":\n",
    "            output, (hidden, _) = self.net(input_tensor)\n",
    "        else:\n",
    "            output, hidden = self.net(input_tensor)\n",
    "            \n",
    "        fc_in = hidden_size\n",
    "\n",
    "        if fusion_method == \"independent\":\n",
    "            fc_in *= 2\n",
    "            \n",
    "        if out == \"h+o\":\n",
    "            fc_in *= 2\n",
    "            \n",
    "        print(\"fc in\", fc_in)\n",
    "            \n",
    "        if single_fc:\n",
    "            self.fc = nn.Linear(fc_in*layers, 6)\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128 , 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 6),   \n",
    "            )\n",
    "            \n",
    "            if fc_in > hidden_size:\n",
    "                if fc_in == 4096:\n",
    "                    print(\"Starts 4096\")\n",
    "                    init_layers = nn.Sequential(\n",
    "                        nn.Linear(4096, 2048),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(2048, 1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(1024 , 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512, 256), \n",
    "                    )\n",
    "                elif fc_in == 512:\n",
    "                    print(\"Starts 512\")\n",
    "                    init_layers = nn.Sequential(\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.ReLU()\n",
    "                    )\n",
    "                elif fc_in == 2048:\n",
    "                    print(\"Starts 2048\")\n",
    "                    init_layers = nn.Sequential(\n",
    "                        nn.Linear(2048, 1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(1024, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.ReLU()\n",
    "                    )\n",
    "                \n",
    "                self.fc = nn.Sequential(init_layers, self.fc)\n",
    "            print(\"\\n\")   \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = x.reshape(batch_size, 2, -1)\n",
    "        if self.conv:\n",
    "            signal1 = self.cnn(x)\n",
    "            signal2 = self.cnn(x)\n",
    "        else:\n",
    "            x = x.reshape(batch_size, -1, self.input_size)\n",
    "\n",
    "        def getOutputs(inp):\n",
    "            if self.temporal_type == \"LSTM\":\n",
    "                o, (h, _) = self.net(inp)\n",
    "            else:\n",
    "                o, h = self.net(inp)\n",
    "            return o, h\n",
    "\n",
    "        \n",
    "        if self.fusion_method == 'multi_channel':\n",
    "            o, h = getOutputs(x.view(batch_size, -1, 2))\n",
    "        elif self.fusion_method == 'independent':\n",
    "            signal_size = self.input_size//2\n",
    "            #signal1 = x[..., :signal_size].reshape(batch_size, -1, signal_size)\n",
    "            #signal2 = x[..., signal_size:].reshape(batch_size, -1, signal_size)\n",
    "            \n",
    "            o1, h1, = getOutputs(signal1)\n",
    "            o2, h2 = getOutputs(signal2)\n",
    "        else:\n",
    "            o, h = getOutputs(x)\n",
    "        \n",
    "        \n",
    "        if self.out == \"f_hidden\":\n",
    "            if self.fusion_method == \"independent\":\n",
    "                x = torch.concat(\n",
    "                    [h1[-1], h2[-1]], dim=1\n",
    "                    ).reshape(batch_size, -1)\n",
    "            else:\n",
    "                x = h[-1].reshape(batch_size, -1)\n",
    "        elif self.out == \"hidden\":\n",
    "            if self.fusion_method == \"independent\":\n",
    "                x = torch.concat(\n",
    "                    [h1, h2], dim=1\n",
    "                    ).reshape(batch_size, -1)\n",
    "            else:   \n",
    "                x = h.reshape(batch_size, -1)\n",
    "        elif self.out == \"f_output\":\n",
    "            if self.fusion_method == \"independent\":\n",
    "                x = torch.concat(\n",
    "                    [o1[:, -1, :], o2[:, -1, :]], dim=1\n",
    "                    ).reshape(batch_size, -1)\n",
    "            else:\n",
    "                x = o[:, -1, :].reshape(batch_size, -1)\n",
    "        elif self.out == \"output\":\n",
    "            if self.fusion_method == \"independent\":\n",
    "                x = torch.concat(\n",
    "                    [o1, o2], dim=1\n",
    "                    ).reshape(batch_size, -1)\n",
    "            else:\n",
    "                x = o.reshape(batch_size, -1)\n",
    "        elif self.out == \"h+o\":\n",
    "                if self.fusion_method == \"independent\":\n",
    "                    x1 = torch.concat(\n",
    "                        [h1[-1], o1[:, -1, :]], dim=1\n",
    "                        )\n",
    "                    \n",
    "                    x2 = torch.concat(\n",
    "                        [h2[-1], o2[:, -1, :]], dim=1\n",
    "                        )\n",
    "                    \n",
    "                    x = torch.concat([x1, x2], dim=1\n",
    "                        ).view(o2.size(0), -1)\n",
    "                else:\n",
    "                    x = torch.concat([h[-1], o[:, -1, :]], dim=1).view(o.size(0), -1)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8923c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc in 512\n",
      "Starts 512\n",
      "\n",
      "\n",
      "128 256\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "best_CNN = nn.Sequential(\n",
    "    nn.Conv1d(2, 128, kernel_size=5, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "\n",
    "    nn.Conv1d(128, 64, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv1d(64, 32, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2, stride=2),\n",
    ")\n",
    "models = {}\n",
    "\n",
    "# Get an LSTM and RNN of each type, paired with each type of dataset\n",
    "for model_type in ['LSTM']:#, 'GRU', 'RNN']:\n",
    "    models[model_type] = {}\n",
    "    \n",
    "    def get_models():\n",
    "        models[model_type][comb][ext_method][fc_type][model_method] = {}\n",
    "        for out in ['f_hidden']:\n",
    "            #for model_method in ['concatenate', 'independent']:#, 'multi_channel', 'independent']:\n",
    "            #print(fc_type)\n",
    "            models[model_type][comb][ext_method][fc_type][model_method][out]= \\\n",
    "            MultiTemporal(\n",
    "                fusion_method=model_method,\n",
    "                temporal_type=model_type,\n",
    "                single_fc=True if fc_type == 'Single FC' else False,\n",
    "                out=out\n",
    "            )\n",
    "            print(lengths[comb][ext_method], 2*lengths[comb][ext_method])\n",
    "\n",
    "    for comb in all_datasets.keys():\n",
    "        models[model_type][comb] = {}\n",
    "        \n",
    "        for ext_method in all_datasets[comb].keys():\n",
    "            models[model_type][comb][ext_method] = {}\n",
    "            for fc_type in ['Multi FC']:          \n",
    "                models[model_type][comb][ext_method][fc_type] = {}     \n",
    "                \n",
    "                if ext_method == ('raw', False, 'None'):\n",
    "                    for model_method in ['independent']:\n",
    "                        get_models()\n",
    "                else:\n",
    "                    model_method = \"independent\"\n",
    "                    get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0b4b0f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LSTM based on independent and Multi FC, with dataset using ('separated', 'natural') ('entropy', False, 'None'), with f_hidden\n",
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 79.43batch/s]\n",
      "100%|████████████████████████████| 56/56 [00:00<00:00, 81.57batch/s, counter=0, epoch=1, lastLoss=0.194, valLoss=0.162]\n",
      "100%|████████████████████████████| 56/56 [00:00<00:00, 85.50batch/s, counter=1, epoch=2, lastLoss=0.167, valLoss=0.167]\n",
      "100%|████████████████████████████| 56/56 [00:00<00:00, 85.85batch/s, counter=0, epoch=3, lastLoss=0.153, valLoss=0.144]\n",
      "100%|████████████████████████████| 56/56 [00:00<00:00, 86.82batch/s, counter=0, epoch=4, lastLoss=0.135, valLoss=0.132]\n",
      "100%|████████████████████████████| 56/56 [00:00<00:00, 83.71batch/s, counter=0, epoch=5, lastLoss=0.118, valLoss=0.124]\n",
      "100%|████████████████████████████| 56/56 [00:00<00:00, 83.36batch/s, counter=0, epoch=6, lastLoss=0.111, valLoss=0.106]\n",
      "100%|████████████████████████████| 56/56 [00:00<00:00, 77.94batch/s, counter=1, epoch=7, lastLoss=0.112, valLoss=0.111]\n",
      "100%|███████████████████████████| 56/56 [00:00<00:00, 81.45batch/s, counter=0, epoch=8, lastLoss=0.105, valLoss=0.0983]\n",
      "100%|███████████████████████████| 56/56 [00:00<00:00, 85.55batch/s, counter=0, epoch=9, lastLoss=0.103, valLoss=0.0947]\n",
      "100%|███████████████████████████| 56/56 [00:00<00:00, 80.42batch/s, counter=1, epoch=10, lastLoss=0.102, valLoss=0.106]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.34batch/s, counter=2, epoch=11, lastLoss=0.0995, valLoss=0.0982]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.43batch/s, counter=0, epoch=12, lastLoss=0.0971, valLoss=0.0914]\n",
      "100%|███████████████████████████| 56/56 [00:00<00:00, 86.49batch/s, counter=1, epoch=13, lastLoss=0.102, valLoss=0.099]\n",
      "100%|████████████████████████████| 56/56 [00:00<00:00, 88.77batch/s, counter=2, epoch=14, lastLoss=0.0954, valLoss=0.1]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 89.06batch/s, counter=3, epoch=15, lastLoss=0.0965, valLoss=0.0952]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 86.37batch/s, counter=4, epoch=16, lastLoss=0.0958, valLoss=0.0969]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.34batch/s, counter=5, epoch=17, lastLoss=0.0948, valLoss=0.0942]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.96batch/s, counter=6, epoch=18, lastLoss=0.0937, valLoss=0.0929]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 86.12batch/s, counter=0, epoch=19, lastLoss=0.0937, valLoss=0.0908]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.48batch/s, counter=1, epoch=20, lastLoss=0.0938, valLoss=0.0914]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.75batch/s, counter=2, epoch=21, lastLoss=0.0917, valLoss=0.0951]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.75batch/s, counter=0, epoch=22, lastLoss=0.0904, valLoss=0.0888]\n",
      "100%|███████████████████████████| 56/56 [00:00<00:00, 88.65batch/s, counter=1, epoch=23, lastLoss=0.095, valLoss=0.101]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 81.67batch/s, counter=0, epoch=24, lastLoss=0.0901, valLoss=0.0866]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.97batch/s, counter=1, epoch=25, lastLoss=0.0897, valLoss=0.0905]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 86.59batch/s, counter=2, epoch=26, lastLoss=0.0867, valLoss=0.0892]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.58batch/s, counter=0, epoch=27, lastLoss=0.0886, valLoss=0.0822]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 87.63batch/s, counter=1, epoch=28, lastLoss=0.0834, valLoss=0.087]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.04batch/s, counter=2, epoch=29, lastLoss=0.0845, valLoss=0.0891]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.98batch/s, counter=3, epoch=30, lastLoss=0.0824, valLoss=0.0931]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.63batch/s, counter=4, epoch=31, lastLoss=0.0819, valLoss=0.0863]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 82.76batch/s, counter=0, epoch=32, lastLoss=0.0781, valLoss=0.0791]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 74.62batch/s, counter=0, epoch=33, lastLoss=0.0803, valLoss=0.0781]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 77.40batch/s, counter=0, epoch=34, lastLoss=0.0783, valLoss=0.0728]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 82.17batch/s, counter=1, epoch=35, lastLoss=0.0751, valLoss=0.077]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 82.03batch/s, counter=0, epoch=36, lastLoss=0.0749, valLoss=0.072]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 75.37batch/s, counter=0, epoch=37, lastLoss=0.0759, valLoss=0.0685]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 82.35batch/s, counter=1, epoch=38, lastLoss=0.0742, valLoss=0.0801]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 81.69batch/s, counter=2, epoch=39, lastLoss=0.0721, valLoss=0.0698]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 84.58batch/s, counter=3, epoch=40, lastLoss=0.0717, valLoss=0.0857]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 86.84batch/s, counter=0, epoch=41, lastLoss=0.0713, valLoss=0.0666]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 86.27batch/s, counter=1, epoch=42, lastLoss=0.0734, valLoss=0.0755]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 83.71batch/s, counter=2, epoch=43, lastLoss=0.0701, valLoss=0.0687]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 83.15batch/s, counter=3, epoch=44, lastLoss=0.0695, valLoss=0.0713]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 83.91batch/s, counter=4, epoch=45, lastLoss=0.068, valLoss=0.0674]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 82.84batch/s, counter=0, epoch=46, lastLoss=0.0674, valLoss=0.0655]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 82.31batch/s, counter=1, epoch=47, lastLoss=0.0686, valLoss=0.076]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.74batch/s, counter=2, epoch=48, lastLoss=0.0665, valLoss=0.0669]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 80.81batch/s, counter=3, epoch=49, lastLoss=0.0682, valLoss=0.0692]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 60.64batch/s, counter=4, epoch=50, lastLoss=0.0654, valLoss=0.0689]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 57.41batch/s, counter=5, epoch=51, lastLoss=0.065, valLoss=0.0664]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 54.90batch/s, counter=6, epoch=52, lastLoss=0.0627, valLoss=0.0709]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 59.51batch/s, counter=7, epoch=53, lastLoss=0.067, valLoss=0.0698]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.60batch/s, counter=8, epoch=54, lastLoss=0.0655, valLoss=0.0701]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 41.70batch/s, counter=0, epoch=55, lastLoss=0.0594, valLoss=0.0644]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 37.79batch/s, counter=0, epoch=56, lastLoss=0.0618, valLoss=0.0621]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 37.14batch/s, counter=1, epoch=57, lastLoss=0.0595, valLoss=0.0637]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 45.44batch/s, counter=2, epoch=58, lastLoss=0.0629, valLoss=0.0734]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.16batch/s, counter=3, epoch=59, lastLoss=0.0651, valLoss=0.0792]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 39.73batch/s, counter=4, epoch=60, lastLoss=0.0592, valLoss=0.0639]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 64.81batch/s, counter=5, epoch=61, lastLoss=0.0662, valLoss=0.0667]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 69.22batch/s, counter=0, epoch=62, lastLoss=0.0599, valLoss=0.0585]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 62.57batch/s, counter=1, epoch=63, lastLoss=0.0621, valLoss=0.0662]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 36.32batch/s, counter=2, epoch=64, lastLoss=0.0595, valLoss=0.0765]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 40.16batch/s, counter=3, epoch=65, lastLoss=0.0602, valLoss=0.0681]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 74.57batch/s, counter=4, epoch=66, lastLoss=0.0608, valLoss=0.0607]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 79.37batch/s, counter=5, epoch=67, lastLoss=0.0594, valLoss=0.0659]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 56/56 [00:00<00:00, 84.38batch/s, counter=6, epoch=68, lastLoss=0.061, valLoss=0.0601]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 87.27batch/s, counter=7, epoch=69, lastLoss=0.059, valLoss=0.0599]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.70batch/s, counter=8, epoch=70, lastLoss=0.0565, valLoss=0.0647]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.30batch/s, counter=0, epoch=71, lastLoss=0.0586, valLoss=0.0567]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.49batch/s, counter=1, epoch=72, lastLoss=0.0581, valLoss=0.0654]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 82.44batch/s, counter=2, epoch=73, lastLoss=0.0585, valLoss=0.0654]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 82.65batch/s, counter=3, epoch=74, lastLoss=0.0583, valLoss=0.0658]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 83.42batch/s, counter=0, epoch=75, lastLoss=0.0568, valLoss=0.0563]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 82.62batch/s, counter=1, epoch=76, lastLoss=0.0554, valLoss=0.0652]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 81.52batch/s, counter=2, epoch=77, lastLoss=0.0564, valLoss=0.0644]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.59batch/s, counter=3, epoch=78, lastLoss=0.0565, valLoss=0.0637]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.32batch/s, counter=0, epoch=79, lastLoss=0.0564, valLoss=0.0561]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.15batch/s, counter=1, epoch=80, lastLoss=0.0541, valLoss=0.0613]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 83.09batch/s, counter=2, epoch=81, lastLoss=0.0555, valLoss=0.0578]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.90batch/s, counter=0, epoch=82, lastLoss=0.0563, valLoss=0.0545]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 89.24batch/s, counter=0, epoch=83, lastLoss=0.0536, valLoss=0.0535]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.94batch/s, counter=1, epoch=84, lastLoss=0.0542, valLoss=0.0607]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.40batch/s, counter=2, epoch=85, lastLoss=0.0559, valLoss=0.0587]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.92batch/s, counter=3, epoch=86, lastLoss=0.0523, valLoss=0.0579]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.06batch/s, counter=4, epoch=87, lastLoss=0.0551, valLoss=0.0574]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 86.38batch/s, counter=5, epoch=88, lastLoss=0.0542, valLoss=0.0599]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 88.40batch/s, counter=6, epoch=89, lastLoss=0.0562, valLoss=0.063]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 85.48batch/s, counter=7, epoch=90, lastLoss=0.054, valLoss=0.0614]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.66batch/s, counter=8, epoch=91, lastLoss=0.0546, valLoss=0.0561]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 89.13batch/s, counter=9, epoch=92, lastLoss=0.0552, valLoss=0.0542]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.26batch/s, counter=10, epoch=93, lastLoss=0.052, valLoss=0.0568]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.25batch/s, counter=11, epoch=94, lastLoss=0.0517, valLoss=0.0593]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.67batch/s, counter=12, epoch=95, lastLoss=0.0524, valLoss=0.0616]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.53batch/s, counter=13, epoch=96, lastLoss=0.0504, valLoss=0.0556]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.17batch/s, counter=14, epoch=97, lastLoss=0.0511, valLoss=0.0553]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 89.24batch/s, counter=15, epoch=98, lastLoss=0.0526, valLoss=0.056]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.67batch/s, counter=16, epoch=99, lastLoss=0.0548, valLoss=0.0681]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 87.47batch/s, counter=17, epoch=100, lastLoss=0.0539, valLoss=0.0586]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.78batch/s, counter=18, epoch=101, lastLoss=0.0509, valLoss=0.0542]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 87.94batch/s, counter=19, epoch=102, lastLoss=0.0511, valLoss=0.0698]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 87.86batch/s, counter=20, epoch=103, lastLoss=0.0527, valLoss=0.0553]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.91batch/s, counter=21, epoch=104, lastLoss=0.0517, valLoss=0.0559]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 87.46batch/s, counter=22, epoch=105, lastLoss=0.0527, valLoss=0.0619]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 88.59batch/s, counter=23, epoch=106, lastLoss=0.051, valLoss=0.0542]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.44batch/s, counter=24, epoch=107, lastLoss=0.0511, valLoss=0.0558]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 90.91batch/s, counter=25, epoch=108, lastLoss=0.0505, valLoss=0.0593]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 84.72batch/s, counter=0, epoch=109, lastLoss=0.0513, valLoss=0.053]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 88.15batch/s, counter=1, epoch=110, lastLoss=0.0519, valLoss=0.0572]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 88.84batch/s, counter=2, epoch=111, lastLoss=0.0501, valLoss=0.0696]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.53batch/s, counter=3, epoch=112, lastLoss=0.0495, valLoss=0.0541]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 89.13batch/s, counter=4, epoch=113, lastLoss=0.0495, valLoss=0.06]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 89.31batch/s, counter=0, epoch=114, lastLoss=0.05, valLoss=0.0526]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 86.18batch/s, counter=1, epoch=115, lastLoss=0.0518, valLoss=0.0608]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 90.27batch/s, counter=2, epoch=116, lastLoss=0.0501, valLoss=0.0548]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 86.63batch/s, counter=3, epoch=117, lastLoss=0.0499, valLoss=0.0531]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 88.54batch/s, counter=4, epoch=118, lastLoss=0.0497, valLoss=0.0571]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.23batch/s, counter=5, epoch=119, lastLoss=0.049, valLoss=0.0562]\n",
      "100%|██████████████████████████| 56/56 [00:00<00:00, 80.32batch/s, counter=6, epoch=120, lastLoss=0.05, valLoss=0.0538]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 81.49batch/s, counter=7, epoch=121, lastLoss=0.0501, valLoss=0.0624]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 80.46batch/s, counter=0, epoch=122, lastLoss=0.0507, valLoss=0.0516]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.08batch/s, counter=1, epoch=123, lastLoss=0.0526, valLoss=0.058]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 85.74batch/s, counter=2, epoch=124, lastLoss=0.0492, valLoss=0.0615]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 90.23batch/s, counter=3, epoch=125, lastLoss=0.0484, valLoss=0.0541]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 82.57batch/s, counter=4, epoch=126, lastLoss=0.049, valLoss=0.0566]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 80.69batch/s, counter=5, epoch=127, lastLoss=0.0483, valLoss=0.054]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 86.86batch/s, counter=6, epoch=128, lastLoss=0.0475, valLoss=0.0586]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 85.56batch/s, counter=7, epoch=129, lastLoss=0.0488, valLoss=0.0562]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 81.69batch/s, counter=8, epoch=130, lastLoss=0.0461, valLoss=0.0609]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 82.84batch/s, counter=9, epoch=131, lastLoss=0.0498, valLoss=0.0583]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 82.96batch/s, counter=10, epoch=132, lastLoss=0.0494, valLoss=0.059]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.38batch/s, counter=11, epoch=133, lastLoss=0.0474, valLoss=0.0555]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.67batch/s, counter=12, epoch=134, lastLoss=0.0474, valLoss=0.0525]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 87.23batch/s, counter=13, epoch=135, lastLoss=0.0482, valLoss=0.0585]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 56/56 [00:00<00:00, 86.55batch/s, counter=14, epoch=136, lastLoss=0.0491, valLoss=0.054]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.42batch/s, counter=15, epoch=137, lastLoss=0.0486, valLoss=0.0608]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 85.69batch/s, counter=16, epoch=138, lastLoss=0.0467, valLoss=0.0542]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 85.75batch/s, counter=17, epoch=139, lastLoss=0.0474, valLoss=0.0627]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 85.43batch/s, counter=18, epoch=140, lastLoss=0.0476, valLoss=0.0529]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.63batch/s, counter=19, epoch=141, lastLoss=0.0473, valLoss=0.0541]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 83.30batch/s, counter=20, epoch=142, lastLoss=0.0464, valLoss=0.0583]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 67.02batch/s, counter=21, epoch=143, lastLoss=0.0452, valLoss=0.0521]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.34batch/s, counter=22, epoch=144, lastLoss=0.0469, valLoss=0.0539]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 80.00batch/s, counter=23, epoch=145, lastLoss=0.0481, valLoss=0.064]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 79.77batch/s, counter=24, epoch=146, lastLoss=0.0498, valLoss=0.0584]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.87batch/s, counter=25, epoch=147, lastLoss=0.0449, valLoss=0.0566]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 81.40batch/s, counter=0, epoch=148, lastLoss=0.0452, valLoss=0.051]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 81.75batch/s, counter=1, epoch=149, lastLoss=0.0454, valLoss=0.0548]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 76.35batch/s, counter=2, epoch=150, lastLoss=0.048, valLoss=0.0655]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 81.87batch/s, counter=3, epoch=151, lastLoss=0.0491, valLoss=0.0555]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 71.51batch/s, counter=4, epoch=152, lastLoss=0.045, valLoss=0.0531]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 83.69batch/s, counter=5, epoch=153, lastLoss=0.0439, valLoss=0.0563]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 83.39batch/s, counter=6, epoch=154, lastLoss=0.0462, valLoss=0.053]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 80.53batch/s, counter=7, epoch=155, lastLoss=0.0466, valLoss=0.0609]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 83.61batch/s, counter=8, epoch=156, lastLoss=0.045, valLoss=0.0586]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 74.57batch/s, counter=9, epoch=157, lastLoss=0.0464, valLoss=0.0527]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 73.93batch/s, counter=10, epoch=158, lastLoss=0.0453, valLoss=0.0538]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.81batch/s, counter=11, epoch=159, lastLoss=0.0463, valLoss=0.0535]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 85.99batch/s, counter=12, epoch=160, lastLoss=0.0458, valLoss=0.0564]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.34batch/s, counter=13, epoch=161, lastLoss=0.0445, valLoss=0.0539]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 87.35batch/s, counter=14, epoch=162, lastLoss=0.0497, valLoss=0.0661]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 84.85batch/s, counter=15, epoch=163, lastLoss=0.0459, valLoss=0.0545]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 78.27batch/s, counter=16, epoch=164, lastLoss=0.0453, valLoss=0.0537]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 78.87batch/s, counter=17, epoch=165, lastLoss=0.045, valLoss=0.0559]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.16batch/s, counter=18, epoch=166, lastLoss=0.0448, valLoss=0.0527]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 86.09batch/s, counter=19, epoch=167, lastLoss=0.0464, valLoss=0.0591]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 84.07batch/s, counter=20, epoch=168, lastLoss=0.0445, valLoss=0.0549]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.44batch/s, counter=21, epoch=169, lastLoss=0.0452, valLoss=0.0593]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 84.17batch/s, counter=22, epoch=170, lastLoss=0.0435, valLoss=0.0542]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 83.70batch/s, counter=0, epoch=171, lastLoss=0.0419, valLoss=0.051]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 85.35batch/s, counter=1, epoch=172, lastLoss=0.0435, valLoss=0.0548]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 78.87batch/s, counter=2, epoch=173, lastLoss=0.0428, valLoss=0.0597]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 83.09batch/s, counter=3, epoch=174, lastLoss=0.0466, valLoss=0.0596]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 88.17batch/s, counter=4, epoch=175, lastLoss=0.0438, valLoss=0.059]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 87.02batch/s, counter=5, epoch=176, lastLoss=0.0428, valLoss=0.0617]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.24batch/s, counter=6, epoch=177, lastLoss=0.0422, valLoss=0.0555]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 90.25batch/s, counter=7, epoch=178, lastLoss=0.0465, valLoss=0.0563]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 88.47batch/s, counter=8, epoch=179, lastLoss=0.0417, valLoss=0.0591]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.94batch/s, counter=9, epoch=180, lastLoss=0.0426, valLoss=0.0526]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.26batch/s, counter=10, epoch=181, lastLoss=0.0416, valLoss=0.0554]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 78.38batch/s, counter=11, epoch=182, lastLoss=0.0424, valLoss=0.0519]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 78.70batch/s, counter=12, epoch=183, lastLoss=0.0427, valLoss=0.0628]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 68.42batch/s, counter=13, epoch=184, lastLoss=0.0426, valLoss=0.053]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 79.92batch/s, counter=14, epoch=185, lastLoss=0.0402, valLoss=0.0529]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 84.29batch/s, counter=15, epoch=186, lastLoss=0.0424, valLoss=0.0524]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 82.17batch/s, counter=16, epoch=187, lastLoss=0.0404, valLoss=0.0577]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 79.21batch/s, counter=17, epoch=188, lastLoss=0.0412, valLoss=0.0534]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 80.81batch/s, counter=18, epoch=189, lastLoss=0.0427, valLoss=0.0674]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 79.66batch/s, counter=19, epoch=190, lastLoss=0.0417, valLoss=0.0564]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 76.87batch/s, counter=20, epoch=191, lastLoss=0.0421, valLoss=0.0529]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 76.50batch/s, counter=21, epoch=192, lastLoss=0.0427, valLoss=0.0528]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 79.26batch/s, counter=22, epoch=193, lastLoss=0.04, valLoss=0.0618]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 82.47batch/s, counter=23, epoch=194, lastLoss=0.0433, valLoss=0.0588]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 80.52batch/s, counter=24, epoch=195, lastLoss=0.0421, valLoss=0.0517]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 82.60batch/s, counter=25, epoch=196, lastLoss=0.0417, valLoss=0.0537]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 80.52batch/s, counter=26, epoch=197, lastLoss=0.0436, valLoss=0.0583]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 77.35batch/s, counter=27, epoch=198, lastLoss=0.0403, valLoss=0.0572]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 79.60batch/s, counter=28, epoch=199, lastLoss=0.0418, valLoss=0.0572]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 79.09batch/s, counter=29, epoch=200, lastLoss=0.04, valLoss=0.0523]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 81.34batch/s, counter=0, epoch=201, lastLoss=0.0407, valLoss=0.0509]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 79.89batch/s, counter=1, epoch=202, lastLoss=0.0405, valLoss=0.0527]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 81.63batch/s, counter=2, epoch=203, lastLoss=0.0407, valLoss=0.0589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 56/56 [00:00<00:00, 77.62batch/s, counter=0, epoch=204, lastLoss=0.0385, valLoss=0.0487]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 80.00batch/s, counter=1, epoch=205, lastLoss=0.0396, valLoss=0.0514]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 81.75batch/s, counter=2, epoch=206, lastLoss=0.0384, valLoss=0.0489]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 84.11batch/s, counter=3, epoch=207, lastLoss=0.0384, valLoss=0.0547]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 77.07batch/s, counter=4, epoch=208, lastLoss=0.0391, valLoss=0.056]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 83.11batch/s, counter=5, epoch=209, lastLoss=0.0411, valLoss=0.0562]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 91.13batch/s, counter=6, epoch=210, lastLoss=0.0401, valLoss=0.0555]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 85.44batch/s, counter=7, epoch=211, lastLoss=0.039, valLoss=0.0489]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 84.06batch/s, counter=8, epoch=212, lastLoss=0.039, valLoss=0.0556]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 85.89batch/s, counter=9, epoch=213, lastLoss=0.0395, valLoss=0.0556]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 80.29batch/s, counter=10, epoch=214, lastLoss=0.0401, valLoss=0.0547]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 87.39batch/s, counter=11, epoch=215, lastLoss=0.0399, valLoss=0.051]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.53batch/s, counter=12, epoch=216, lastLoss=0.0396, valLoss=0.0597]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 87.05batch/s, counter=0, epoch=217, lastLoss=0.0397, valLoss=0.0485]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.85batch/s, counter=1, epoch=218, lastLoss=0.0389, valLoss=0.0556]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.21batch/s, counter=2, epoch=219, lastLoss=0.0402, valLoss=0.0589]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 90.05batch/s, counter=3, epoch=220, lastLoss=0.039, valLoss=0.0524]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 87.91batch/s, counter=4, epoch=221, lastLoss=0.0396, valLoss=0.0543]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.15batch/s, counter=5, epoch=222, lastLoss=0.0388, valLoss=0.0489]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 87.88batch/s, counter=6, epoch=223, lastLoss=0.0398, valLoss=0.0548]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 86.25batch/s, counter=7, epoch=224, lastLoss=0.0383, valLoss=0.0549]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 86.63batch/s, counter=8, epoch=225, lastLoss=0.0403, valLoss=0.0532]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 87.28batch/s, counter=9, epoch=226, lastLoss=0.0397, valLoss=0.052]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 81.48batch/s, counter=10, epoch=227, lastLoss=0.0388, valLoss=0.05]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 84.55batch/s, counter=11, epoch=228, lastLoss=0.037, valLoss=0.054]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 85.61batch/s, counter=12, epoch=229, lastLoss=0.0384, valLoss=0.0558]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 74.72batch/s, counter=13, epoch=230, lastLoss=0.0359, valLoss=0.0514]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 86.07batch/s, counter=14, epoch=231, lastLoss=0.0358, valLoss=0.0505]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 80.98batch/s, counter=15, epoch=232, lastLoss=0.0375, valLoss=0.0534]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 82.32batch/s, counter=16, epoch=233, lastLoss=0.0381, valLoss=0.0511]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 84.91batch/s, counter=17, epoch=234, lastLoss=0.039, valLoss=0.056]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 82.13batch/s, counter=18, epoch=235, lastLoss=0.0364, valLoss=0.0511]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 74.22batch/s, counter=19, epoch=236, lastLoss=0.0377, valLoss=0.0572]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 72.73batch/s, counter=20, epoch=237, lastLoss=0.0384, valLoss=0.0505]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 74.09batch/s, counter=21, epoch=238, lastLoss=0.0372, valLoss=0.0514]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 63.03batch/s, counter=22, epoch=239, lastLoss=0.0367, valLoss=0.0516]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 63.10batch/s, counter=23, epoch=240, lastLoss=0.0379, valLoss=0.0586]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 62.67batch/s, counter=24, epoch=241, lastLoss=0.0352, valLoss=0.0499]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 61.44batch/s, counter=25, epoch=242, lastLoss=0.0368, valLoss=0.0525]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 61.88batch/s, counter=26, epoch=243, lastLoss=0.0364, valLoss=0.0501]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 62.54batch/s, counter=27, epoch=244, lastLoss=0.0361, valLoss=0.0526]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 62.61batch/s, counter=28, epoch=245, lastLoss=0.0369, valLoss=0.0532]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 61.17batch/s, counter=29, epoch=246, lastLoss=0.037, valLoss=0.0493]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 62.29batch/s, counter=30, epoch=247, lastLoss=0.0342, valLoss=0.0547]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 62.08batch/s, counter=31, epoch=248, lastLoss=0.0368, valLoss=0.0567]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 61.98batch/s, counter=32, epoch=249, lastLoss=0.0359, valLoss=0.0493]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 59.93batch/s, counter=33, epoch=250, lastLoss=0.0376, valLoss=0.0512]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 61.61batch/s, counter=34, epoch=251, lastLoss=0.0359, valLoss=0.0553]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 60.87batch/s, counter=35, epoch=252, lastLoss=0.039, valLoss=0.0638]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 62.89batch/s, counter=36, epoch=253, lastLoss=0.0361, valLoss=0.0543]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 61.61batch/s, counter=37, epoch=254, lastLoss=0.036, valLoss=0.0519]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 62.02batch/s, counter=38, epoch=255, lastLoss=0.0354, valLoss=0.0494]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 60.77batch/s, counter=39, epoch=256, lastLoss=0.0359, valLoss=0.0518]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 61.54batch/s, counter=40, epoch=257, lastLoss=0.0344, valLoss=0.0503]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 63.17batch/s, counter=0, epoch=258, lastLoss=0.0358, valLoss=0.0482]\n",
      "100%|█████████████████████████| 56/56 [00:00<00:00, 62.36batch/s, counter=1, epoch=259, lastLoss=0.035, valLoss=0.0551]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 62.29batch/s, counter=2, epoch=260, lastLoss=0.0341, valLoss=0.0487]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 60.97batch/s, counter=3, epoch=261, lastLoss=0.0361, valLoss=0.0526]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 61.78batch/s, counter=4, epoch=262, lastLoss=0.0338, valLoss=0.0541]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 60.87batch/s, counter=5, epoch=263, lastLoss=0.0334, valLoss=0.0503]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 61.91batch/s, counter=6, epoch=264, lastLoss=0.0338, valLoss=0.0516]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 62.12batch/s, counter=7, epoch=265, lastLoss=0.0349, valLoss=0.0505]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 61.98batch/s, counter=8, epoch=266, lastLoss=0.0343, valLoss=0.0547]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 60.94batch/s, counter=9, epoch=267, lastLoss=0.0323, valLoss=0.0525]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 60.87batch/s, counter=10, epoch=268, lastLoss=0.0357, valLoss=0.0541]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 67.18batch/s, counter=11, epoch=269, lastLoss=0.0329, valLoss=0.0519]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.94batch/s, counter=12, epoch=270, lastLoss=0.0345, valLoss=0.0487]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 84.09batch/s, counter=13, epoch=271, lastLoss=0.0362, valLoss=0.0483]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.99batch/s, counter=14, epoch=272, lastLoss=0.0322, valLoss=0.0525]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 81.11batch/s, counter=0, epoch=273, lastLoss=0.0327, valLoss=0.0461]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 86.76batch/s, counter=1, epoch=274, lastLoss=0.0333, valLoss=0.0538]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 85.60batch/s, counter=2, epoch=275, lastLoss=0.0314, valLoss=0.0503]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 85.84batch/s, counter=3, epoch=276, lastLoss=0.0365, valLoss=0.0496]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 88.05batch/s, counter=4, epoch=277, lastLoss=0.0324, valLoss=0.0507]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.73batch/s, counter=5, epoch=278, lastLoss=0.0342, valLoss=0.0518]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 90.13batch/s, counter=6, epoch=279, lastLoss=0.0334, valLoss=0.0512]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.60batch/s, counter=7, epoch=280, lastLoss=0.0328, valLoss=0.0537]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 88.31batch/s, counter=8, epoch=281, lastLoss=0.0317, valLoss=0.0484]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 87.79batch/s, counter=9, epoch=282, lastLoss=0.0335, valLoss=0.0504]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.87batch/s, counter=10, epoch=283, lastLoss=0.0332, valLoss=0.0611]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.46batch/s, counter=11, epoch=284, lastLoss=0.0331, valLoss=0.0484]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.10batch/s, counter=12, epoch=285, lastLoss=0.0329, valLoss=0.0518]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.37batch/s, counter=13, epoch=286, lastLoss=0.0329, valLoss=0.0474]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.24batch/s, counter=14, epoch=287, lastLoss=0.035, valLoss=0.0601]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 89.74batch/s, counter=15, epoch=288, lastLoss=0.036, valLoss=0.0517]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.55batch/s, counter=16, epoch=289, lastLoss=0.0329, valLoss=0.0494]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 90.33batch/s, counter=17, epoch=290, lastLoss=0.0331, valLoss=0.052]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.32batch/s, counter=18, epoch=291, lastLoss=0.0336, valLoss=0.0515]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.65batch/s, counter=19, epoch=292, lastLoss=0.0314, valLoss=0.0518]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 90.15batch/s, counter=20, epoch=293, lastLoss=0.0314, valLoss=0.0518]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 90.90batch/s, counter=21, epoch=294, lastLoss=0.0358, valLoss=0.0576]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 90.58batch/s, counter=22, epoch=295, lastLoss=0.0332, valLoss=0.0505]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 90.79batch/s, counter=23, epoch=296, lastLoss=0.0309, valLoss=0.049]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.12batch/s, counter=24, epoch=297, lastLoss=0.0313, valLoss=0.0484]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.22batch/s, counter=25, epoch=298, lastLoss=0.0312, valLoss=0.0521]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 90.50batch/s, counter=26, epoch=299, lastLoss=0.0315, valLoss=0.0521]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.45batch/s, counter=27, epoch=300, lastLoss=0.0316, valLoss=0.0473]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.88batch/s, counter=28, epoch=301, lastLoss=0.0305, valLoss=0.0468]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.89batch/s, counter=29, epoch=302, lastLoss=0.0293, valLoss=0.0497]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.69batch/s, counter=30, epoch=303, lastLoss=0.0329, valLoss=0.0662]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 88.77batch/s, counter=31, epoch=304, lastLoss=0.0335, valLoss=0.0499]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 86.86batch/s, counter=32, epoch=305, lastLoss=0.0347, valLoss=0.0519]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 87.77batch/s, counter=33, epoch=306, lastLoss=0.0349, valLoss=0.0576]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 86.58batch/s, counter=34, epoch=307, lastLoss=0.0311, valLoss=0.0512]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 87.50batch/s, counter=35, epoch=308, lastLoss=0.0313, valLoss=0.0503]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.08batch/s, counter=36, epoch=309, lastLoss=0.0313, valLoss=0.0517]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 79.60batch/s, counter=37, epoch=310, lastLoss=0.0317, valLoss=0.0511]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 82.68batch/s, counter=38, epoch=311, lastLoss=0.0302, valLoss=0.0511]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 65.25batch/s, counter=39, epoch=312, lastLoss=0.0303, valLoss=0.0488]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 84.21batch/s, counter=40, epoch=313, lastLoss=0.0316, valLoss=0.0496]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 86.30batch/s, counter=41, epoch=314, lastLoss=0.0317, valLoss=0.0538]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 84.24batch/s, counter=42, epoch=315, lastLoss=0.0328, valLoss=0.0488]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 89.56batch/s, counter=43, epoch=316, lastLoss=0.0316, valLoss=0.0487]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 88.52batch/s, counter=44, epoch=317, lastLoss=0.0313, valLoss=0.049]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 84.98batch/s, counter=45, epoch=318, lastLoss=0.0311, valLoss=0.0546]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 74.88batch/s, counter=46, epoch=319, lastLoss=0.0296, valLoss=0.0519]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 82.79batch/s, counter=47, epoch=320, lastLoss=0.0304, valLoss=0.0466]\n",
      "100%|████████████████████████| 56/56 [00:00<00:00, 77.99batch/s, counter=48, epoch=321, lastLoss=0.031, valLoss=0.0493]\n",
      "100%|███████████████████████| 56/56 [00:00<00:00, 81.03batch/s, counter=49, epoch=322, lastLoss=0.0297, valLoss=0.0482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping after 323 epochs\n",
      "Average train loss: 0.023485059879758227\n",
      "Average validation loss: 0.02825133689705814\n"
     ]
    }
   ],
   "source": [
    "train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f7571ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM based on independent, with dataset using ('separated', 'natural') ('entropy', False, 'None') using f_hidden\n",
      "Multi FC, MAPE: 91.56067562103271, MAE: 0.04947319179773331\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0e47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
