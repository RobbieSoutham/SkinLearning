{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2afefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "# Req for package\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from SkinLearning.NN.Helpers import train, test, DEVICE, get_parameter_loss, set_seed\n",
    "from SkinLearning.NN.Models import MultiTemporal\n",
    "from SkinLearning.Utils.Dataset import get_dataset, get_split\n",
    "from SkinLearning.Utils.Plotting import plot_parameter_bars\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa54d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4ffeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:35<00:00, 62.47it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, scaler = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87683cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "276f1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm1 = MultiTemporal(out=\"f_output\", single_fc=False, temporal_type=\"LSTM\")\n",
    "#lstm2 = MultiTemporal(out=\"f_output\", single_fc=False, temporal_type=\"LSTM\", old=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd74da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_k_fold(dataset, model, scaler,k=5, ):\n",
    "    # Initialize k and KFold\n",
    "    kfold = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    accuracies = []\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(kfold.split(dataset), start=1):\n",
    "        print(f\"Testing fold {fold}\")\n",
    "        \n",
    "        train_set = Subset(dataset, train_index)\n",
    "        valid_set = Subset(dataset, valid_index)\n",
    "\n",
    "        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_set, batch_size=32)\n",
    "\n",
    "        # Train the model\n",
    "        train(train_loader, model, val_loader=valid_loader, LR=0.0001, epochs=300)\n",
    "\n",
    "        accuracy, _, _ = test(valid_loader, model,scaler)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Fold {fold} accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Calculate average accuracy across all folds\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(f\"Average accuracy: {average_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed707be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc883e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e02385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fold 1\n",
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 56/56 [00:02<00:00, 22.36batch/s]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 44.24batch/s, counter=0, epoch=1, lastLoss=0.353, valLoss=0.233]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 44.25batch/s, counter=0, epoch=2, lastLoss=0.191, valLoss=0.169]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 43.99batch/s, counter=0, epoch=3, lastLoss=0.165, valLoss=0.155]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 43.95batch/s, counter=0, epoch=4, lastLoss=0.153, valLoss=0.143]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 44.06batch/s, counter=0, epoch=5, lastLoss=0.143, valLoss=0.137]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 42.74batch/s, counter=0, epoch=6, lastLoss=0.134, valLoss=0.128]\n",
      "100%|█████████████████████████████| 56/56 [00:01<00:00, 44.42batch/s, counter=0, epoch=7, lastLoss=0.125, valLoss=0.12]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 44.69batch/s, counter=0, epoch=8, lastLoss=0.118, valLoss=0.114]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 42.28batch/s, counter=0, epoch=9, lastLoss=0.112, valLoss=0.108]\n",
      "100%|███████████████████████████| 56/56 [00:01<00:00, 42.33batch/s, counter=0, epoch=10, lastLoss=0.107, valLoss=0.105]\n",
      "100%|███████████████████████████| 56/56 [00:01<00:00, 43.47batch/s, counter=0, epoch=11, lastLoss=0.105, valLoss=0.104]\n",
      "100%|███████████████████████████| 56/56 [00:01<00:00, 42.76batch/s, counter=0, epoch=12, lastLoss=0.106, valLoss=0.101]\n",
      "100%|███████████████████████████| 56/56 [00:01<00:00, 43.77batch/s, counter=0, epoch=13, lastLoss=0.103, valLoss=0.102]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 41.99batch/s, counter=0, epoch=14, lastLoss=0.101, valLoss=0.0982]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 41.30batch/s, counter=0, epoch=15, lastLoss=0.0997, valLoss=0.097]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 42.47batch/s, counter=0, epoch=16, lastLoss=0.102, valLoss=0.0968]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 43.10batch/s, counter=0, epoch=17, lastLoss=0.1, valLoss=0.0963]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 41.06batch/s, counter=0, epoch=18, lastLoss=0.0973, valLoss=0.097]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.20batch/s, counter=0, epoch=19, lastLoss=0.0985, valLoss=0.0952]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 41.46batch/s, counter=0, epoch=20, lastLoss=0.095, valLoss=0.0992]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 42.01batch/s, counter=0, epoch=21, lastLoss=0.096, valLoss=0.0927]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 41.07batch/s, counter=0, epoch=22, lastLoss=0.0941, valLoss=0.0978]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.30batch/s, counter=0, epoch=23, lastLoss=0.0937, valLoss=0.0934]\n",
      "100%|███████████████████████████| 56/56 [00:01<00:00, 43.72batch/s, counter=0, epoch=24, lastLoss=0.095, valLoss=0.106]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.59batch/s, counter=0, epoch=25, lastLoss=0.0965, valLoss=0.0936]\n",
      "100%|███████████████████████████| 56/56 [00:01<00:00, 45.04batch/s, counter=0, epoch=26, lastLoss=0.0931, valLoss=0.09]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 44.20batch/s, counter=0, epoch=27, lastLoss=0.0934, valLoss=0.092]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 44.73batch/s, counter=0, epoch=28, lastLoss=0.093, valLoss=0.0911]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.64batch/s, counter=0, epoch=29, lastLoss=0.0922, valLoss=0.0967]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.81batch/s, counter=0, epoch=30, lastLoss=0.0918, valLoss=0.0921]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.13batch/s, counter=0, epoch=31, lastLoss=0.0907, valLoss=0.0894]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.67batch/s, counter=0, epoch=32, lastLoss=0.0917, valLoss=0.0888]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.61batch/s, counter=0, epoch=33, lastLoss=0.0916, valLoss=0.0931]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.68batch/s, counter=0, epoch=34, lastLoss=0.0912, valLoss=0.0887]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.63batch/s, counter=0, epoch=35, lastLoss=0.0901, valLoss=0.0888]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.17batch/s, counter=0, epoch=36, lastLoss=0.0893, valLoss=0.0885]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.30batch/s, counter=0, epoch=37, lastLoss=0.0896, valLoss=0.0891]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 43.27batch/s, counter=0, epoch=38, lastLoss=0.091, valLoss=0.0888]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.68batch/s, counter=0, epoch=39, lastLoss=0.0906, valLoss=0.0905]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.45batch/s, counter=0, epoch=40, lastLoss=0.0892, valLoss=0.0873]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.08batch/s, counter=0, epoch=41, lastLoss=0.0886, valLoss=0.0883]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.09batch/s, counter=0, epoch=42, lastLoss=0.0905, valLoss=0.0863]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.85batch/s, counter=0, epoch=43, lastLoss=0.0879, valLoss=0.0859]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.06batch/s, counter=0, epoch=44, lastLoss=0.0879, valLoss=0.0883]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.95batch/s, counter=0, epoch=45, lastLoss=0.0871, valLoss=0.0867]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.63batch/s, counter=0, epoch=46, lastLoss=0.0882, valLoss=0.0845]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.88batch/s, counter=0, epoch=47, lastLoss=0.0879, valLoss=0.0863]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 42.35batch/s, counter=0, epoch=48, lastLoss=0.087, valLoss=0.0843]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.05batch/s, counter=0, epoch=49, lastLoss=0.0881, valLoss=0.0893]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 43.04batch/s, counter=0, epoch=50, lastLoss=0.0861, valLoss=0.085]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.02batch/s, counter=0, epoch=51, lastLoss=0.0861, valLoss=0.0861]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.12batch/s, counter=0, epoch=52, lastLoss=0.0857, valLoss=0.0844]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 44.02batch/s, counter=0, epoch=53, lastLoss=0.0854, valLoss=0.084]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.01batch/s, counter=0, epoch=54, lastLoss=0.0841, valLoss=0.0798]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.53batch/s, counter=0, epoch=55, lastLoss=0.0817, valLoss=0.0883]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 45.30batch/s, counter=0, epoch=56, lastLoss=0.084, valLoss=0.0846]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 43.99batch/s, counter=0, epoch=57, lastLoss=0.087, valLoss=0.0841]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.52batch/s, counter=0, epoch=58, lastLoss=0.0837, valLoss=0.0817]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.06batch/s, counter=0, epoch=59, lastLoss=0.0848, valLoss=0.0885]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.69batch/s, counter=0, epoch=60, lastLoss=0.0843, valLoss=0.0839]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.36batch/s, counter=0, epoch=61, lastLoss=0.0835, valLoss=0.0832]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.23batch/s, counter=0, epoch=62, lastLoss=0.0795, valLoss=0.0774]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 42.85batch/s, counter=0, epoch=63, lastLoss=0.083, valLoss=0.0769]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.11batch/s, counter=0, epoch=64, lastLoss=0.0793, valLoss=0.0803]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 43.97batch/s, counter=0, epoch=65, lastLoss=0.079, valLoss=0.0836]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.24batch/s, counter=0, epoch=66, lastLoss=0.0798, valLoss=0.0802]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.28batch/s, counter=0, epoch=67, lastLoss=0.0779, valLoss=0.0744]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.66batch/s, counter=0, epoch=68, lastLoss=0.0769, valLoss=0.0742]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.48batch/s, counter=0, epoch=69, lastLoss=0.0769, valLoss=0.0777]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.65batch/s, counter=0, epoch=70, lastLoss=0.0785, valLoss=0.0771]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 45.21batch/s, counter=0, epoch=71, lastLoss=0.0772, valLoss=0.0721]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 42.90batch/s, counter=0, epoch=72, lastLoss=0.078, valLoss=0.0741]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.50batch/s, counter=0, epoch=73, lastLoss=0.0771, valLoss=0.0768]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.08batch/s, counter=0, epoch=74, lastLoss=0.0746, valLoss=0.0712]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.73batch/s, counter=0, epoch=75, lastLoss=0.0737, valLoss=0.0734]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.31batch/s, counter=0, epoch=76, lastLoss=0.0757, valLoss=0.0762]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.20batch/s, counter=0, epoch=77, lastLoss=0.0732, valLoss=0.0719]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.56batch/s, counter=0, epoch=78, lastLoss=0.0732, valLoss=0.0689]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.81batch/s, counter=0, epoch=79, lastLoss=0.0718, valLoss=0.0664]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.10batch/s, counter=0, epoch=80, lastLoss=0.0729, valLoss=0.0709]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.58batch/s, counter=0, epoch=81, lastLoss=0.0724, valLoss=0.0705]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.81batch/s, counter=0, epoch=82, lastLoss=0.0742, valLoss=0.0725]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.55batch/s, counter=0, epoch=83, lastLoss=0.0717, valLoss=0.0732]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.84batch/s, counter=0, epoch=84, lastLoss=0.0698, valLoss=0.0705]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.71batch/s, counter=0, epoch=85, lastLoss=0.0708, valLoss=0.0659]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.41batch/s, counter=0, epoch=86, lastLoss=0.0707, valLoss=0.0684]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 41.33batch/s, counter=0, epoch=87, lastLoss=0.0718, valLoss=0.0794]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.34batch/s, counter=0, epoch=88, lastLoss=0.0718, valLoss=0.0676]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.04batch/s, counter=0, epoch=89, lastLoss=0.0707, valLoss=0.0682]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.08batch/s, counter=0, epoch=90, lastLoss=0.0711, valLoss=0.0719]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.78batch/s, counter=0, epoch=91, lastLoss=0.0733, valLoss=0.0682]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.19batch/s, counter=0, epoch=92, lastLoss=0.0683, valLoss=0.0648]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 42.99batch/s, counter=0, epoch=93, lastLoss=0.0681, valLoss=0.065]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.10batch/s, counter=0, epoch=94, lastLoss=0.0678, valLoss=0.0653]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.03batch/s, counter=0, epoch=95, lastLoss=0.0689, valLoss=0.0622]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.61batch/s, counter=0, epoch=96, lastLoss=0.0665, valLoss=0.0636]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.52batch/s, counter=0, epoch=97, lastLoss=0.0678, valLoss=0.0616]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.02batch/s, counter=0, epoch=98, lastLoss=0.0666, valLoss=0.0647]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 43.12batch/s, counter=0, epoch=99, lastLoss=0.067, valLoss=0.0652]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.64batch/s, counter=0, epoch=100, lastLoss=0.0669, valLoss=0.0614]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.13batch/s, counter=0, epoch=101, lastLoss=0.0652, valLoss=0.0622]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.01batch/s, counter=0, epoch=102, lastLoss=0.0652, valLoss=0.0636]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.93batch/s, counter=0, epoch=103, lastLoss=0.066, valLoss=0.0638]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.65batch/s, counter=0, epoch=104, lastLoss=0.0679, valLoss=0.0658]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.57batch/s, counter=0, epoch=105, lastLoss=0.064, valLoss=0.0624]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.11batch/s, counter=0, epoch=106, lastLoss=0.0625, valLoss=0.059]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.92batch/s, counter=0, epoch=107, lastLoss=0.0643, valLoss=0.0655]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.36batch/s, counter=0, epoch=108, lastLoss=0.0675, valLoss=0.0659]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.13batch/s, counter=0, epoch=109, lastLoss=0.0648, valLoss=0.0606]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.05batch/s, counter=0, epoch=110, lastLoss=0.0636, valLoss=0.0625]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.60batch/s, counter=0, epoch=111, lastLoss=0.0633, valLoss=0.0589]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 40.77batch/s, counter=0, epoch=112, lastLoss=0.0631, valLoss=0.067]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.09batch/s, counter=0, epoch=113, lastLoss=0.0626, valLoss=0.0592]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.88batch/s, counter=0, epoch=114, lastLoss=0.0624, valLoss=0.061]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.10batch/s, counter=0, epoch=115, lastLoss=0.0642, valLoss=0.0587]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.28batch/s, counter=0, epoch=116, lastLoss=0.0631, valLoss=0.0634]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.67batch/s, counter=0, epoch=117, lastLoss=0.0636, valLoss=0.0628]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.06batch/s, counter=0, epoch=118, lastLoss=0.0613, valLoss=0.059]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.44batch/s, counter=0, epoch=119, lastLoss=0.0637, valLoss=0.0581]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.62batch/s, counter=0, epoch=120, lastLoss=0.0613, valLoss=0.0591]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.43batch/s, counter=0, epoch=121, lastLoss=0.0629, valLoss=0.0624]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.62batch/s, counter=0, epoch=122, lastLoss=0.0627, valLoss=0.0615]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.62batch/s, counter=0, epoch=123, lastLoss=0.0615, valLoss=0.0573]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.88batch/s, counter=0, epoch=124, lastLoss=0.0644, valLoss=0.0613]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.35batch/s, counter=0, epoch=125, lastLoss=0.0615, valLoss=0.061]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.57batch/s, counter=0, epoch=126, lastLoss=0.0613, valLoss=0.0612]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.80batch/s, counter=0, epoch=127, lastLoss=0.0613, valLoss=0.0668]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.17batch/s, counter=0, epoch=128, lastLoss=0.0641, valLoss=0.0598]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.12batch/s, counter=0, epoch=129, lastLoss=0.0606, valLoss=0.0681]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.19batch/s, counter=0, epoch=130, lastLoss=0.0624, valLoss=0.059]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.22batch/s, counter=0, epoch=131, lastLoss=0.0612, valLoss=0.0595]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 42.83batch/s, counter=0, epoch=132, lastLoss=0.06, valLoss=0.0606]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.53batch/s, counter=0, epoch=133, lastLoss=0.0599, valLoss=0.0552]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.15batch/s, counter=0, epoch=134, lastLoss=0.0566, valLoss=0.0552]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.05batch/s, counter=0, epoch=135, lastLoss=0.0592, valLoss=0.0674]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 56/56 [00:01<00:00, 44.33batch/s, counter=0, epoch=136, lastLoss=0.06, valLoss=0.0565]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 40.40batch/s, counter=0, epoch=137, lastLoss=0.0583, valLoss=0.0612]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.45batch/s, counter=0, epoch=138, lastLoss=0.0591, valLoss=0.057]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.26batch/s, counter=0, epoch=139, lastLoss=0.0587, valLoss=0.0595]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.00batch/s, counter=0, epoch=140, lastLoss=0.0613, valLoss=0.0597]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.65batch/s, counter=0, epoch=141, lastLoss=0.0603, valLoss=0.0619]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.27batch/s, counter=0, epoch=142, lastLoss=0.0571, valLoss=0.0547]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.87batch/s, counter=0, epoch=143, lastLoss=0.0582, valLoss=0.0584]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 40.81batch/s, counter=0, epoch=144, lastLoss=0.0578, valLoss=0.0555]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.79batch/s, counter=0, epoch=145, lastLoss=0.0581, valLoss=0.0581]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.73batch/s, counter=0, epoch=146, lastLoss=0.0564, valLoss=0.057]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.95batch/s, counter=0, epoch=147, lastLoss=0.0584, valLoss=0.0573]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.53batch/s, counter=0, epoch=148, lastLoss=0.0591, valLoss=0.0546]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.30batch/s, counter=0, epoch=149, lastLoss=0.0565, valLoss=0.057]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.45batch/s, counter=0, epoch=150, lastLoss=0.0587, valLoss=0.0537]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.14batch/s, counter=0, epoch=151, lastLoss=0.0572, valLoss=0.056]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.54batch/s, counter=0, epoch=152, lastLoss=0.0565, valLoss=0.0589]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.64batch/s, counter=0, epoch=153, lastLoss=0.0573, valLoss=0.058]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.18batch/s, counter=0, epoch=154, lastLoss=0.0576, valLoss=0.0578]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.85batch/s, counter=0, epoch=155, lastLoss=0.0615, valLoss=0.0567]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 42.25batch/s, counter=0, epoch=156, lastLoss=0.056, valLoss=0.056]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.38batch/s, counter=0, epoch=157, lastLoss=0.0585, valLoss=0.0559]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.90batch/s, counter=0, epoch=158, lastLoss=0.0557, valLoss=0.0556]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.98batch/s, counter=0, epoch=159, lastLoss=0.0579, valLoss=0.057]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.52batch/s, counter=0, epoch=160, lastLoss=0.0566, valLoss=0.0553]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.21batch/s, counter=0, epoch=161, lastLoss=0.0556, valLoss=0.0567]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.36batch/s, counter=0, epoch=162, lastLoss=0.0597, valLoss=0.054]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.60batch/s, counter=0, epoch=163, lastLoss=0.0549, valLoss=0.059]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.59batch/s, counter=0, epoch=164, lastLoss=0.0584, valLoss=0.0554]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.19batch/s, counter=0, epoch=165, lastLoss=0.0549, valLoss=0.0543]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.78batch/s, counter=0, epoch=166, lastLoss=0.0545, valLoss=0.0578]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.78batch/s, counter=0, epoch=167, lastLoss=0.0564, valLoss=0.0565]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.82batch/s, counter=0, epoch=168, lastLoss=0.0546, valLoss=0.0572]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.73batch/s, counter=0, epoch=169, lastLoss=0.0554, valLoss=0.0539]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.83batch/s, counter=0, epoch=170, lastLoss=0.0548, valLoss=0.0547]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.36batch/s, counter=0, epoch=171, lastLoss=0.0564, valLoss=0.0591]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.59batch/s, counter=0, epoch=172, lastLoss=0.0561, valLoss=0.0581]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.15batch/s, counter=0, epoch=173, lastLoss=0.0568, valLoss=0.0553]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.26batch/s, counter=0, epoch=174, lastLoss=0.056, valLoss=0.0532]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.91batch/s, counter=0, epoch=175, lastLoss=0.0534, valLoss=0.0591]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.96batch/s, counter=0, epoch=176, lastLoss=0.0576, valLoss=0.0558]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.14batch/s, counter=0, epoch=177, lastLoss=0.0539, valLoss=0.0535]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.72batch/s, counter=0, epoch=178, lastLoss=0.0558, valLoss=0.056]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.68batch/s, counter=0, epoch=179, lastLoss=0.0556, valLoss=0.0552]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.41batch/s, counter=0, epoch=180, lastLoss=0.055, valLoss=0.0553]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.16batch/s, counter=0, epoch=181, lastLoss=0.0565, valLoss=0.0562]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.33batch/s, counter=0, epoch=182, lastLoss=0.0542, valLoss=0.0547]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.41batch/s, counter=0, epoch=183, lastLoss=0.0536, valLoss=0.0553]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.08batch/s, counter=0, epoch=184, lastLoss=0.0535, valLoss=0.0522]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.43batch/s, counter=0, epoch=185, lastLoss=0.0538, valLoss=0.053]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.76batch/s, counter=0, epoch=186, lastLoss=0.0543, valLoss=0.0589]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.13batch/s, counter=0, epoch=187, lastLoss=0.0548, valLoss=0.0548]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.24batch/s, counter=0, epoch=188, lastLoss=0.0529, valLoss=0.0548]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.79batch/s, counter=0, epoch=189, lastLoss=0.0544, valLoss=0.0527]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.65batch/s, counter=0, epoch=190, lastLoss=0.0548, valLoss=0.0523]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.41batch/s, counter=0, epoch=191, lastLoss=0.0546, valLoss=0.0513]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.68batch/s, counter=0, epoch=192, lastLoss=0.0523, valLoss=0.0548]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.04batch/s, counter=0, epoch=193, lastLoss=0.0554, valLoss=0.0532]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.22batch/s, counter=0, epoch=194, lastLoss=0.0531, valLoss=0.0552]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.59batch/s, counter=0, epoch=195, lastLoss=0.0527, valLoss=0.0551]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.19batch/s, counter=0, epoch=196, lastLoss=0.0528, valLoss=0.0517]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.61batch/s, counter=0, epoch=197, lastLoss=0.0529, valLoss=0.0502]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.09batch/s, counter=0, epoch=198, lastLoss=0.0558, valLoss=0.0536]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.49batch/s, counter=0, epoch=199, lastLoss=0.0526, valLoss=0.0523]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.36batch/s, counter=0, epoch=200, lastLoss=0.0551, valLoss=0.0552]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.89batch/s, counter=0, epoch=201, lastLoss=0.0534, valLoss=0.0514]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.55batch/s, counter=0, epoch=202, lastLoss=0.0524, valLoss=0.0515]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.71batch/s, counter=0, epoch=203, lastLoss=0.0523, valLoss=0.0613]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 56/56 [00:01<00:00, 42.39batch/s, counter=0, epoch=204, lastLoss=0.0517, valLoss=0.055]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.79batch/s, counter=0, epoch=205, lastLoss=0.0533, valLoss=0.0532]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.87batch/s, counter=0, epoch=206, lastLoss=0.0525, valLoss=0.0493]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.60batch/s, counter=0, epoch=207, lastLoss=0.0501, valLoss=0.0499]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.50batch/s, counter=0, epoch=208, lastLoss=0.0519, valLoss=0.0507]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.72batch/s, counter=0, epoch=209, lastLoss=0.0532, valLoss=0.0559]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.51batch/s, counter=0, epoch=210, lastLoss=0.0536, valLoss=0.0521]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.68batch/s, counter=0, epoch=211, lastLoss=0.0546, valLoss=0.0522]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.33batch/s, counter=0, epoch=212, lastLoss=0.0526, valLoss=0.0538]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.53batch/s, counter=0, epoch=213, lastLoss=0.0517, valLoss=0.0515]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.82batch/s, counter=0, epoch=214, lastLoss=0.0515, valLoss=0.0518]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.89batch/s, counter=0, epoch=215, lastLoss=0.0508, valLoss=0.0496]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.31batch/s, counter=0, epoch=216, lastLoss=0.0519, valLoss=0.0505]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.18batch/s, counter=0, epoch=217, lastLoss=0.051, valLoss=0.0534]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.97batch/s, counter=0, epoch=218, lastLoss=0.054, valLoss=0.0532]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.90batch/s, counter=0, epoch=219, lastLoss=0.0554, valLoss=0.0499]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.02batch/s, counter=0, epoch=220, lastLoss=0.0528, valLoss=0.0575]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.77batch/s, counter=0, epoch=221, lastLoss=0.0524, valLoss=0.0497]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.83batch/s, counter=0, epoch=222, lastLoss=0.0516, valLoss=0.0515]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.65batch/s, counter=0, epoch=223, lastLoss=0.0511, valLoss=0.0514]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.22batch/s, counter=0, epoch=224, lastLoss=0.0521, valLoss=0.0554]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.53batch/s, counter=0, epoch=225, lastLoss=0.0505, valLoss=0.0502]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.58batch/s, counter=0, epoch=226, lastLoss=0.0516, valLoss=0.0527]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.44batch/s, counter=0, epoch=227, lastLoss=0.0508, valLoss=0.0511]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.69batch/s, counter=0, epoch=228, lastLoss=0.0522, valLoss=0.0555]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.96batch/s, counter=0, epoch=229, lastLoss=0.0506, valLoss=0.0515]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 39.44batch/s, counter=0, epoch=230, lastLoss=0.0507, valLoss=0.0533]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.26batch/s, counter=0, epoch=231, lastLoss=0.0501, valLoss=0.0535]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.53batch/s, counter=0, epoch=232, lastLoss=0.0532, valLoss=0.0512]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 39.08batch/s, counter=0, epoch=233, lastLoss=0.0499, valLoss=0.0589]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 39.38batch/s, counter=0, epoch=234, lastLoss=0.0505, valLoss=0.0563]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.26batch/s, counter=0, epoch=235, lastLoss=0.0521, valLoss=0.0503]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.23batch/s, counter=0, epoch=236, lastLoss=0.0512, valLoss=0.0502]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.97batch/s, counter=0, epoch=237, lastLoss=0.0498, valLoss=0.0514]\n",
      "100%|██████████████████████████| 56/56 [00:01<00:00, 44.15batch/s, counter=0, epoch=238, lastLoss=0.0507, valLoss=0.05]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.86batch/s, counter=0, epoch=239, lastLoss=0.0507, valLoss=0.0561]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.65batch/s, counter=0, epoch=240, lastLoss=0.0486, valLoss=0.0605]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.07batch/s, counter=0, epoch=241, lastLoss=0.0501, valLoss=0.0507]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.66batch/s, counter=0, epoch=242, lastLoss=0.0513, valLoss=0.0567]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.43batch/s, counter=0, epoch=243, lastLoss=0.0501, valLoss=0.0513]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.99batch/s, counter=0, epoch=244, lastLoss=0.0515, valLoss=0.0507]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.29batch/s, counter=0, epoch=245, lastLoss=0.0497, valLoss=0.0555]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.50batch/s, counter=0, epoch=246, lastLoss=0.0508, valLoss=0.0498]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.41batch/s, counter=0, epoch=247, lastLoss=0.0493, valLoss=0.0485]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.12batch/s, counter=0, epoch=248, lastLoss=0.0512, valLoss=0.0549]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.19batch/s, counter=0, epoch=249, lastLoss=0.0494, valLoss=0.0526]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 41.96batch/s, counter=0, epoch=250, lastLoss=0.051, valLoss=0.0507]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.47batch/s, counter=0, epoch=251, lastLoss=0.0509, valLoss=0.0522]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.44batch/s, counter=0, epoch=252, lastLoss=0.0517, valLoss=0.0492]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.41batch/s, counter=0, epoch=253, lastLoss=0.0483, valLoss=0.0494]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.75batch/s, counter=0, epoch=254, lastLoss=0.0491, valLoss=0.0582]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.47batch/s, counter=0, epoch=255, lastLoss=0.0504, valLoss=0.0512]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.29batch/s, counter=0, epoch=256, lastLoss=0.049, valLoss=0.0497]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.70batch/s, counter=0, epoch=257, lastLoss=0.0495, valLoss=0.0472]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.53batch/s, counter=0, epoch=258, lastLoss=0.0496, valLoss=0.0494]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.59batch/s, counter=0, epoch=259, lastLoss=0.0478, valLoss=0.0499]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.77batch/s, counter=0, epoch=260, lastLoss=0.0494, valLoss=0.0516]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.25batch/s, counter=0, epoch=261, lastLoss=0.0492, valLoss=0.0522]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.65batch/s, counter=0, epoch=262, lastLoss=0.0473, valLoss=0.0506]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.51batch/s, counter=0, epoch=263, lastLoss=0.0481, valLoss=0.0522]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.23batch/s, counter=0, epoch=264, lastLoss=0.048, valLoss=0.0477]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.28batch/s, counter=0, epoch=265, lastLoss=0.049, valLoss=0.0487]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 45.18batch/s, counter=0, epoch=266, lastLoss=0.0495, valLoss=0.0501]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 45.22batch/s, counter=0, epoch=267, lastLoss=0.0515, valLoss=0.0526]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.49batch/s, counter=0, epoch=268, lastLoss=0.047, valLoss=0.0486]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.54batch/s, counter=0, epoch=269, lastLoss=0.0484, valLoss=0.0538]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.27batch/s, counter=0, epoch=270, lastLoss=0.0501, valLoss=0.0515]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.71batch/s, counter=0, epoch=271, lastLoss=0.0476, valLoss=0.0502]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 56/56 [00:01<00:00, 43.62batch/s, counter=0, epoch=272, lastLoss=0.048, valLoss=0.0504]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 44.49batch/s, counter=0, epoch=273, lastLoss=0.0484, valLoss=0.053]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.86batch/s, counter=0, epoch=274, lastLoss=0.0482, valLoss=0.0524]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.66batch/s, counter=0, epoch=275, lastLoss=0.0471, valLoss=0.0513]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.06batch/s, counter=0, epoch=276, lastLoss=0.0493, valLoss=0.0503]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.44batch/s, counter=0, epoch=277, lastLoss=0.0484, valLoss=0.0543]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 40.71batch/s, counter=0, epoch=278, lastLoss=0.0486, valLoss=0.0531]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 38.21batch/s, counter=0, epoch=279, lastLoss=0.0479, valLoss=0.0506]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.26batch/s, counter=0, epoch=280, lastLoss=0.0487, valLoss=0.0494]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 44.07batch/s, counter=0, epoch=281, lastLoss=0.0471, valLoss=0.0481]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 37.30batch/s, counter=0, epoch=282, lastLoss=0.0468, valLoss=0.0498]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 39.65batch/s, counter=0, epoch=283, lastLoss=0.0465, valLoss=0.0487]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.80batch/s, counter=0, epoch=284, lastLoss=0.0461, valLoss=0.0466]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 37.40batch/s, counter=0, epoch=285, lastLoss=0.0461, valLoss=0.0483]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.08batch/s, counter=0, epoch=286, lastLoss=0.0466, valLoss=0.0482]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.97batch/s, counter=0, epoch=287, lastLoss=0.0473, valLoss=0.0498]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.74batch/s, counter=0, epoch=288, lastLoss=0.0457, valLoss=0.0508]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.98batch/s, counter=0, epoch=289, lastLoss=0.0469, valLoss=0.0513]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 43.46batch/s, counter=0, epoch=290, lastLoss=0.0466, valLoss=0.0465]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 40.82batch/s, counter=0, epoch=291, lastLoss=0.0478, valLoss=0.0511]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 37.30batch/s, counter=0, epoch=292, lastLoss=0.0484, valLoss=0.0506]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 33.27batch/s, counter=0, epoch=293, lastLoss=0.0467, valLoss=0.0483]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 42.11batch/s, counter=0, epoch=294, lastLoss=0.0455, valLoss=0.0522]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 41.50batch/s, counter=0, epoch=295, lastLoss=0.048, valLoss=0.0522]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.34batch/s, counter=0, epoch=296, lastLoss=0.0483, valLoss=0.0478]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 41.78batch/s, counter=0, epoch=297, lastLoss=0.0456, valLoss=0.0541]\n",
      "100%|█████████████████████████| 56/56 [00:01<00:00, 41.42batch/s, counter=0, epoch=298, lastLoss=0.047, valLoss=0.0538]\n",
      "100%|████████████████████████| 56/56 [00:01<00:00, 37.41batch/s, counter=0, epoch=299, lastLoss=0.0474, valLoss=0.0479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.0664737522371468\n",
      "Average validation loss: 0.0658276944036285\n",
      "Fold 1 accuracy: 92.54\n",
      "Testing fold 2\n",
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 57/57 [00:01<00:00, 36.77batch/s]\n",
      "100%|███████████████████████████| 57/57 [00:01<00:00, 43.23batch/s, counter=0, epoch=1, lastLoss=0.0543, valLoss=0.112]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 38.44batch/s, counter=0, epoch=2, lastLoss=0.0494, valLoss=0.0581]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.47batch/s, counter=0, epoch=3, lastLoss=0.0524, valLoss=0.0495]\n",
      "100%|███████████████████████████| 57/57 [00:01<00:00, 42.49batch/s, counter=0, epoch=4, lastLoss=0.053, valLoss=0.0508]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 42.38batch/s, counter=0, epoch=5, lastLoss=0.0464, valLoss=0.0526]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 41.58batch/s, counter=0, epoch=6, lastLoss=0.0504, valLoss=0.0476]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 42.51batch/s, counter=0, epoch=7, lastLoss=0.0502, valLoss=0.0463]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.13batch/s, counter=0, epoch=8, lastLoss=0.0513, valLoss=0.0536]\n",
      "100%|███████████████████████████| 57/57 [00:01<00:00, 43.53batch/s, counter=0, epoch=9, lastLoss=0.0516, valLoss=0.116]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.66batch/s, counter=0, epoch=10, lastLoss=0.0482, valLoss=0.0495]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.19batch/s, counter=0, epoch=11, lastLoss=0.0493, valLoss=0.0499]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.54batch/s, counter=0, epoch=12, lastLoss=0.0503, valLoss=0.0485]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.46batch/s, counter=0, epoch=13, lastLoss=0.052, valLoss=0.0567]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 41.74batch/s, counter=0, epoch=14, lastLoss=0.049, valLoss=0.0522]\n",
      "100%|███████████████████████████| 57/57 [00:01<00:00, 43.89batch/s, counter=0, epoch=15, lastLoss=0.05, valLoss=0.0552]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.94batch/s, counter=0, epoch=16, lastLoss=0.0477, valLoss=0.0502]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.69batch/s, counter=0, epoch=17, lastLoss=0.0527, valLoss=0.0985]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 41.45batch/s, counter=0, epoch=18, lastLoss=0.0478, valLoss=0.0477]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 40.08batch/s, counter=0, epoch=19, lastLoss=0.0499, valLoss=0.0626]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.29batch/s, counter=0, epoch=20, lastLoss=0.0557, valLoss=0.0565]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.70batch/s, counter=0, epoch=21, lastLoss=0.0503, valLoss=0.0556]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.30batch/s, counter=0, epoch=22, lastLoss=0.049, valLoss=0.0895]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.17batch/s, counter=0, epoch=23, lastLoss=0.0494, valLoss=0.0481]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.95batch/s, counter=0, epoch=24, lastLoss=0.048, valLoss=0.0687]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.44batch/s, counter=0, epoch=25, lastLoss=0.0513, valLoss=0.0534]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.60batch/s, counter=0, epoch=26, lastLoss=0.0495, valLoss=0.0459]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.02batch/s, counter=0, epoch=27, lastLoss=0.0478, valLoss=0.0484]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 42.00batch/s, counter=0, epoch=28, lastLoss=0.049, valLoss=0.0447]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.32batch/s, counter=0, epoch=29, lastLoss=0.0504, valLoss=0.0513]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.63batch/s, counter=0, epoch=30, lastLoss=0.0489, valLoss=0.0495]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.56batch/s, counter=0, epoch=31, lastLoss=0.0481, valLoss=0.0494]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.05batch/s, counter=0, epoch=32, lastLoss=0.0492, valLoss=0.0445]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.14batch/s, counter=0, epoch=33, lastLoss=0.0469, valLoss=0.0465]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.25batch/s, counter=0, epoch=34, lastLoss=0.054, valLoss=0.0476]\n",
      "100%|███████████████████████████| 57/57 [00:01<00:00, 43.97batch/s, counter=0, epoch=35, lastLoss=0.0502, valLoss=0.06]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.35batch/s, counter=0, epoch=36, lastLoss=0.0463, valLoss=0.0528]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 40.95batch/s, counter=0, epoch=37, lastLoss=0.0487, valLoss=0.0559]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.17batch/s, counter=0, epoch=38, lastLoss=0.0505, valLoss=0.0448]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.62batch/s, counter=0, epoch=39, lastLoss=0.0502, valLoss=0.0505]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.58batch/s, counter=0, epoch=40, lastLoss=0.0476, valLoss=0.0551]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.87batch/s, counter=0, epoch=41, lastLoss=0.0499, valLoss=0.0445]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.61batch/s, counter=0, epoch=42, lastLoss=0.0463, valLoss=0.0473]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.60batch/s, counter=0, epoch=43, lastLoss=0.0492, valLoss=0.0459]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 44.31batch/s, counter=0, epoch=44, lastLoss=0.0479, valLoss=0.051]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.47batch/s, counter=0, epoch=45, lastLoss=0.0464, valLoss=0.0541]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.92batch/s, counter=0, epoch=46, lastLoss=0.0475, valLoss=0.0494]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.21batch/s, counter=0, epoch=47, lastLoss=0.0456, valLoss=0.0497]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.13batch/s, counter=0, epoch=48, lastLoss=0.0475, valLoss=0.0507]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.35batch/s, counter=0, epoch=49, lastLoss=0.0444, valLoss=0.0494]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.42batch/s, counter=0, epoch=50, lastLoss=0.0431, valLoss=0.0459]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.94batch/s, counter=0, epoch=51, lastLoss=0.0474, valLoss=0.052]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.54batch/s, counter=0, epoch=52, lastLoss=0.0487, valLoss=0.062]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.64batch/s, counter=0, epoch=53, lastLoss=0.0473, valLoss=0.0446]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.17batch/s, counter=0, epoch=54, lastLoss=0.0464, valLoss=0.0503]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.69batch/s, counter=0, epoch=55, lastLoss=0.0448, valLoss=0.0466]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.97batch/s, counter=0, epoch=56, lastLoss=0.0458, valLoss=0.057]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.25batch/s, counter=0, epoch=57, lastLoss=0.0462, valLoss=0.052]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.52batch/s, counter=0, epoch=58, lastLoss=0.0478, valLoss=0.0673]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.56batch/s, counter=0, epoch=59, lastLoss=0.0467, valLoss=0.0461]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 42.94batch/s, counter=0, epoch=60, lastLoss=0.0483, valLoss=0.0483]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.33batch/s, counter=0, epoch=61, lastLoss=0.0438, valLoss=0.0503]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.88batch/s, counter=0, epoch=62, lastLoss=0.0473, valLoss=0.0868]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 43.84batch/s, counter=0, epoch=63, lastLoss=0.048, valLoss=0.0594]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.51batch/s, counter=0, epoch=64, lastLoss=0.0458, valLoss=0.0565]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 42.86batch/s, counter=0, epoch=65, lastLoss=0.052, valLoss=0.0479]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.18batch/s, counter=0, epoch=66, lastLoss=0.0441, valLoss=0.0683]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 41.04batch/s, counter=0, epoch=67, lastLoss=0.0454, valLoss=0.0475]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 57/57 [00:01<00:00, 42.22batch/s, counter=0, epoch=68, lastLoss=0.048, valLoss=0.0911]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.40batch/s, counter=0, epoch=69, lastLoss=0.0484, valLoss=0.0493]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.94batch/s, counter=0, epoch=70, lastLoss=0.0464, valLoss=0.0964]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.14batch/s, counter=0, epoch=71, lastLoss=0.0446, valLoss=0.0527]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 45.27batch/s, counter=0, epoch=72, lastLoss=0.0458, valLoss=0.0448]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.27batch/s, counter=0, epoch=73, lastLoss=0.0447, valLoss=0.0488]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 45.40batch/s, counter=0, epoch=74, lastLoss=0.0465, valLoss=0.125]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 45.17batch/s, counter=0, epoch=75, lastLoss=0.0469, valLoss=0.0516]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.09batch/s, counter=0, epoch=76, lastLoss=0.0434, valLoss=0.0528]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.44batch/s, counter=0, epoch=77, lastLoss=0.0475, valLoss=0.0564]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.62batch/s, counter=0, epoch=78, lastLoss=0.0444, valLoss=0.0474]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.30batch/s, counter=0, epoch=79, lastLoss=0.0435, valLoss=0.0448]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.57batch/s, counter=0, epoch=80, lastLoss=0.0475, valLoss=0.0515]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 44.41batch/s, counter=0, epoch=81, lastLoss=0.048, valLoss=0.0441]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 45.21batch/s, counter=0, epoch=82, lastLoss=0.0453, valLoss=0.0474]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.62batch/s, counter=0, epoch=83, lastLoss=0.0476, valLoss=0.0527]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.44batch/s, counter=0, epoch=84, lastLoss=0.0459, valLoss=0.0458]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 44.30batch/s, counter=0, epoch=85, lastLoss=0.044, valLoss=0.0465]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.77batch/s, counter=0, epoch=86, lastLoss=0.0459, valLoss=0.0482]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 45.10batch/s, counter=0, epoch=87, lastLoss=0.0468, valLoss=0.0498]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.79batch/s, counter=0, epoch=88, lastLoss=0.0439, valLoss=0.0634]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 44.10batch/s, counter=0, epoch=89, lastLoss=0.0466, valLoss=0.0549]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 44.32batch/s, counter=0, epoch=90, lastLoss=0.0449, valLoss=0.058]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 43.89batch/s, counter=0, epoch=91, lastLoss=0.0449, valLoss=0.0572]\n",
      "100%|██████████████████████████| 57/57 [00:01<00:00, 42.89batch/s, counter=0, epoch=92, lastLoss=0.0475, valLoss=0.052]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 41.78batch/s, counter=0, epoch=93, lastLoss=0.0452, valLoss=0.0499]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 41.08batch/s, counter=0, epoch=94, lastLoss=0.0421, valLoss=0.0432]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 40.06batch/s, counter=0, epoch=95, lastLoss=0.0428, valLoss=0.0512]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 35.80batch/s, counter=0, epoch=96, lastLoss=0.0435, valLoss=0.0587]\n",
      "100%|██████████████████████████| 57/57 [00:02<00:00, 27.14batch/s, counter=0, epoch=97, lastLoss=0.051, valLoss=0.0509]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 29.94batch/s, counter=0, epoch=98, lastLoss=0.0471, valLoss=0.0566]\n",
      "100%|█████████████████████████| 57/57 [00:01<00:00, 38.01batch/s, counter=0, epoch=99, lastLoss=0.0447, valLoss=0.0547]\n",
      " 70%|████████████████▊       | 40/57 [00:00<00:00, 40.94batch/s, counter=0, epoch=100, lastLoss=0.0439, valLoss=0.0597]"
     ]
    }
   ],
   "source": [
    "test_k_fold(dataset, lstm1, scaler) # 79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30507258",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, lstm2, scaler, cluster=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "045491a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46.195518493652344,\n",
       " array([ 91.46532 ,  44.912945,  99.646706,  99.12827 , -89.05632 ,\n",
       "         31.076202], dtype=float32),\n",
       " 0.1683460146188736)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader, lstm1, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ffb7242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 56/56 [00:01<00:00, 42.08batch/s]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 43.57batch/s, counter=0, epoch=1, lastLoss=0.302, valLoss=0.266]\n",
      "100%|█████████████████████████████| 56/56 [00:01<00:00, 42.84batch/s, counter=0, epoch=2, lastLoss=0.24, valLoss=0.216]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 44.74batch/s, counter=0, epoch=3, lastLoss=0.194, valLoss=0.184]\n",
      "100%|█████████████████████████████| 56/56 [00:01<00:00, 44.52batch/s, counter=0, epoch=4, lastLoss=0.18, valLoss=0.181]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 44.78batch/s, counter=0, epoch=5, lastLoss=0.173, valLoss=0.174]\n",
      "100%|████████████████████████████| 56/56 [00:01<00:00, 44.16batch/s, counter=1, epoch=6, lastLoss=0.172, valLoss=0.175]\n",
      "100%|█████████████████████████████| 56/56 [00:01<00:00, 44.84batch/s, counter=2, epoch=7, lastLoss=0.17, valLoss=0.179]\n",
      "100%|█████████████████████████████| 56/56 [00:01<00:00, 44.00batch/s, counter=3, epoch=8, lastLoss=0.17, valLoss=0.176]\n",
      "100%|█████████████████████████████| 56/56 [00:01<00:00, 45.07batch/s, counter=0, epoch=9, lastLoss=0.171, valLoss=0.17]\n",
      "100%|███████████████████████████| 56/56 [00:01<00:00, 43.97batch/s, counter=0, epoch=10, lastLoss=0.168, valLoss=0.169]\n",
      "100%|███████████████████████████| 56/56 [00:01<00:00, 42.39batch/s, counter=0, epoch=11, lastLoss=0.167, valLoss=0.169]\n",
      " 16%|████▌                       | 9/56 [00:00<00:01, 39.47batch/s, counter=0, epoch=12, lastLoss=0.165, valLoss=0.164]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\NN\\Helpers.py:115\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader, early_stopping, patience, optimizer, plot, cluster)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    113\u001b[0m             it\u001b[38;5;241m.\u001b[39mset_postfix(lastLoss\u001b[38;5;241m=\u001b[39mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], valLoss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_losses) \\\n\u001b[0;32m    114\u001b[0m                  \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m val_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], counter\u001b[38;5;241m=\u001b[39mcounter, epoch\u001b[38;5;241m=\u001b[39mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mepochs)\n\u001b[1;32m--> 115\u001b[0m         \u001b[43mprocessBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m    118\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\NN\\Helpers.py:102\u001b[0m, in \u001b[0;36mtrain.<locals>.processBatch\u001b[1;34m(ittr)\u001b[0m\n\u001b[0;32m    100\u001b[0m cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m    101\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m--> 102\u001b[0m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_loader, lstm1, val_loader=test_loader, LR=0.0001, epochs=1500, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, lstm2, early_stopping=True, epochs=1500, val_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTemporal(nn.Module):\n",
    "    def __init__(self, hidden_size=256, single_fc=True, out=\"f_hidden\", layers=1, temporal_type=\"RNN\", old=True):\n",
    "        super(MultiTemporal, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out = out\n",
    "        \n",
    "        self.cnn = deepcopy(best_CNN)\n",
    "\n",
    "        if temporal_type == \"RNN\":\n",
    "            net = nn.RNN\n",
    "        elif temporal_type == \"LSTM\":\n",
    "            net = nn.LSTM\n",
    "        elif temporal_type == \"GRU\":\n",
    "            net = nn.GRU\n",
    "        else:\n",
    "            raise Exception(\"Not a valid NN type.\")\n",
    "            \n",
    "        if not old:\n",
    "            net_inp = 15\n",
    "            self.cnn = deepcopy(new_CNN)\n",
    "        else:\n",
    "            net_inp = 6\n",
    "            self.cnn = deepcopy(best_CNN)\n",
    "        \n",
    "        self.net = net(\n",
    "            net_inp,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=layers,\n",
    "            )\n",
    "        \n",
    "        # Check size of output to determine FC input\n",
    "        input_tensor = torch.zeros(32, 512, net_inp)\n",
    "        output, _ = self.net(input_tensor)\n",
    "        \n",
    "        fc_in = hidden_size\n",
    "        if out == 'output':\n",
    "            fc_in = output.shape[1] * output.shape[2]\n",
    "        elif out == 'f-output':\n",
    "            fc_in = output.shape[2]\n",
    "        elif fc_in == 'hidden' or out == 'cell':\n",
    "            out = hidden_size * output.shape[2]\n",
    "        elif out == 'f-hiden' or out == 'f-cell':\n",
    "            fc_in = output.shape[2]\n",
    "        elif out == 'h+o' or out == 'h+c' :\n",
    "            fc_in = output.shape[1]\n",
    "        \n",
    "        if single_fc:\n",
    "            self.fc = nn.Linear(fc_in*layers, 6)\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(fc_in*layers, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128 , 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 6),   \n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.cnn(x)\n",
    "    \n",
    "        o, (h, c) = self.net(x)\n",
    "        \n",
    "        if self.out == \"f_hidden\":\n",
    "            x = h[-1].reshape(batch_size, -1)\n",
    "        elif self.out == \"hidden\":\n",
    "            x = h.reshape(batch_size, -1)\n",
    "        elif self.out == \"f_output\":\n",
    "            x = o[:, -1, :].reshape(batch_size, -1)\n",
    "        elif self.out == \"output\":\n",
    "            x = o.reshape(batch_size, -1)\n",
    "        elif self.out == \"h+c\":\n",
    "            x = torch.concat([h[-1], c[-1]], dim=1).view(batch_size, -1)\n",
    "        elif self.out == \"h+o\":\n",
    "            x = torch.concat([h[-1], o[:, -1, :]], dim=1).view(o.size(0), -1)\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_CNN = nn.Sequential(\n",
    "    nn.Conv1d(2, 128, kernel_size=5, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=5, stride=5),\n",
    "\n",
    "    nn.Conv1d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv1d(256, 512, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2, stride=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_CNN = nn.Sequential(\n",
    "    nn.Conv1d(2, 128, kernel_size=5, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "\n",
    "    nn.Conv1d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv1d(256, 512, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2, stride=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01838d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Running model: Up Sample) (1/1\n",
      "--------------------------------------------------\n",
      "Testing fold 1\n",
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/56 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (1024x1x1). Calculated output size: (1024x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m acc, p_acc \u001b[38;5;241m=\u001b[39m \u001b[43mKFCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[0;32m     29\u001b[0m p_accs\u001b[38;5;241m.\u001b[39mappend(p_acc)\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\NN\\Helpers.py:278\u001b[0m, in \u001b[0;36mKFCV\u001b[1;34m(dataset, model, scaler, k, cluster)\u001b[0m\n\u001b[0;32m    275\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(valid_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcluster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m accuracy, p_accuracy, _ \u001b[38;5;241m=\u001b[39m test(valid_loader, model, scaler, cluster\u001b[38;5;241m=\u001b[39mcluster)\n\u001b[0;32m    287\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\NN\\Helpers.py:115\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader, early_stopping, patience, optimizer, plot, cluster)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    113\u001b[0m             it\u001b[38;5;241m.\u001b[39mset_postfix(lastLoss\u001b[38;5;241m=\u001b[39mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], valLoss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_losses) \\\n\u001b[0;32m    114\u001b[0m                  \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m val_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], counter\u001b[38;5;241m=\u001b[39mcounter, epoch\u001b[38;5;241m=\u001b[39mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mepochs)\n\u001b[1;32m--> 115\u001b[0m         \u001b[43mprocessBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m    118\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\NN\\Helpers.py:98\u001b[0m, in \u001b[0;36mtrain.<locals>.processBatch\u001b[1;34m(ittr)\u001b[0m\n\u001b[0;32m     95\u001b[0m inp, out \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     97\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 98\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m    101\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\NN\\Models.py:384\u001b[0m, in \u001b[0;36mDualUp.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    383\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 384\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    388\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:92\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:696\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (1024x1x1). Calculated output size: (1024x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "from SkinLearning.NN.Models import DualDown, DualDownUp, DualUp, DualUpDown\n",
    "from SkinLearning.NN.Helpers import KFCV\n",
    "models = [\n",
    "       # nn.DataParallel(DualDownUp()),\n",
    "        nn.DataParallel(DualUp()),\n",
    "       # nn.DataParallel(DualDown()),\n",
    "       # nn.DataParallel(DualUpDown())\n",
    "\n",
    "    ]\n",
    "    \n",
    "names = [\n",
    "    #\"Down Sample/Up Sample\",\n",
    "    \"Up Sample\",\n",
    "    #\"Down Sample\",\n",
    "    #\"Up Sample/Down Sample\"\n",
    "]\n",
    "\n",
    "#dataset, scaler = getDataset()\n",
    "\n",
    "accuracies = []\n",
    "p_accs = []\n",
    "for i, model in enumerate(models):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Running model: {names[i]}) ({i+1}/{len(names)}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    acc, p_acc = KFCV(dataset, model, scaler, cluster=False)\n",
    "    accuracies.append(acc)\n",
    "    p_accs.append(p_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8d1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
