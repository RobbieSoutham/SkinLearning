{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5838657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import jit\n",
    "import pickle\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import pywt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.cuda import FloatTensor\n",
    "\n",
    "# Req for package\n",
    "sys.path.append(\"../\")\n",
    "from SkinLearning.Utils.NN import train, test, DEVICE\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037faeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder name will correspond to index of sample\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, scaler, signalFolder=\"D:/SamplingResults2\", sampleFile=\"../Data/newSamples.pkl\", runs=range(65535), steps=128):\n",
    "        # Load both disp1 and disp2 from each folder\n",
    "        # Folders ordered according to index of sample\n",
    "        # Use the corresponding sample as y -> append probe?\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        with open(f\"{sampleFile}\", \"rb\") as f:\n",
    "             samples = pickle.load(f)\n",
    "        \n",
    "        self.min = np.min(samples[runs])\n",
    "        self.max = np.max(samples[runs])\n",
    "        \n",
    "        \n",
    "        for run in tqdm(runs):\n",
    "            inp = []\n",
    "            fail = False\n",
    "            \n",
    "            files = os.listdir(f\"{signalFolder}/{run}/\")\n",
    "            \n",
    "            if files != ['Disp1.csv', 'Disp2.csv']:\n",
    "                continue\n",
    "            \n",
    "            for file in files:\n",
    "                a = pd.read_csv(f\"{signalFolder}/{run}/{file}\")\n",
    "                a.rename(columns = {'0':'x', '0.1': 'y'}, inplace = True)\n",
    "                \n",
    "                # Skip if unconverged\n",
    "                if a['x'].max() != 7.0:\n",
    "                    fail = True\n",
    "                    break\n",
    "\n",
    "                # Interpolate curve for consistent x values\n",
    "                xNew = np.linspace(0, 7, num=steps, endpoint=False)\n",
    "                interped = interp1d(a['x'], a['y'], kind='cubic', fill_value=\"extrapolate\")(xNew)\n",
    "                    \n",
    "                \n",
    "                inp.append(interped.astype(\"float32\"))\n",
    "            \n",
    "            if not fail:\n",
    "                if len(inp) != 2:\n",
    "                    raise Exception(\"sdf\")\n",
    "\n",
    "                self.input.append(inp)\n",
    "                self.output.append(samples[int(run)])\n",
    "        \n",
    "        scaler.fit(self.output)\n",
    "        self.output = scaler.fit_transform(self.output)\n",
    "        self.output = tensor(self.output).type(FloatTensor)\n",
    "        \n",
    "        self.input = [waveletExtraction(sample) for sample in self.input]\n",
    "        self.input = tensor(self.input).type(FloatTensor)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"input\": self.input[idx], \"output\": self.output[idx]}\n",
    "        return sample\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3e5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates the data set from filtered samples\n",
    "    Returns the dataset and the scaler\n",
    "\"\"\"\n",
    "def getDataset(**kwargs):\n",
    "    # Get filtered data\n",
    "    if not 'runs' in kwargs.keys():\n",
    "        with open(\"../Data/filtered.pkl\", \"rb\") as f:\n",
    "            runs = pickle.load(f)\n",
    "\n",
    "        kwargs['runs'] = runs\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset = SkinDataset(scaler=scaler, **kwargs)\n",
    "\n",
    "    return dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "383e9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates a train/test split from the given data\n",
    "    Returns train and test data loaders\n",
    "\"\"\"\n",
    "def getSplit(dataset, p1=0.8):\n",
    "    train_n = int(p1 * len(dataset))\n",
    "    test_n = len(dataset) - train_n\n",
    "    train_set, test_set = random_split(dataset, [train_n, test_n])\n",
    "\n",
    "    return DataLoader(train_set, batch_size=32, shuffle=True), \\\n",
    "        DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f0adfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(x, wavelet='db1', level=4):\n",
    "# perform wavelet packet decomposition on signal 1\n",
    "    wp = pywt.WaveletPacket(x[0], wavelet, mode='symmetric', maxlevel=level)\n",
    "    coeffs1 = []\n",
    "    for node in wp.get_level(level, 'natural'):\n",
    "        if node.path.endswith('a') or node.path.endswith('d'):\n",
    "            coeffs1.append(node.data)\n",
    "    coeffs1 = np.concatenate(coeffs1)\n",
    "    \n",
    "    # perform wavelet packet decomposition on signal 2\n",
    "    wp = pywt.WaveletPacket(x[1], wavelet, mode='symmetric', maxlevel=level)\n",
    "    coeffs2 = []\n",
    "    for node in wp.get_level(level, 'natural'):\n",
    "        if node.path.endswith('a') or node.path.endswith('d'):\n",
    "            coeffs2.append(node.data)\n",
    "    coeffs2 = np.concatenate(coeffs2)\n",
    "    \n",
    "    # concatenate the two coefficient arrays\n",
    "    feature_vector = np.concatenate((coeffs1, coeffs2))\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0284c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:35<00:00, 63.39it/s]\n",
      "C:\\Users\\rjsou\\AppData\\Local\\Temp\\ipykernel_12052\\4192844632.py:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  self.input = tensor(self.input).type(FloatTensor)\n"
     ]
    }
   ],
   "source": [
    "dataset, scaler = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38074a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = getSplit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "89cfc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRNN(nn.Module):\n",
    "    def __init__(self, input_size=256, hidden_size=512):\n",
    "        super(SiameseRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=3, batch_first=True)\n",
    "                \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = x.reshape(batch_size, 1, -1)\n",
    "        \n",
    "        #x1 = x[:, 0, :, :].reshape(batch_size, 1, -1)\n",
    "        #x2 = x[:, 0, :, :].reshape(batch_size, 1, -1)\n",
    "        \n",
    "        h0 = torch.zeros(3, batch_size, self.hidden_size).to(DEVICE)\n",
    "        _, h1 = self.rnn(x, h0)  # Add a batch dimension\n",
    "        #_, h2 = self.rnn(x2, h0)  # Add a batch dimension\n",
    "        \n",
    "        out = h1[-1]\n",
    "        out = out.reshape(batch_size, -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "44d31e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, input_size=256, hidden_size=1024, num_layers=3):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048 , 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 6),\n",
    "        )\n",
    "\n",
    "     \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        #x1 = x[:, 0, :].unsqueeze(1)\n",
    "        #x2 = x[:, 0, :].unsqueeze(1)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers*1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*1, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass through the LSTM layer\n",
    "        o, h = self.lstm(x, (h0, c0))\n",
    "        #out2 = self.lstm(x2, (h0, c0))[1][-1]\n",
    "\n",
    "        \n",
    "        #out = torch.cat([out1, out2], dim=0)\n",
    "        out = h[-1]\n",
    "        out = out.reshape(batch_size, -1)\n",
    "        # Pass the last hidden state through the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "83474d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sRNN = SiameseRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "690080cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|█████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 127.27batch/s]\n",
      "Epoch 2/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 132.70batch/s, lastLoss=0.204, valLoss=0.178]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 124.86batch/s, lastLoss=0.18, valLoss=0.182]\n",
      "Epoch 4/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 139.65batch/s, lastLoss=0.168, valLoss=0.171]\n",
      "Epoch 5/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 136.25batch/s, lastLoss=0.164, valLoss=0.155]\n",
      "Epoch 6/400: 100%|███████████████████████████████████| 56/56 [00:00<00:00, 135.26batch/s, lastLoss=0.16, valLoss=0.155]\n",
      "Epoch 7/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 139.13batch/s, lastLoss=0.154, valLoss=0.156]\n",
      "Epoch 8/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 134.13batch/s, lastLoss=0.151, valLoss=0.146]\n",
      "Epoch 9/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 131.45batch/s, lastLoss=0.143, valLoss=0.139]\n",
      "Epoch 10/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 130.84batch/s, lastLoss=0.141, valLoss=0.147]\n",
      "Epoch 11/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 138.61batch/s, lastLoss=0.137, valLoss=0.143]\n",
      "Epoch 12/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 136.25batch/s, lastLoss=0.134, valLoss=0.142]\n",
      "Epoch 13/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 136.08batch/s, lastLoss=0.139, valLoss=0.137]\n",
      "Epoch 14/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 135.26batch/s, lastLoss=0.132, valLoss=0.129]\n",
      "Epoch 15/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 137.08batch/s, lastLoss=0.127, valLoss=0.126]\n",
      "Epoch 16/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 131.45batch/s, lastLoss=0.121, valLoss=0.115]\n",
      "Epoch 17/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 135.92batch/s, lastLoss=0.117, valLoss=0.119]\n",
      "Epoch 18/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 137.93batch/s, lastLoss=0.117, valLoss=0.117]\n",
      "Epoch 19/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 131.45batch/s, lastLoss=0.114, valLoss=0.116]\n",
      "Epoch 20/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 138.10batch/s, lastLoss=0.112, valLoss=0.108]\n",
      "Epoch 21/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 133.33batch/s, lastLoss=0.108, valLoss=0.102]\n",
      "Epoch 22/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 133.33batch/s, lastLoss=0.107, valLoss=0.115]\n",
      "Epoch 23/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 131.76batch/s, lastLoss=0.106, valLoss=0.116]\n",
      "Epoch 24/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 136.75batch/s, lastLoss=0.105, valLoss=0.116]\n",
      "Epoch 25/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 137.59batch/s, lastLoss=0.11, valLoss=0.112]\n",
      "Epoch 26/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 132.23batch/s, lastLoss=0.104, valLoss=0.107]\n",
      "Epoch 27/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 132.86batch/s, lastLoss=0.102, valLoss=0.0989]\n",
      "Epoch 28/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 132.23batch/s, lastLoss=0.101, valLoss=0.109]\n",
      "Epoch 29/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 122.54batch/s, lastLoss=0.105, valLoss=0.0989]\n",
      "Epoch 30/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 125.98batch/s, lastLoss=0.101, valLoss=0.103]\n",
      "Epoch 31/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 126.27batch/s, lastLoss=0.0979, valLoss=0.0988]\n",
      "Epoch 32/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 132.23batch/s, lastLoss=0.101, valLoss=0.0934]\n",
      "Epoch 33/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 134.13batch/s, lastLoss=0.0984, valLoss=0.104]\n",
      "Epoch 34/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 126.84batch/s, lastLoss=0.0984, valLoss=0.103]\n",
      "Epoch 35/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 131.30batch/s, lastLoss=0.0993, valLoss=0.0979]\n",
      "Epoch 36/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 120.56batch/s, lastLoss=0.101, valLoss=0.108]\n",
      "Epoch 37/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 133.17batch/s, lastLoss=0.0971, valLoss=0.101]\n",
      "Epoch 38/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 134.45batch/s, lastLoss=0.0961, valLoss=0.0931]\n",
      "Epoch 39/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 130.38batch/s, lastLoss=0.0966, valLoss=0.0903]\n",
      "Epoch 40/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 133.97batch/s, lastLoss=0.095, valLoss=0.102]\n",
      "Epoch 41/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 128.14batch/s, lastLoss=0.0945, valLoss=0.0978]\n",
      "Epoch 42/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 126.55batch/s, lastLoss=0.0955, valLoss=0.0973]\n",
      "Epoch 43/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 127.42batch/s, lastLoss=0.0954, valLoss=0.0933]\n",
      "Epoch 44/400:  86%|██████████████████████████▌    | 48/56 [00:00<00:00, 129.38batch/s, lastLoss=0.0953, valLoss=0.0937]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [196]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sRNN_train_loss, sRNN_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     33\u001b[0m         cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     34\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 35\u001b[0m         \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sRNN_train_loss, sRNN_val_loss =  train(train_loader, sRNN, val_loader=test_loader, LR=0.001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sLSTM = SiameseLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a786494c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|█████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 136.42batch/s]\n",
      "Epoch 2/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 136.75batch/s, lastLoss=0.211, valLoss=0.178]\n",
      "Epoch 3/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 134.61batch/s, lastLoss=0.178, valLoss=0.173]\n",
      "Epoch 4/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 132.86batch/s, lastLoss=0.167, valLoss=0.165]\n",
      "Epoch 5/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 140.00batch/s, lastLoss=0.161, valLoss=0.164]\n",
      "Epoch 6/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 136.92batch/s, lastLoss=0.159, valLoss=0.163]\n",
      "Epoch 7/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 144.14batch/s, lastLoss=0.155, valLoss=0.148]\n",
      "Epoch 8/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 132.86batch/s, lastLoss=0.151, valLoss=0.146]\n",
      "Epoch 9/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 136.75batch/s, lastLoss=0.145, valLoss=0.143]\n",
      "Epoch 10/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 133.17batch/s, lastLoss=0.143, valLoss=0.142]\n",
      "Epoch 11/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 144.70batch/s, lastLoss=0.141, valLoss=0.139]\n",
      "Epoch 12/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 133.81batch/s, lastLoss=0.137, valLoss=0.141]\n",
      "Epoch 13/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 143.59batch/s, lastLoss=0.137, valLoss=0.137]\n",
      "Epoch 14/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 138.27batch/s, lastLoss=0.129, valLoss=0.127]\n",
      "Epoch 15/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 137.42batch/s, lastLoss=0.122, valLoss=0.116]\n",
      "Epoch 16/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 130.38batch/s, lastLoss=0.116, valLoss=0.124]\n",
      "Epoch 17/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 143.04batch/s, lastLoss=0.113, valLoss=0.114]\n",
      "Epoch 18/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 132.07batch/s, lastLoss=0.112, valLoss=0.125]\n",
      "Epoch 19/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 136.92batch/s, lastLoss=0.112, valLoss=0.111]\n",
      "Epoch 20/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 128.00batch/s, lastLoss=0.108, valLoss=0.112]\n",
      "Epoch 21/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 135.75batch/s, lastLoss=0.108, valLoss=0.105]\n",
      "Epoch 22/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 132.23batch/s, lastLoss=0.109, valLoss=0.113]\n",
      "Epoch 23/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 135.26batch/s, lastLoss=0.104, valLoss=0.107]\n",
      "Epoch 24/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 137.42batch/s, lastLoss=0.104, valLoss=0.106]\n",
      "Epoch 25/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 134.13batch/s, lastLoss=0.1, valLoss=0.0971]\n",
      "Epoch 26/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 135.59batch/s, lastLoss=0.101, valLoss=0.0987]\n",
      "Epoch 27/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 137.42batch/s, lastLoss=0.0996, valLoss=0.113]\n",
      "Epoch 28/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 142.49batch/s, lastLoss=0.0995, valLoss=0.115]\n",
      "Epoch 29/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 133.17batch/s, lastLoss=0.102, valLoss=0.0979]\n",
      "Epoch 30/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 138.95batch/s, lastLoss=0.0974, valLoss=0.098]\n",
      "Epoch 31/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 134.61batch/s, lastLoss=0.0946, valLoss=0.0972]\n",
      "Epoch 32/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 143.59batch/s, lastLoss=0.0981, valLoss=0.104]\n",
      "Epoch 33/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 139.65batch/s, lastLoss=0.0933, valLoss=0.0967]\n",
      "Epoch 34/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 138.10batch/s, lastLoss=0.0943, valLoss=0.0965]\n",
      "Epoch 35/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 129.78batch/s, lastLoss=0.0935, valLoss=0.0981]\n",
      "Epoch 36/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 140.88batch/s, lastLoss=0.0943, valLoss=0.0941]\n",
      "Epoch 37/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 136.08batch/s, lastLoss=0.0909, valLoss=0.0938]\n",
      "Epoch 38/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 144.70batch/s, lastLoss=0.0936, valLoss=0.0919]\n",
      "Epoch 39/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 130.69batch/s, lastLoss=0.0905, valLoss=0.0947]\n",
      "Epoch 40/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 137.08batch/s, lastLoss=0.0939, valLoss=0.0936]\n",
      "Epoch 41/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 138.10batch/s, lastLoss=0.0945, valLoss=0.102]\n",
      "Epoch 42/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 139.13batch/s, lastLoss=0.0888, valLoss=0.0939]\n",
      "Epoch 43/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 138.95batch/s, lastLoss=0.0888, valLoss=0.0874]\n",
      "Epoch 44/400: 100%|██████████████████████████████████| 56/56 [00:00<00:00, 141.23batch/s, lastLoss=0.09, valLoss=0.089]\n",
      "Epoch 45/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 136.92batch/s, lastLoss=0.0883, valLoss=0.0911]\n",
      "Epoch 46/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 139.13batch/s, lastLoss=0.0886, valLoss=0.0842]\n",
      "Epoch 47/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 134.13batch/s, lastLoss=0.0877, valLoss=0.0888]\n",
      "Epoch 48/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 136.75batch/s, lastLoss=0.0906, valLoss=0.0928]\n",
      "Epoch 49/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 139.13batch/s, lastLoss=0.0862, valLoss=0.0865]\n",
      "Epoch 50/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 132.39batch/s, lastLoss=0.0873, valLoss=0.0909]\n",
      "Epoch 51/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 137.76batch/s, lastLoss=0.0868, valLoss=0.0949]\n",
      "Epoch 52/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 136.08batch/s, lastLoss=0.0878, valLoss=0.0868]\n",
      "Epoch 53/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 135.76batch/s, lastLoss=0.0865, valLoss=0.0881]\n",
      "Epoch 54/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 135.92batch/s, lastLoss=0.0848, valLoss=0.0873]\n",
      "Epoch 55/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 140.88batch/s, lastLoss=0.0876, valLoss=0.0885]\n",
      "Epoch 56/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 132.54batch/s, lastLoss=0.0862, valLoss=0.0879]\n",
      "Epoch 57/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 144.70batch/s, lastLoss=0.0868, valLoss=0.0947]\n",
      "Epoch 58/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 138.44batch/s, lastLoss=0.0861, valLoss=0.0883]\n",
      "Epoch 59/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 146.40batch/s, lastLoss=0.0866, valLoss=0.0864]\n",
      "Epoch 60/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 139.30batch/s, lastLoss=0.0871, valLoss=0.0858]\n",
      "Epoch 61/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 142.49batch/s, lastLoss=0.0846, valLoss=0.0858]\n",
      "Epoch 62/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 137.25batch/s, lastLoss=0.0843, valLoss=0.0901]\n",
      "Epoch 63/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 142.49batch/s, lastLoss=0.084, valLoss=0.0861]\n",
      "Epoch 64/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 133.97batch/s, lastLoss=0.085, valLoss=0.0844]\n",
      "Epoch 65/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 143.04batch/s, lastLoss=0.089, valLoss=0.0864]\n",
      "Epoch 66/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 136.59batch/s, lastLoss=0.0887, valLoss=0.0886]\n",
      "Epoch 67/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 142.31batch/s, lastLoss=0.0859, valLoss=0.0941]\n",
      "Epoch 68/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 133.49batch/s, lastLoss=0.0846, valLoss=0.0874]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 140.88batch/s, lastLoss=0.0841, valLoss=0.0918]\n",
      "Epoch 70/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 135.75batch/s, lastLoss=0.0835, valLoss=0.0868]\n",
      "Epoch 71/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 141.77batch/s, lastLoss=0.083, valLoss=0.0845]\n",
      "Epoch 72/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 140.35batch/s, lastLoss=0.0835, valLoss=0.0861]\n",
      "Epoch 73/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 142.67batch/s, lastLoss=0.0833, valLoss=0.0866]\n",
      "Epoch 74/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 140.35batch/s, lastLoss=0.0849, valLoss=0.0844]\n",
      "Epoch 75/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 136.92batch/s, lastLoss=0.0853, valLoss=0.0917]\n",
      "Epoch 76/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 143.04batch/s, lastLoss=0.0855, valLoss=0.087]\n",
      "Epoch 77/400: 100%|█████████████████████████████████| 56/56 [00:00<00:00, 137.76batch/s, lastLoss=0.083, valLoss=0.094]\n",
      "Epoch 78/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 142.49batch/s, lastLoss=0.0849, valLoss=0.0932]\n",
      "Epoch 79/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 139.13batch/s, lastLoss=0.0842, valLoss=0.0883]\n",
      "Epoch 80/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 142.13batch/s, lastLoss=0.084, valLoss=0.0914]\n",
      "Epoch 81/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 135.75batch/s, lastLoss=0.084, valLoss=0.0828]\n",
      "Epoch 82/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 141.95batch/s, lastLoss=0.0837, valLoss=0.0921]\n",
      "Epoch 83/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 133.97batch/s, lastLoss=0.0856, valLoss=0.0829]\n",
      "Epoch 84/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 144.51batch/s, lastLoss=0.0823, valLoss=0.0974]\n",
      "Epoch 85/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 138.27batch/s, lastLoss=0.0847, valLoss=0.0884]\n",
      "Epoch 86/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 142.49batch/s, lastLoss=0.085, valLoss=0.0866]\n",
      "Epoch 87/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 137.42batch/s, lastLoss=0.0816, valLoss=0.0864]\n",
      "Epoch 88/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 143.78batch/s, lastLoss=0.0845, valLoss=0.0918]\n",
      "Epoch 89/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 136.09batch/s, lastLoss=0.0849, valLoss=0.0855]\n",
      "Epoch 90/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 133.97batch/s, lastLoss=0.0829, valLoss=0.085]\n",
      "Epoch 91/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 133.97batch/s, lastLoss=0.0845, valLoss=0.0926]\n",
      "Epoch 92/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 140.35batch/s, lastLoss=0.0836, valLoss=0.0883]\n",
      "Epoch 93/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 135.26batch/s, lastLoss=0.0836, valLoss=0.0901]\n",
      "Epoch 94/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 137.08batch/s, lastLoss=0.0832, valLoss=0.0846]\n",
      "Epoch 95/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 128.59batch/s, lastLoss=0.0836, valLoss=0.0894]\n",
      "Epoch 96/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 138.78batch/s, lastLoss=0.0822, valLoss=0.0867]\n",
      "Epoch 97/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 137.93batch/s, lastLoss=0.0834, valLoss=0.0885]\n",
      "Epoch 98/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 144.33batch/s, lastLoss=0.0818, valLoss=0.0866]\n",
      "Epoch 99/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 129.77batch/s, lastLoss=0.0828, valLoss=0.0948]\n",
      "Epoch 100/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 137.76batch/s, lastLoss=0.0869, valLoss=0.0892]\n",
      "Epoch 101/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 135.43batch/s, lastLoss=0.0833, valLoss=0.0906]\n",
      "Epoch 102/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 138.61batch/s, lastLoss=0.0825, valLoss=0.0876]\n",
      "Epoch 103/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 145.26batch/s, lastLoss=0.0816, valLoss=0.087]\n",
      "Epoch 104/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 137.94batch/s, lastLoss=0.0833, valLoss=0.0907]\n",
      "Epoch 105/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 140.17batch/s, lastLoss=0.0833, valLoss=0.0846]\n",
      "Epoch 106/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 135.75batch/s, lastLoss=0.083, valLoss=0.0871]\n",
      "Epoch 107/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 138.96batch/s, lastLoss=0.0841, valLoss=0.0883]\n",
      "Epoch 108/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 132.54batch/s, lastLoss=0.0833, valLoss=0.0929]\n",
      "Epoch 109/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 143.04batch/s, lastLoss=0.084, valLoss=0.0859]\n",
      "Epoch 110/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 138.95batch/s, lastLoss=0.0828, valLoss=0.0903]\n",
      "Epoch 111/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 145.07batch/s, lastLoss=0.0818, valLoss=0.0954]\n",
      "Epoch 112/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 137.93batch/s, lastLoss=0.0842, valLoss=0.0836]\n",
      "Epoch 113/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 142.13batch/s, lastLoss=0.0827, valLoss=0.0907]\n",
      "Epoch 114/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 135.10batch/s, lastLoss=0.0839, valLoss=0.0851]\n",
      "Epoch 115/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 142.67batch/s, lastLoss=0.082, valLoss=0.0839]\n",
      "Epoch 116/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 136.92batch/s, lastLoss=0.0831, valLoss=0.0882]\n",
      "Epoch 117/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 141.23batch/s, lastLoss=0.0834, valLoss=0.0831]\n",
      "Epoch 118/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 138.78batch/s, lastLoss=0.0822, valLoss=0.105]\n",
      "Epoch 119/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 147.37batch/s, lastLoss=0.0838, valLoss=0.0863]\n",
      "Epoch 120/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 128.00batch/s, lastLoss=0.0811, valLoss=0.0864]\n",
      "Epoch 121/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 146.21batch/s, lastLoss=0.0812, valLoss=0.0871]\n",
      "Epoch 122/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 134.29batch/s, lastLoss=0.0823, valLoss=0.092]\n",
      "Epoch 123/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 144.89batch/s, lastLoss=0.0821, valLoss=0.0894]\n",
      "Epoch 124/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 137.93batch/s, lastLoss=0.0816, valLoss=0.0826]\n",
      "Epoch 125/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 139.65batch/s, lastLoss=0.0853, valLoss=0.084]\n",
      "Epoch 126/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 140.17batch/s, lastLoss=0.0813, valLoss=0.0879]\n",
      "Epoch 127/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 141.59batch/s, lastLoss=0.0816, valLoss=0.0863]\n",
      "Epoch 128/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 137.76batch/s, lastLoss=0.0829, valLoss=0.0874]\n",
      "Epoch 129/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 141.41batch/s, lastLoss=0.0825, valLoss=0.0894]\n",
      "Epoch 130/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 140.52batch/s, lastLoss=0.0809, valLoss=0.0894]\n",
      "Epoch 131/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 137.08batch/s, lastLoss=0.0812, valLoss=0.095]\n",
      "Epoch 132/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 138.78batch/s, lastLoss=0.0811, valLoss=0.0841]\n",
      "Epoch 133/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 129.33batch/s, lastLoss=0.0839, valLoss=0.0863]\n",
      "Epoch 134/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 143.21batch/s, lastLoss=0.0828, valLoss=0.0856]\n",
      "Epoch 135/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 134.45batch/s, lastLoss=0.0811, valLoss=0.0893]\n",
      "Epoch 136/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 144.51batch/s, lastLoss=0.0822, valLoss=0.0839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 138.10batch/s, lastLoss=0.0812, valLoss=0.0823]\n",
      "Epoch 138/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 148.93batch/s, lastLoss=0.0823, valLoss=0.0868]\n",
      "Epoch 139/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 139.30batch/s, lastLoss=0.0837, valLoss=0.0875]\n",
      "Epoch 140/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 144.51batch/s, lastLoss=0.0836, valLoss=0.0937]\n",
      "Epoch 141/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 140.00batch/s, lastLoss=0.0821, valLoss=0.083]\n",
      "Epoch 142/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 138.27batch/s, lastLoss=0.0804, valLoss=0.0874]\n",
      "Epoch 143/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 137.08batch/s, lastLoss=0.0822, valLoss=0.088]\n",
      "Epoch 144/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 143.40batch/s, lastLoss=0.0814, valLoss=0.0841]\n",
      "Epoch 145/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 135.60batch/s, lastLoss=0.0812, valLoss=0.084]\n",
      "Epoch 146/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 142.49batch/s, lastLoss=0.0826, valLoss=0.0912]\n",
      "Epoch 147/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 138.95batch/s, lastLoss=0.0804, valLoss=0.0884]\n",
      "Epoch 148/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 140.52batch/s, lastLoss=0.0817, valLoss=0.0848]\n",
      "Epoch 149/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 130.99batch/s, lastLoss=0.0811, valLoss=0.0943]\n",
      "Epoch 150/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 142.31batch/s, lastLoss=0.0811, valLoss=0.0884]\n",
      "Epoch 151/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 185.73batch/s, lastLoss=0.0836, valLoss=0.0913]\n",
      "Epoch 152/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.88batch/s, lastLoss=0.0818, valLoss=0.0916]\n",
      "Epoch 153/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 198.58batch/s, lastLoss=0.0812, valLoss=0.0875]\n",
      "Epoch 154/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 215.80batch/s, lastLoss=0.0823, valLoss=0.0848]\n",
      "Epoch 155/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 229.62batch/s, lastLoss=0.0801, valLoss=0.0871]\n",
      "Epoch 156/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 235.72batch/s, lastLoss=0.08, valLoss=0.0901]\n",
      "Epoch 157/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 235.73batch/s, lastLoss=0.0806, valLoss=0.0868]\n",
      "Epoch 158/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 224.88batch/s, lastLoss=0.0798, valLoss=0.0877]\n",
      "Epoch 159/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 215.38batch/s, lastLoss=0.0809, valLoss=0.092]\n",
      "Epoch 160/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 221.56batch/s, lastLoss=0.0806, valLoss=0.0892]\n",
      "Epoch 161/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.13batch/s, lastLoss=0.0801, valLoss=0.0904]\n",
      "Epoch 162/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 226.40batch/s, lastLoss=0.083, valLoss=0.0868]\n",
      "Epoch 163/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 230.98batch/s, lastLoss=0.08, valLoss=0.0876]\n",
      "Epoch 164/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 219.59batch/s, lastLoss=0.084, valLoss=0.0837]\n",
      "Epoch 165/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 223.09batch/s, lastLoss=0.0801, valLoss=0.0845]\n",
      "Epoch 166/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 236.27batch/s, lastLoss=0.0829, valLoss=0.095]\n",
      "Epoch 167/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 231.60batch/s, lastLoss=0.0806, valLoss=0.0912]\n",
      "Epoch 168/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 220.47batch/s, lastLoss=0.0794, valLoss=0.0894]\n",
      "Epoch 169/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 238.29batch/s, lastLoss=0.0797, valLoss=0.0888]\n",
      "Epoch 170/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 231.83batch/s, lastLoss=0.0806, valLoss=0.0883]\n",
      "Epoch 171/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 231.17batch/s, lastLoss=0.0817, valLoss=0.0904]\n",
      "Epoch 172/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 231.41batch/s, lastLoss=0.08, valLoss=0.0938]\n",
      "Epoch 173/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 234.29batch/s, lastLoss=0.0799, valLoss=0.0872]\n",
      "Epoch 174/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 229.97batch/s, lastLoss=0.081, valLoss=0.0916]\n",
      "Epoch 175/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 235.77batch/s, lastLoss=0.0816, valLoss=0.0889]\n",
      "Epoch 176/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 235.78batch/s, lastLoss=0.0845, valLoss=0.0948]\n",
      "Epoch 177/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 215.79batch/s, lastLoss=0.08, valLoss=0.0935]\n",
      "Epoch 178/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 227.64batch/s, lastLoss=0.0791, valLoss=0.0894]\n",
      "Epoch 179/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 218.19batch/s, lastLoss=0.0801, valLoss=0.0859]\n",
      "Epoch 180/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 228.09batch/s, lastLoss=0.0788, valLoss=0.091]\n",
      "Epoch 181/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 231.87batch/s, lastLoss=0.079, valLoss=0.088]\n",
      "Epoch 182/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 230.83batch/s, lastLoss=0.0816, valLoss=0.0921]\n",
      "Epoch 183/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 232.60batch/s, lastLoss=0.0792, valLoss=0.0843]\n",
      "Epoch 184/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 234.30batch/s, lastLoss=0.079, valLoss=0.104]\n",
      "Epoch 185/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 225.08batch/s, lastLoss=0.0811, valLoss=0.0854]\n",
      "Epoch 186/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 227.88batch/s, lastLoss=0.0809, valLoss=0.0885]\n",
      "Epoch 187/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 234.30batch/s, lastLoss=0.0796, valLoss=0.0836]\n",
      "Epoch 188/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 233.81batch/s, lastLoss=0.0786, valLoss=0.0857]\n",
      "Epoch 189/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 232.19batch/s, lastLoss=0.081, valLoss=0.0854]\n",
      "Epoch 190/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 229.08batch/s, lastLoss=0.0794, valLoss=0.0877]\n",
      "Epoch 191/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 231.08batch/s, lastLoss=0.0802, valLoss=0.0919]\n",
      "Epoch 192/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 235.22batch/s, lastLoss=0.0797, valLoss=0.0895]\n",
      "Epoch 193/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.72batch/s, lastLoss=0.0798, valLoss=0.0849]\n",
      "Epoch 194/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 227.26batch/s, lastLoss=0.0789, valLoss=0.0878]\n",
      "Epoch 195/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 222.20batch/s, lastLoss=0.0803, valLoss=0.0864]\n",
      "Epoch 196/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 230.92batch/s, lastLoss=0.0809, valLoss=0.095]\n",
      "Epoch 197/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 223.10batch/s, lastLoss=0.0809, valLoss=0.0852]\n",
      "Epoch 198/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 228.03batch/s, lastLoss=0.0815, valLoss=0.0902]\n",
      "Epoch 199/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.81batch/s, lastLoss=0.0802, valLoss=0.0881]\n",
      "Epoch 200/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 224.68batch/s, lastLoss=0.0803, valLoss=0.0944]\n",
      "Epoch 201/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 223.01batch/s, lastLoss=0.0801, valLoss=0.0952]\n",
      "Epoch 202/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 233.32batch/s, lastLoss=0.08, valLoss=0.0888]\n",
      "Epoch 203/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.52batch/s, lastLoss=0.0784, valLoss=0.0907]\n",
      "Epoch 204/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 214.55batch/s, lastLoss=0.0803, valLoss=0.0899]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 205/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 171.25batch/s, lastLoss=0.0793, valLoss=0.0876]\n",
      "Epoch 206/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 185.43batch/s, lastLoss=0.0779, valLoss=0.0856]\n",
      "Epoch 207/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.89batch/s, lastLoss=0.0775, valLoss=0.0929]\n",
      "Epoch 208/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.56batch/s, lastLoss=0.0792, valLoss=0.0877]\n",
      "Epoch 209/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.03batch/s, lastLoss=0.0769, valLoss=0.0899]\n",
      "Epoch 210/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 218.66batch/s, lastLoss=0.0806, valLoss=0.0946]\n",
      "Epoch 211/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.74batch/s, lastLoss=0.0787, valLoss=0.0831]\n",
      "Epoch 212/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 193.43batch/s, lastLoss=0.0786, valLoss=0.0923]\n",
      "Epoch 213/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.64batch/s, lastLoss=0.0796, valLoss=0.0918]\n",
      "Epoch 214/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.18batch/s, lastLoss=0.0793, valLoss=0.0828]\n",
      "Epoch 215/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 186.77batch/s, lastLoss=0.0781, valLoss=0.0914]\n",
      "Epoch 216/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 139.82batch/s, lastLoss=0.0776, valLoss=0.088]\n",
      "Epoch 217/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 166.42batch/s, lastLoss=0.0779, valLoss=0.0878]\n",
      "Epoch 218/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 205.50batch/s, lastLoss=0.079, valLoss=0.0859]\n",
      "Epoch 219/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 195.80batch/s, lastLoss=0.0775, valLoss=0.0948]\n",
      "Epoch 220/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 216.62batch/s, lastLoss=0.077, valLoss=0.0873]\n",
      "Epoch 221/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 201.43batch/s, lastLoss=0.078, valLoss=0.0922]\n",
      "Epoch 222/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 215.38batch/s, lastLoss=0.0766, valLoss=0.0884]\n",
      "Epoch 223/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.96batch/s, lastLoss=0.0784, valLoss=0.0929]\n",
      "Epoch 224/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 169.43batch/s, lastLoss=0.0817, valLoss=0.0903]\n",
      "Epoch 225/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 203.63batch/s, lastLoss=0.0788, valLoss=0.0931]\n",
      "Epoch 226/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.90batch/s, lastLoss=0.0793, valLoss=0.0856]\n",
      "Epoch 227/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.16batch/s, lastLoss=0.0778, valLoss=0.0834]\n",
      "Epoch 228/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 207.79batch/s, lastLoss=0.0768, valLoss=0.089]\n",
      "Epoch 229/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 192.44batch/s, lastLoss=0.0774, valLoss=0.086]\n",
      "Epoch 230/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.75batch/s, lastLoss=0.0764, valLoss=0.0886]\n",
      "Epoch 231/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.49batch/s, lastLoss=0.0781, valLoss=0.0893]\n",
      "Epoch 232/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 193.36batch/s, lastLoss=0.0787, valLoss=0.0876]\n",
      "Epoch 233/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.90batch/s, lastLoss=0.0786, valLoss=0.0874]\n",
      "Epoch 234/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 214.55batch/s, lastLoss=0.0805, valLoss=0.088]\n",
      "Epoch 235/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 201.07batch/s, lastLoss=0.0776, valLoss=0.086]\n",
      "Epoch 236/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.0762, valLoss=0.0899]\n",
      "Epoch 237/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 173.10batch/s, lastLoss=0.0792, valLoss=0.0922]\n",
      "Epoch 238/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 211.32batch/s, lastLoss=0.0773, valLoss=0.09]\n",
      "Epoch 239/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.53batch/s, lastLoss=0.0768, valLoss=0.0863]\n",
      "Epoch 240/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 198.23batch/s, lastLoss=0.0773, valLoss=0.0932]\n",
      "Epoch 241/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 176.93batch/s, lastLoss=0.0785, valLoss=0.0864]\n",
      "Epoch 242/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.75batch/s, lastLoss=0.0791, valLoss=0.0908]\n",
      "Epoch 243/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 213.93batch/s, lastLoss=0.0765, valLoss=0.0925]\n",
      "Epoch 244/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.43batch/s, lastLoss=0.0773, valLoss=0.0874]\n",
      "Epoch 245/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.52batch/s, lastLoss=0.0786, valLoss=0.0873]\n",
      "Epoch 246/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 214.14batch/s, lastLoss=0.0774, valLoss=0.0876]\n",
      "Epoch 247/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 184.40batch/s, lastLoss=0.0767, valLoss=0.0841]\n",
      "Epoch 248/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 199.28batch/s, lastLoss=0.0778, valLoss=0.092]\n",
      "Epoch 249/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.52batch/s, lastLoss=0.0765, valLoss=0.0908]\n",
      "Epoch 250/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.28batch/s, lastLoss=0.0781, valLoss=0.0957]\n",
      "Epoch 251/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 188.54batch/s, lastLoss=0.078, valLoss=0.086]\n",
      "Epoch 252/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.68batch/s, lastLoss=0.0764, valLoss=0.0878]\n",
      "Epoch 253/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 179.48batch/s, lastLoss=0.0785, valLoss=0.0886]\n",
      "Epoch 254/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 163.98batch/s, lastLoss=0.0772, valLoss=0.0976]\n",
      "Epoch 255/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 183.60batch/s, lastLoss=0.0803, valLoss=0.0914]\n",
      "Epoch 256/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 189.83batch/s, lastLoss=0.0775, valLoss=0.0949]\n",
      "Epoch 257/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 194.78batch/s, lastLoss=0.0801, valLoss=0.0965]\n",
      "Epoch 258/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 184.82batch/s, lastLoss=0.077, valLoss=0.0926]\n",
      "Epoch 259/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 200.35batch/s, lastLoss=0.0787, valLoss=0.0872]\n",
      "Epoch 260/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.89batch/s, lastLoss=0.0771, valLoss=0.0883]\n",
      "Epoch 261/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.52batch/s, lastLoss=0.0787, valLoss=0.0882]\n",
      "Epoch 262/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 172.83batch/s, lastLoss=0.079, valLoss=0.0939]\n",
      "Epoch 263/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 190.15batch/s, lastLoss=0.0763, valLoss=0.0908]\n",
      "Epoch 264/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 135.43batch/s, lastLoss=0.0766, valLoss=0.0849]\n",
      "Epoch 265/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 182.41batch/s, lastLoss=0.0751, valLoss=0.0867]\n",
      "Epoch 266/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 164.46batch/s, lastLoss=0.0749, valLoss=0.0853]\n",
      "Epoch 267/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 131.61batch/s, lastLoss=0.0752, valLoss=0.091]\n",
      "Epoch 268/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 171.25batch/s, lastLoss=0.0796, valLoss=0.0865]\n",
      "Epoch 269/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 194.78batch/s, lastLoss=0.0781, valLoss=0.0874]\n",
      "Epoch 270/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 185.12batch/s, lastLoss=0.0758, valLoss=0.088]\n",
      "Epoch 271/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 190.80batch/s, lastLoss=0.0777, valLoss=0.0882]\n",
      "Epoch 272/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.89batch/s, lastLoss=0.0777, valLoss=0.0861]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 273/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.12batch/s, lastLoss=0.0764, valLoss=0.0941]\n",
      "Epoch 274/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.56batch/s, lastLoss=0.0762, valLoss=0.0907]\n",
      "Epoch 275/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 173.37batch/s, lastLoss=0.0766, valLoss=0.0835]\n",
      "Epoch 276/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 178.34batch/s, lastLoss=0.0751, valLoss=0.0881]\n",
      "Epoch 277/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.18batch/s, lastLoss=0.0764, valLoss=0.0862]\n",
      "Epoch 278/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.53batch/s, lastLoss=0.0753, valLoss=0.0981]\n",
      "Epoch 279/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.077, valLoss=0.0841]\n",
      "Epoch 280/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 198.93batch/s, lastLoss=0.0762, valLoss=0.084]\n",
      "Epoch 281/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.99batch/s, lastLoss=0.0758, valLoss=0.0916]\n",
      "Epoch 282/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.16batch/s, lastLoss=0.0764, valLoss=0.0845]\n",
      "Epoch 283/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 194.45batch/s, lastLoss=0.0745, valLoss=0.0905]\n",
      "Epoch 284/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 206.23batch/s, lastLoss=0.0745, valLoss=0.0919]\n",
      "Epoch 285/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.43batch/s, lastLoss=0.0757, valLoss=0.0895]\n",
      "Epoch 286/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.12batch/s, lastLoss=0.0773, valLoss=0.0906]\n",
      "Epoch 287/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 167.41batch/s, lastLoss=0.0755, valLoss=0.0843]\n",
      "Epoch 288/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.18batch/s, lastLoss=0.0743, valLoss=0.0874]\n",
      "Epoch 289/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 200.71batch/s, lastLoss=0.0756, valLoss=0.0888]\n",
      "Epoch 290/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 215.38batch/s, lastLoss=0.0762, valLoss=0.0837]\n",
      "Epoch 291/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.04batch/s, lastLoss=0.0751, valLoss=0.0853]\n",
      "Epoch 292/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 197.18batch/s, lastLoss=0.0726, valLoss=0.0892]\n",
      "Epoch 293/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 205.92batch/s, lastLoss=0.075, valLoss=0.0916]\n",
      "Epoch 294/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.03batch/s, lastLoss=0.0754, valLoss=0.0841]\n",
      "Epoch 295/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 198.23batch/s, lastLoss=0.074, valLoss=0.0836]\n",
      "Epoch 296/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.49batch/s, lastLoss=0.0738, valLoss=0.0849]\n",
      "Epoch 297/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 203.26batch/s, lastLoss=0.0737, valLoss=0.0914]\n",
      "Epoch 298/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 219.60batch/s, lastLoss=0.0747, valLoss=0.0893]\n",
      "Epoch 299/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 220.90batch/s, lastLoss=0.0749, valLoss=0.0954]\n",
      "Epoch 300/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.75batch/s, lastLoss=0.0737, valLoss=0.0887]\n",
      "Epoch 301/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 218.32batch/s, lastLoss=0.0763, valLoss=0.0908]\n",
      "Epoch 302/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.02batch/s, lastLoss=0.0764, valLoss=0.0913]\n",
      "Epoch 303/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 215.84batch/s, lastLoss=0.0727, valLoss=0.0845]\n",
      "Epoch 304/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 216.00batch/s, lastLoss=0.0746, valLoss=0.0887]\n",
      "Epoch 305/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.49batch/s, lastLoss=0.0735, valLoss=0.0875]\n",
      "Epoch 306/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 200.00batch/s, lastLoss=0.0731, valLoss=0.0853]\n",
      "Epoch 307/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 218.75batch/s, lastLoss=0.0779, valLoss=0.0856]\n",
      "Epoch 308/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.97batch/s, lastLoss=0.0733, valLoss=0.0884]\n",
      "Epoch 309/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.92batch/s, lastLoss=0.0776, valLoss=0.0884]\n",
      "Epoch 310/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 217.18batch/s, lastLoss=0.076, valLoss=0.0838]\n",
      "Epoch 311/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 216.19batch/s, lastLoss=0.075, valLoss=0.0847]\n",
      "Epoch 312/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 219.62batch/s, lastLoss=0.073, valLoss=0.0807]\n",
      "Epoch 313/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.89batch/s, lastLoss=0.0728, valLoss=0.0858]\n",
      "Epoch 314/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.92batch/s, lastLoss=0.0733, valLoss=0.0866]\n",
      "Epoch 315/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.53batch/s, lastLoss=0.0724, valLoss=0.0841]\n",
      "Epoch 316/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.0718, valLoss=0.0851]\n",
      "Epoch 317/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.81batch/s, lastLoss=0.0747, valLoss=0.0848]\n",
      "Epoch 318/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 215.62batch/s, lastLoss=0.0747, valLoss=0.0908]\n",
      "Epoch 319/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.92batch/s, lastLoss=0.0725, valLoss=0.0866]\n",
      "Epoch 320/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.98batch/s, lastLoss=0.0744, valLoss=0.0901]\n",
      "Epoch 321/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 213.89batch/s, lastLoss=0.0725, valLoss=0.0868]\n",
      "Epoch 322/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.32batch/s, lastLoss=0.0722, valLoss=0.0866]\n",
      "Epoch 323/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 199.64batch/s, lastLoss=0.0721, valLoss=0.0872]\n",
      "Epoch 324/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 213.33batch/s, lastLoss=0.072, valLoss=0.0879]\n",
      "Epoch 325/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 212.92batch/s, lastLoss=0.075, valLoss=0.0846]\n",
      "Epoch 326/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 215.33batch/s, lastLoss=0.0702, valLoss=0.0839]\n",
      "Epoch 327/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.55batch/s, lastLoss=0.0737, valLoss=0.0914]\n",
      "Epoch 328/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 218.14batch/s, lastLoss=0.0724, valLoss=0.0844]\n",
      "Epoch 329/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.12batch/s, lastLoss=0.0716, valLoss=0.0886]\n",
      "Epoch 330/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.65batch/s, lastLoss=0.0717, valLoss=0.0949]\n",
      "Epoch 331/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.47batch/s, lastLoss=0.0732, valLoss=0.0788]\n",
      "Epoch 332/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.87batch/s, lastLoss=0.0715, valLoss=0.0886]\n",
      "Epoch 333/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 193.77batch/s, lastLoss=0.0712, valLoss=0.0921]\n",
      "Epoch 334/400: 100%|████████████████████████████████| 56/56 [00:00<00:00, 207.40batch/s, lastLoss=0.07, valLoss=0.0833]\n",
      "Epoch 335/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 188.23batch/s, lastLoss=0.0725, valLoss=0.0865]\n",
      "Epoch 336/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.47batch/s, lastLoss=0.0724, valLoss=0.0862]\n",
      "Epoch 337/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 207.49batch/s, lastLoss=0.0731, valLoss=0.0914]\n",
      "Epoch 338/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 214.97batch/s, lastLoss=0.0705, valLoss=0.0815]\n",
      "Epoch 339/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 219.17batch/s, lastLoss=0.0689, valLoss=0.084]\n",
      "Epoch 340/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.75batch/s, lastLoss=0.0687, valLoss=0.0823]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 341/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.39batch/s, lastLoss=0.0712, valLoss=0.0838]\n",
      "Epoch 342/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 220.46batch/s, lastLoss=0.0685, valLoss=0.0776]\n",
      "Epoch 343/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 216.79batch/s, lastLoss=0.0717, valLoss=0.0898]\n",
      "Epoch 344/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 216.62batch/s, lastLoss=0.0771, valLoss=0.0782]\n",
      "Epoch 345/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 219.17batch/s, lastLoss=0.0704, valLoss=0.0819]\n",
      "Epoch 346/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.05batch/s, lastLoss=0.0715, valLoss=0.0818]\n",
      "Epoch 347/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 220.61batch/s, lastLoss=0.0719, valLoss=0.0818]\n",
      "Epoch 348/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 209.99batch/s, lastLoss=0.0702, valLoss=0.084]\n",
      "Epoch 349/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 218.75batch/s, lastLoss=0.0711, valLoss=0.084]\n",
      "Epoch 350/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.05batch/s, lastLoss=0.0713, valLoss=0.0801]\n",
      "Epoch 351/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 216.44batch/s, lastLoss=0.0667, valLoss=0.0859]\n",
      "Epoch 352/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 202.65batch/s, lastLoss=0.0712, valLoss=0.0822]\n",
      "Epoch 353/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 216.21batch/s, lastLoss=0.0712, valLoss=0.0843]\n",
      "Epoch 354/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.60batch/s, lastLoss=0.0701, valLoss=0.0786]\n",
      "Epoch 355/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.66batch/s, lastLoss=0.0686, valLoss=0.0922]\n",
      "Epoch 356/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 219.59batch/s, lastLoss=0.0694, valLoss=0.0886]\n",
      "Epoch 357/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 220.03batch/s, lastLoss=0.0681, valLoss=0.0849]\n",
      "Epoch 358/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.68batch/s, lastLoss=0.0701, valLoss=0.0788]\n",
      "Epoch 359/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 202.47batch/s, lastLoss=0.069, valLoss=0.0829]\n",
      "Epoch 360/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.50batch/s, lastLoss=0.0676, valLoss=0.0862]\n",
      "Epoch 361/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 205.02batch/s, lastLoss=0.0718, valLoss=0.0774]\n",
      "Epoch 362/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 196.49batch/s, lastLoss=0.0685, valLoss=0.0813]\n",
      "Epoch 363/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 214.14batch/s, lastLoss=0.0724, valLoss=0.0788]\n",
      "Epoch 364/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.92batch/s, lastLoss=0.0646, valLoss=0.0796]\n",
      "Epoch 365/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 192.15batch/s, lastLoss=0.0656, valLoss=0.0755]\n",
      "Epoch 366/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.31batch/s, lastLoss=0.0705, valLoss=0.0887]\n",
      "Epoch 367/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 188.87batch/s, lastLoss=0.0658, valLoss=0.0777]\n",
      "Epoch 368/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 199.99batch/s, lastLoss=0.0674, valLoss=0.0752]\n",
      "Epoch 369/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 209.24batch/s, lastLoss=0.0639, valLoss=0.0779]\n",
      "Epoch 370/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 211.29batch/s, lastLoss=0.0646, valLoss=0.0771]\n",
      "Epoch 371/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 218.19batch/s, lastLoss=0.0703, valLoss=0.0767]\n",
      "Epoch 372/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 209.74batch/s, lastLoss=0.067, valLoss=0.0801]\n",
      "Epoch 373/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 216.63batch/s, lastLoss=0.0632, valLoss=0.0768]\n",
      "Epoch 374/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 216.63batch/s, lastLoss=0.0663, valLoss=0.083]\n",
      "Epoch 375/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 204.00batch/s, lastLoss=0.0672, valLoss=0.0808]\n",
      "Epoch 376/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 192.11batch/s, lastLoss=0.0671, valLoss=0.0828]\n",
      "Epoch 377/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.95batch/s, lastLoss=0.0634, valLoss=0.0763]\n",
      "Epoch 378/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 211.71batch/s, lastLoss=0.0639, valLoss=0.081]\n",
      "Epoch 379/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 206.02batch/s, lastLoss=0.0646, valLoss=0.076]\n",
      "Epoch 380/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 199.64batch/s, lastLoss=0.0679, valLoss=0.0774]\n",
      "Epoch 381/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 217.88batch/s, lastLoss=0.0659, valLoss=0.0848]\n",
      "Epoch 382/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 217.76batch/s, lastLoss=0.067, valLoss=0.0823]\n",
      "Epoch 383/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 215.40batch/s, lastLoss=0.0645, valLoss=0.0763]\n",
      "Epoch 384/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 219.61batch/s, lastLoss=0.0636, valLoss=0.0785]\n",
      "Epoch 385/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 214.50batch/s, lastLoss=0.0621, valLoss=0.0786]\n",
      "Epoch 386/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.13batch/s, lastLoss=0.0638, valLoss=0.0787]\n",
      "Epoch 387/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.92batch/s, lastLoss=0.0637, valLoss=0.0793]\n",
      "Epoch 388/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 208.56batch/s, lastLoss=0.0632, valLoss=0.0758]\n",
      "Epoch 389/400: 100%|███████████████████████████████| 56/56 [00:00<00:00, 217.42batch/s, lastLoss=0.065, valLoss=0.0726]\n",
      "Epoch 390/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 214.54batch/s, lastLoss=0.0628, valLoss=0.0789]\n",
      "Epoch 391/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 212.09batch/s, lastLoss=0.0626, valLoss=0.0799]\n",
      "Epoch 392/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 210.52batch/s, lastLoss=0.0601, valLoss=0.0715]\n",
      "Epoch 393/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 201.80batch/s, lastLoss=0.0653, valLoss=0.0859]\n",
      "Epoch 394/400: 100%|██████████████████████████████| 56/56 [00:00<00:00, 200.00batch/s, lastLoss=0.0636, valLoss=0.0822]\n",
      "Epoch 395/400:  79%|████████████████████████▎      | 44/56 [00:00<00:00, 208.41batch/s, lastLoss=0.063, valLoss=0.0767]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sLSTM_train_loss, sLSTM_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msLSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\Notebooks\\..\\SkinLearning\\Utils\\NN.py:30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     27\u001b[0m it\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m inp, out \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 30\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m predicted \u001b[38;5;241m=\u001b[39m net(inp)\n\u001b[0;32m     33\u001b[0m cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:271\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[0;32m    272\u001b[0m             p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sLSTM_train_loss, sLSTM_val_loss =  train(train_loader, sLSTM, val_loader=test_loader, LR=0.001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368b9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
