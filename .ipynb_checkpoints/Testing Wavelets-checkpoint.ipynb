{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5838657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import jit\n",
    "import pickle\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import pywt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.cuda import FloatTensor\n",
    "\n",
    "# Req for package\n",
    "sys.path.append(\"../\")\n",
    "from SkinLearning.Utils.NN import train, test, DEVICE\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037faeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder name will correspond to index of sample\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, scaler, signalFolder=\"D:/SamplingResults2\", sampleFile=\"Data/newSamples.pkl\", runs=range(65535), steps=128):\n",
    "        # Load both disp1 and disp2 from each folder\n",
    "        # Folders ordered according to index of sample\n",
    "        # Use the corresponding sample as y -> append probe?\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        with open(f\"{sampleFile}\", \"rb\") as f:\n",
    "             samples = pickle.load(f)\n",
    "        \n",
    "        self.min = np.min(samples[runs])\n",
    "        self.max = np.max(samples[runs])\n",
    "        \n",
    "        \n",
    "        for run in tqdm(runs):\n",
    "            inp = []\n",
    "            fail = False\n",
    "            \n",
    "            files = os.listdir(f\"{signalFolder}/{run}/\")\n",
    "            \n",
    "            if files != ['Disp1.csv', 'Disp2.csv']:\n",
    "                continue\n",
    "            \n",
    "            for file in files:\n",
    "                a = pd.read_csv(f\"{signalFolder}/{run}/{file}\")\n",
    "                a.rename(columns = {'0':'x', '0.1': 'y'}, inplace = True)\n",
    "                \n",
    "                # Skip if unconverged\n",
    "                if a['x'].max() != 7.0:\n",
    "                    fail = True\n",
    "                    break\n",
    "\n",
    "                # Interpolate curve for consistent x values\n",
    "                xNew = np.linspace(0, 7, num=steps, endpoint=False)\n",
    "                interped = interp1d(a['x'], a['y'], kind='cubic', fill_value=\"extrapolate\")(xNew)\n",
    "                    \n",
    "                \n",
    "                inp.append(interped.astype(\"float32\"))\n",
    "            \n",
    "            if not fail:\n",
    "                if len(inp) != 2:\n",
    "                    raise Exception(\"sdf\")\n",
    "\n",
    "                self.input.append(inp)\n",
    "                self.output.append(samples[int(run)])\n",
    "        \n",
    "        scaler.fit(self.output)\n",
    "        self.output = scaler.fit_transform(self.output)\n",
    "        self.output = tensor(self.output).type(FloatTensor)\n",
    "        \n",
    "        self.input = [waveletExtraction(sample) for sample in self.input]\n",
    "        self.input = tensor(self.input).type(FloatTensor)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"input\": self.input[idx], \"output\": self.output[idx]}\n",
    "        return sample\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3e5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates the data set from filtered samples\n",
    "    Returns the dataset and the scaler\n",
    "\"\"\"\n",
    "def getDataset(**kwargs):\n",
    "    # Get filtered data\n",
    "    if not 'runs' in kwargs.keys():\n",
    "        with open(\"Data/filtered.pkl\", \"rb\") as f:\n",
    "            runs = pickle.load(f)\n",
    "\n",
    "        kwargs['runs'] = runs\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset = SkinDataset(scaler=scaler, **kwargs)\n",
    "\n",
    "    return dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "383e9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Creates a train/test split from the given data\n",
    "    Returns train and test data loaders\n",
    "\"\"\"\n",
    "def getSplit(dataset, p1=0.8):\n",
    "    train_n = int(p1 * len(dataset))\n",
    "    test_n = len(dataset) - train_n\n",
    "    train_set, test_set = random_split(dataset, [train_n, test_n])\n",
    "\n",
    "    return DataLoader(train_set, batch_size=32, shuffle=True), \\\n",
    "        DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0adfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveletExtraction(x, wavelet='db1', level=4):\n",
    "# perform wavelet packet decomposition on signal 1\n",
    "    wp = pywt.WaveletPacket(x[0], wavelet, mode='symmetric', maxlevel=level)\n",
    "    coeffs1 = []\n",
    "    for node in wp.get_level(level, 'natural'):\n",
    "        if node.path.endswith('a') or node.path.endswith('d'):\n",
    "            coeffs1.append(node.data)\n",
    "    coeffs1 = np.concatenate(coeffs1)\n",
    "    \n",
    "    # perform wavelet packet decomposition on signal 2\n",
    "    wp = pywt.WaveletPacket(x[1], wavelet, mode='symmetric', maxlevel=level)\n",
    "    coeffs2 = []\n",
    "    for node in wp.get_level(level, 'natural'):\n",
    "        if node.path.endswith('a') or node.path.endswith('d'):\n",
    "            coeffs2.append(node.data)\n",
    "    coeffs2 = np.concatenate(coeffs2)\n",
    "    \n",
    "    # concatenate the two coefficient arrays\n",
    "    feature_vector = np.concatenate((coeffs1, coeffs2))\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0284c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:09<00:00, 243.38it/s]\n",
      "C:\\Users\\rjsou\\AppData\\Local\\Temp\\ipykernel_17016\\3142177655.py:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  self.input = tensor(self.input).type(FloatTensor)\n"
     ]
    }
   ],
   "source": [
    "dataset, scaler = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38074a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = getSplit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89cfc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRNN(nn.Module):\n",
    "    def __init__(self, input_size=256, hidden_size=1024):\n",
    "        super(SiameseRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "                \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 6)\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = x.reshape(batch_size, 1, -1)\n",
    "        \n",
    "        #x1 = x[:, 0, :, :].reshape(batch_size, 1, -1)\n",
    "        #x2 = x[:, 0, :, :].reshape(batch_size, 1, -1)\n",
    "        \n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(DEVICE)\n",
    "        _, h1 = self.rnn(x, h0)  # Add a batch dimension\n",
    "       # _, h2 = self.rnn(x2, h0)  # Add a batch dimension\n",
    "        \n",
    "       # out = torch.cat([h1[-1], h2[-1]], dim=1)\n",
    "        out=h1[-1]\n",
    "        out = out.reshape(batch_size, -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "218fb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 256, kernel_size=5, padding=1, bias=False)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(256, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(512, 1024, kernel_size=3, padding=1, bias=False)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        self.rnn = nn.RNN(31, 256, batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 1024)\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024 , 512)\n",
    "        self.d2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.d3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, 1, -1)\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        h0 = torch.zeros(1, batch_size, 256).to(x.device)\n",
    "        x, h = self.rnn(x, h0)\n",
    "        x = h[-1].reshape(batch_size, -1)\n",
    "        x = self.d1(torch.relu(self.fc1(x)))\n",
    "        \n",
    "        x = self.d2(torch.relu(self.fc2(x)))\n",
    "        \n",
    "        x = self.d3(torch.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        x = x.view(batch_size, 6)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83474d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sRNN = SiameseRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690080cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sRNN_train_loss, sRNN_val_loss =  train(train_loader, sRNN, val_loader=test_loader, LR=0.001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4b53c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SkinLearning.Utils.Dataset import getDataset, getSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9720d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/filtered.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mgetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\SkinLearning\\Utils\\Dataset.py:16\u001b[0m, in \u001b[0;36mgetDataset\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetDataset\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Get filtered data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Data/filtered.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     17\u001b[0m             runs \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     19\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m runs\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/filtered.pkl'"
     ]
    }
   ],
   "source": [
    "dataset, scaler = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf3b559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2027b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: 100%|██████████████████████████████████████████████████████████████████| 56/56 [00:01<00:00, 37.25batch/s]\n",
      "Epoch 2/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.21batch/s, lastLoss=0.254, valLoss=0.195]\n",
      "Epoch 3/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.07batch/s, lastLoss=0.197, valLoss=0.155]\n",
      "Epoch 4/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.31batch/s, lastLoss=0.185, valLoss=0.152]\n",
      "Epoch 5/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.13batch/s, lastLoss=0.169, valLoss=0.132]\n",
      "Epoch 6/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.16batch/s, lastLoss=0.165, valLoss=0.126]\n",
      "Epoch 7/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.25batch/s, lastLoss=0.159, valLoss=0.121]\n",
      "Epoch 8/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.11batch/s, lastLoss=0.156, valLoss=0.128]\n",
      "Epoch 9/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.20batch/s, lastLoss=0.152, valLoss=0.127]\n",
      "Epoch 10/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.27batch/s, lastLoss=0.15, valLoss=0.123]\n",
      "Epoch 11/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 37.20batch/s, lastLoss=0.146, valLoss=0.12]\n",
      "Epoch 12/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.74batch/s, lastLoss=0.145, valLoss=0.113]\n",
      "Epoch 13/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.72batch/s, lastLoss=0.142, valLoss=0.109]\n",
      "Epoch 14/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.48batch/s, lastLoss=0.141, valLoss=0.124]\n",
      "Epoch 15/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.53batch/s, lastLoss=0.138, valLoss=0.104]\n",
      "Epoch 16/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.83batch/s, lastLoss=0.136, valLoss=0.123]\n",
      "Epoch 17/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.90batch/s, lastLoss=0.135, valLoss=0.107]\n",
      "Epoch 18/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.85batch/s, lastLoss=0.133, valLoss=0.105]\n",
      "Epoch 19/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 37.05batch/s, lastLoss=0.132, valLoss=0.108]\n",
      "Epoch 20/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.97batch/s, lastLoss=0.132, valLoss=0.102]\n",
      "Epoch 21/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.95batch/s, lastLoss=0.13, valLoss=0.0994]\n",
      "Epoch 22/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.87batch/s, lastLoss=0.128, valLoss=0.097]\n",
      "Epoch 23/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.98batch/s, lastLoss=0.13, valLoss=0.104]\n",
      "Epoch 24/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.78batch/s, lastLoss=0.126, valLoss=0.107]\n",
      "Epoch 25/400: 100%|███████████████████████████████████| 56/56 [00:01<00:00, 36.81batch/s, lastLoss=0.127, valLoss=0.11]\n",
      "Epoch 26/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.08batch/s, lastLoss=0.125, valLoss=0.0939]\n",
      "Epoch 27/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.05batch/s, lastLoss=0.124, valLoss=0.0942]\n",
      "Epoch 28/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.87batch/s, lastLoss=0.123, valLoss=0.0973]\n",
      "Epoch 29/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.94batch/s, lastLoss=0.124, valLoss=0.114]\n",
      "Epoch 30/400: 100%|████████████████████████████████████| 56/56 [00:01<00:00, 36.99batch/s, lastLoss=0.125, valLoss=0.1]\n",
      "Epoch 31/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.00batch/s, lastLoss=0.122, valLoss=0.0929]\n",
      "Epoch 32/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.79batch/s, lastLoss=0.122, valLoss=0.0995]\n",
      "Epoch 33/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.00batch/s, lastLoss=0.121, valLoss=0.0959]\n",
      "Epoch 34/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.82batch/s, lastLoss=0.12, valLoss=0.0897]\n",
      "Epoch 35/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.80batch/s, lastLoss=0.121, valLoss=0.0926]\n",
      "Epoch 36/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.90batch/s, lastLoss=0.119, valLoss=0.0931]\n",
      "Epoch 37/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.06batch/s, lastLoss=0.119, valLoss=0.0972]\n",
      "Epoch 38/400: 100%|██████████████████████████████████| 56/56 [00:01<00:00, 36.96batch/s, lastLoss=0.119, valLoss=0.096]\n",
      "Epoch 39/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.67batch/s, lastLoss=0.121, valLoss=0.0971]\n",
      "Epoch 40/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.85batch/s, lastLoss=0.118, valLoss=0.0987]\n",
      "Epoch 41/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.03batch/s, lastLoss=0.119, valLoss=0.0941]\n",
      "Epoch 42/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 36.89batch/s, lastLoss=0.117, valLoss=0.0966]\n",
      "Epoch 43/400: 100%|█████████████████████████████████| 56/56 [00:01<00:00, 37.13batch/s, lastLoss=0.116, valLoss=0.0928]\n",
      "Epoch 44/400:  64%|█████████████████████▏           | 36/56 [00:00<00:00, 36.46batch/s, lastLoss=0.117, valLoss=0.0884]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rnn_train_loss, rnn_val_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MastersDiss\\SkinLearning\\Utils\\NN.py:31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     28\u001b[0m inp, out \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool3(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n\u001b[1;32m---> 35\u001b[0m h0 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m x, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, h0)\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m h[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_train_loss, rnn_val_loss =  train(train_loader, rnn, val_loader=test_loader, LR=0.0001, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
